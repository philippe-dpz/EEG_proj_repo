{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Chargement des différentes librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Chargement des différentes librairies\n",
    "\n",
    "import sys, os #, math, time\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "from src.thot.sesh import *\n",
    "from src.thot.catch_features import *\n",
    "\n",
    "import pywt, librosa\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import model_selection, metrics\n",
    "# Supervised learning\n",
    "from sklearn import ensemble, svm, linear_model, neighbors\n",
    "# Unsupervised learning\n",
    "from sklearn import discriminant_analysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.ensemble import VotingClassifier, StackingClassifier, RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# import seaborn as sns\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.metrics import classification_report             # type: ignore\n",
    "# from sklearn import model_selection, preprocessing as sk_p    # type: ignore\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import VotingClassifier, GradientBoostingClassifier\n",
    "\n",
    "# from keras.models import Sequential                             # type: ignore\n",
    "# from keras.callbacks import EarlyStopping                       # type: ignore\n",
    "# from keras.layers import GlobalAveragePooling1D, MaxPooling1D   # type: ignore\n",
    "# from keras.layers import ReLU, Dense, Dropout, Conv1D, Flatten  # type: ignore\n",
    "# from keras.layers import LSTM, LeakyReLU, PReLU, ConvLSTM1D\n",
    "# from keras.layers import Bidirectional, TimeDistributed, RepeatVector, Flatten\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.optimizers import AdamW, Adam            # type: ignore\n",
    "\n",
    "# from pyriemann.spatialfilters import CSP\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Déclaration de constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Déclaration de constantes\n",
    "\n",
    "# Fréquence d'échantillonnage - Hz (Nombre de valeur / sec)\n",
    "SAMPLE_RATE = 250\n",
    "#\n",
    "PW2: int   = int(np.floor(np.log2(SAMPLE_RATE))) # 2 << SAMPLE_RATE // 32\n",
    "#\n",
    "NFFT: int  = 1 << PW2\n",
    "# Epoque en sec donnée en nombre d'échantillon consectutif # 4\" de données (multiple de 2)\n",
    "CHUNK: int = ceil_pow2(SAMPLE_RATE) * 4 # SAMPLE_RATE << 1 # (1 << PW2) * 4\n",
    "# Temps additionel pour étendre le domaines d'étude.\n",
    "LAG: int   = 62 # 0 # CHUNK >> 2 # SAMPLE_RATE >> 2 #    # Décalage du signal dû signal ~250ms\n",
    "# Taille des lots\n",
    "BATCH: int = 32\n",
    "#\n",
    "SLIDE: bool = False\n",
    "\n",
    "# Correspondance pour la classification\n",
    "hands_event = {0: 'Left', 1: 'Right'}\n",
    "# Nombre dévènement à prédire\n",
    "numbers     = range(len(hands_event))\n",
    "# Les bandes de fréquences d'intérêt\n",
    "eeg_bands   = {'Delta': (.1, 4),\n",
    "               'Theta': (4, 8),\n",
    "               'Alpha': (8, 14),\n",
    "               'Beta' : (14, 31),\n",
    "               'Gamma': (31, (SAMPLE_RATE >> 2) - 1),}\n",
    "# Coefficients pour filtres Butterworth numérique d'ordre N pour le filtrage passe-bande\n",
    "bands_coeff = {band: butter_bandpass(low, high, SAMPLE_RATE) for band, (low, high) in eeg_bands.items()}\n",
    "# Largeur de bande retenue pour étude de cas\n",
    "band_interest = butter_bandpass(1e-3, eeg_bands['Alpha'][1], SAMPLE_RATE)\n",
    "\n",
    "LAG, CHUNK, (CHUNK if SLIDE else (CHUNK - LAG))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catch(data: Board, col: str, channels: Clause, event: int,\n",
    "            norm: bool = False) -> Board:\n",
    "    func = [np.min, np.max, np.median]\n",
    "    #, np.mean func = [np.min, np.max, np.std, np.var, np.mean, np.median]\n",
    "    # func = np.append(func, catch_)\n",
    "    room = data[data[col] == event]\n",
    "    name = [f.__name__ for f in func]\n",
    "    # head = [f\"{f}_diff\" for f in name]\n",
    "    head = [f\"{c}_{f}\" for c in channels for f in name] \\\n",
    "         + [f\"{f}_diff\" for f in name]\n",
    "    \n",
    "    if norm:\n",
    "        temp = [[normalized(x) for x in room[c]] for c in channels]\n",
    "        c3, c4 = [[[f(v) for v in s] for f in func] for s in temp]\n",
    "    else:\n",
    "        c3, c4 = [[[f(v) for v in room[c]] for f in func] for c in channels]\n",
    "    \n",
    "    sub = np.subtract(c3, c4)\n",
    "    _df = pd.DataFrame(np.array((*c3, *c4, *sub)).T, columns = head)\n",
    "    # _df = pd.DataFrame(np.array(sub).T, columns = head)\n",
    "\n",
    "    print(np.array((np.stack([c3, c4], axis= 1), *sub)))\n",
    "\n",
    "    # display(_df)\n",
    "    \n",
    "    _df[col] = event\n",
    "    \n",
    "    return _df\n",
    "\n",
    "def pool_(data: Board, event: int) -> Board:\n",
    "    return catch(data, 'EventType', Graphein_DatasLoader.eeg_Chans[: -1], event, norm = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Acquisition des données d'entrainement et de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Acquisition des données d'entrainement\n",
    "path     = '../data/data.zip'\n",
    "datas    = Graphein_DatasLoader(path, 'train', True)\n",
    "files    = datas.files\n",
    "entrants = datas.runs\n",
    "targets  = datas.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Acquisition des données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = csv_in_zip(path, directory = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Pré-traitement (Segmentation) des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y = train_test_init(entrants, targets, files, methode = -1, reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo_filter_01(data: Vector) -> Vector:\n",
    "    b, a = bands_coeff['Delta']\n",
    "    res  = [bandpass_filter(d[1] - d[0], b, a) for d in data]\n",
    "\n",
    "    return np.array(res)\n",
    "\n",
    "def foo_filter_02(data: Vector) -> Vector:\n",
    "    b, a = bands_coeff['Delta']\n",
    "    v1   = [np.append(bandpass_filter(d[0], b, a), bandpass_filter(d[1], b, a)) for d in data]\n",
    "    \n",
    "    return np.array(v1)\n",
    "\n",
    "def foo_filter_03(data: Vector) -> Vector:\n",
    "    b, a = bands_coeff['Delta']\n",
    "    v1   = [np.append(bandpass_filter(d[1] - d[0], b, a), bandpass_filter(d[2], b, a)) for d in data]\n",
    "    \n",
    "    return np.array(v1)\n",
    "\n",
    "def foo_filter_04(data: Vector) -> Vector:\n",
    "    return np.array([d[1] - d[0] for d in data])\n",
    "\n",
    "def foo_welch(data: Vector, fs: float = SAMPLE_RATE) -> Vector:\n",
    "    return np.array([signal.welch(d[1] - d[0], fs, nperseg = 256)[1] for d in data])\n",
    "\n",
    "def foo_concate(X): return np.array([np.concatenate(x) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test de classification - Proposition 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csp = CSP(nfilter = 2)\n",
    "# pca = PCA(CHUNK >> 4)\n",
    "# std = RobustScaler()\n",
    "std = StandardScaler()\n",
    "#\n",
    "clf = svm.SVC() # gamma = 'auto'\n",
    "#\n",
    "lrg = linear_model.LogisticRegression(max_iter = 80000)\n",
    "#\n",
    "lsg = linear_model.SGDClassifier(early_stopping = True)\n",
    "#\n",
    "dal = discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "# \n",
    "gbc = ensemble.GradientBoostingClassifier(max_features = 'sqrt')\n",
    "#\n",
    "knc = neighbors.KNeighborsClassifier(n_neighbors = 2)\n",
    "\n",
    "chans = Graphein_DatasLoader.eeg_Chans # + Graphein_DatasLoader.ecg_Chans\n",
    "func  = foo_filter_01 # foo_concate # foo_filter_02 # foo_welch # foo_filter_01 # foo_filter_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc   = 50\n",
    "loc   = 0\n",
    "table = []\n",
    "vclf  = [clf, lrg, dal]\n",
    "leg   = ['SVC', 'LRC', 'DAL']\n",
    "\n",
    "for i in np.append(range(1000, 900, -inc), range(250, 0, -inc)):\n",
    "    for j in range(0, min(i - inc, (SAMPLE_RATE >> 0) + inc), inc):\n",
    "        print(titre(f'[{len(table) // 3}]: {j}, {i}', 55))\n",
    "\n",
    "        X_train, y_train = Graphein(test_X, test_y, chans, numbers, i, j,\n",
    "                                 level = False, slide = SLIDE).shuffle(func)\n",
    "        X_test, y_test   = Graphein(train_X, train_y, chans, numbers, i, j,\n",
    "                                 level = False, slide = SLIDE).shuffle(func)\n",
    "        X_train_scaled   = std.fit_transform(X_train)\n",
    "        X_test_scaled    = std.transform(X_test)\n",
    "\n",
    "        for mdl in vclf:\n",
    "            mdl.fit(X_train_scaled, y_train)\n",
    "\n",
    "            pred_train = metrics.accuracy_score(y_train, mdl.predict(X_train_scaled))\n",
    "            pred_test  = metrics.accuracy_score(y_test, mdl.predict(X_test_scaled))\n",
    "            score_test = cross_validate(mdl, X_test_scaled, y_test, cv = 4, scoring = ['accuracy', 'f1',])\n",
    "\n",
    "            table.append([j, i, pred_train, pred_test, \n",
    "                          np.mean([score_test['test_accuracy'].mean(), score_test['test_f1'].mean()])])\n",
    "\n",
    "            print(f\"• {mdl}:\",\n",
    "                  *[f'{x:.1%}' for x in table[-1][2:]],\n",
    "                  f\"\\tTest - [acc: {score_test['test_accuracy'].mean():.1%}]\",\n",
    "                  f\"[f1: {score_test['test_f1'].mean():.1%}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb  = len(vclf)\n",
    "v1  = [table[i :: nb] for i in range(nb)]\n",
    "pos = [f\"{x[0]}, {x[1]}\" for x in v1[0]]\n",
    "xtk = range(len(pos))\n",
    "\n",
    "for i in range(nb):\n",
    "    plt.figure(figsize = (18, 4))\n",
    "    plt.xlim([xtk[0], xtk[-1]])\n",
    "    plt.grid(visible = True, ls = '-.')\n",
    "    plt.xticks(xtk, labels = pos, rotation = 'vertical')\n",
    "\n",
    "    for j in range(1, 4):\n",
    "        crv  = np.array([float(x[1 + j]) for x in v1[i]])\n",
    "        max_ = max(crv)\n",
    "        x_   = np.where(crv == max_)[0][0]\n",
    "        k    = j % 2 == 0\n",
    "        off  = .2 * ([-1, 1][k])\n",
    "        fnt  = {\n",
    "                  'fontsize': 13,\n",
    "                  'fontweight': 'medium',\n",
    "                  'horizontalalignment': ['right', 'left'][k],\n",
    "                  'bbox': dict(facecolor = 'white', alpha = .75),\n",
    "                }\n",
    "        line = plt.plot(xtk, crv, f'{'_oxd'[j]}-', label = f'{leg[i]} ({['', 'Train', 'Test', 'Acc'][j]})')\n",
    "        c    = line[-1].get_color()\n",
    "\n",
    "        plt.hlines(np.mean(crv), 0, 250, colors = c, ls = '--')\n",
    "        plt.vlines(x_, .45, max_, ls = ':', colors = c, lw = 3)\n",
    "        plt.text(x_ + off , max_, f'{max_:.1%}', c = c, fontdict = fnt)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tnse = TSNE(n_components = 2, learning_rate = 'auto')\n",
    "\n",
    "X_embedded = tnse.fit_transform(X_train_scaled)\n",
    "\n",
    "print(tnse.kl_divergence_, tnse.learning_rate_)\n",
    "\n",
    "plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c = y_train)\n",
    "plt.show()\n",
    "\n",
    "# pd.crosstab(X_embedded[:, 0].T, y_train), random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://raphaelvallat.com/bandpower.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test EEG Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import datasets            # type: ignore\n",
    "from torchvision.transforms import ToTensor # type: ignore\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/banggiangle/cnn-eeg-pytorch\n",
    "\n",
    "# class EEG_Dataset(Dataset):\n",
    "#     def __init__(self, datas: list[Board], labels: list[Board] | None, Channels: Clause,\n",
    "#                  events: int | Index, chunk_size: int, gap: int, transform = None):\n",
    "#         self.transform  = transform\n",
    "#         self.parts      = torch_split(datas, labels, Channels, events, chunk_size, gap)\n",
    "#         self.len        = len(self.parts)\n",
    "#         # self.X          = datas\n",
    "#         # self.y          = labels\n",
    "#         # self.Channels   = Channels\n",
    "#         # self.events     = events\n",
    "#         # self.chunk_size = chunk_size\n",
    "#         # self.gap        = gap\n",
    "        \n",
    "#     def __getitem__(self, index: int):\n",
    "#             source_ = self.parts[index]\n",
    "\n",
    "#             if self.transform is not None: source_ = self.transform(source_)\n",
    "            \n",
    "#             return source_, torch.tensor((index << 1) // self.len)\n",
    "\n",
    "#     def __len__(self): return self.len\n",
    "\n",
    "class EEG_RNN(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        super(EEG_RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first = True)\n",
    "        self.fc  = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0      = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _  = self.rnn(x, h0)\n",
    "        out     = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parts = Graphein(train_X, train_y, Graphein_DatasLoader.eeg_Chans, numbers, CHUNK, LAG)\n",
    "test_parts  = Graphein(test_X, test_y, Graphein_DatasLoader.eeg_Chans, numbers, CHUNK, LAG)\n",
    "\n",
    "train_loader = DataLoader(train_parts, batch_size = BATCH, shuffle = True)\n",
    "test_loader  = DataLoader(test_parts, batch_size = BATCH, shuffle = True)\n",
    "\n",
    "print(titre(\"Custome 'train_test_split'\", 40))\n",
    "print('Torseur train\\t:', *train_parts.parts)\n",
    "print('Torseur test\\t:', *np.shape(test_parts.parts))\n",
    "print('-' * 40)\n",
    "\n",
    "# X, y = next(iter(train_loader))\n",
    "# X, y = next(iter(DataLoader(train_parts, batch_size = BATCH, shuffle = True)))\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "model = nn.Sequential(\n",
    "   nn.Conv2d(in_channels = BATCH, out_channels = BATCH >> 1, kernel_size = 3),\n",
    "   nn.MaxPool2d(kernel_size = 2),\n",
    "   nn.ReLU(),\n",
    "    \n",
    "   nn.Conv2d(in_channels = 512 >> 1, out_channels = 512 >> 2, kernel_size = 3),\n",
    "   nn.ReLU(),\n",
    "   nn.MaxPool2d(kernel_size = 2),\n",
    "    \n",
    "   nn.Conv2d(in_channels = 512 >> 2, out_channels = 512 >> 3, kernel_size = 3),\n",
    "   nn.ReLU(),\n",
    "   nn.MaxPool2d(kernel_size = 2),\n",
    "    \n",
    "   nn.Flatten(),\n",
    "#    nn.Linear(64 * 6 * 6, 64),\n",
    "   nn.ReLU(),\n",
    "   nn.Linear(512 >> 3, 4)\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# criterion  = nn.RNN((3, 512), hidden_dim, layer_dim, batch_first = True, nonlinearity = 'relu') # nn.CrossEntropyLoss()\n",
    "# preprocess = weights.transforms()\n",
    "# y_pred     = model(X.to(device))\n",
    "\n",
    "# criterion(y_pred, y.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    # outputs = model(X.unsqueeze(2))  # Add a dimension for input size\n",
    "    # loss    = criterion(outputs, y.unsqueeze(2))\n",
    "    \n",
    "    loss_total   = 0\n",
    "    progress_bar = tqdm(train_loader, desc = f\"Epoch {epoch:1d}\",\n",
    "                        leave = True, disable = False)\n",
    "    \n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        # Batch de données\n",
    "        X_batch, y_batch = batch\n",
    "        # Device\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        # Gradient mis 0\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calcul de prédiction\n",
    "        y_pred = model(X_batch.to(torch.float32))\n",
    "\n",
    "        # Calcul de la fonction de perte\n",
    "        loss =  criterion(y_pred, y_batch) \n",
    "        # Backpropagation: calculer le gradient de la loss en fonction de chaque couche\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clipper le gradient entre 0 et 1 pour plus de stabilité\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Descente de gradient: actualisation des paramètres\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_total += loss.item()\n",
    "        \n",
    "        progress_bar.set_postfix({\"training_loss\": \"{:.3f}\".format(loss_total / (i + 1))})\n",
    "    \n",
    "    # if (epoch + 1) % 10 == 0:\n",
    "    #     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation des spectrogrammes / Test\n",
    "\n",
    "def logMelSpectrogram(data: Vector, rate: int, dt: float = 1e-2) -> Vector:\n",
    "    tps = 1 << int(np.floor(np.log2(rate * dt)))\n",
    "    # print(tps)\n",
    "    # Spectrogramme\n",
    "    stfts = np.abs(librosa.stft(y = data, n_fft = tps, hop_length = 1 << 2, center = True)).T\n",
    "    # Filtre de MEL\n",
    "    liny  = librosa.filters.mel(sr = rate, n_fft = tps + 1, n_mels = stfts.shape[-1]).T\n",
    "    # Application du filtre au spectrogramme\n",
    "    mel_  = np.tensordot(stfts, liny, 1)\n",
    "\n",
    "    return np.log(mel_ + 1e-6)\n",
    "    \n",
    "def structure(data: Board | Vector, rate: int, whr: Clause) -> Vector:\n",
    "    # return np.array([logMelSpectrogram(X, rate, 2) for X in data[whr]])\n",
    "    # return np.stack([[signal.welch(X, rate)[1] for X in data[c]] for c in whr], axis = 2)\n",
    "    return np.stack(data['C4'] - data['C3'], axis = 0)\n",
    "    # return np.stack([[X for X in data[c]] for c in whr], axis = 2)\n",
    "\n",
    "def img_spectrogram(raw: Vector, rate: int, nfft: int = 1 << 10) -> Vector:\n",
    "    return librosa.feature.melspectrogram(y = raw, sr = rate, hop_length = 1, \n",
    "                            n_fft = nfft, n_mels = 32, fmin = 0, fmax = 20, win_length = 32)\n",
    "\n",
    "def spectrogram_dep(data: Board, rate: int, channels: Clause, n_row: int = 5, n_col: int = 12):\n",
    "    sample = np.random.default_rng().integers(data.shape[0], size = n_row)\n",
    "\n",
    "    sample.sort()\n",
    "\n",
    "    plt.figure(figsize = (18, 2 * .48 * n_row))\n",
    "\n",
    "    pos = 0\n",
    "\n",
    "    for k in sample:\n",
    "        for c in channels:\n",
    "            # x   = normalized(data[c][k])\n",
    "            raw = img_spectrogram(raw = data[c][k], rate = rate)\n",
    "            pos += 1\n",
    "            \n",
    "            plt.subplot(n_row, n_col, pos)\n",
    "            plt.title(f\"{((pos - 1) // 3) + 1} . {k} - {c}\", fontsize = 8)\n",
    "            librosa.display.specshow(data = 1 - raw, sr = rate, hop_length = 1)\n",
    "            \n",
    "            # pos = n_col * (i >> 1) + j\n",
    "            # f, t, Sxx = signal.spectrogram(x, rate)\n",
    "            # plt.subplot(n_row, n_col, pos + 4)\n",
    "            # plt.pcolormesh(t, f, 1 - Sxx, shading = 'gouraud')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show();\n",
    "\n",
    "def spectrogram(data: Board, rate: int, channels: Clause, n_row: int = 5, n_col: int = 12):\n",
    "    sample = np.random.default_rng().integers(data.shape[0], size = n_row)\n",
    "\n",
    "    sample.sort()\n",
    "\n",
    "    plt.figure(figsize = (18, 2 * .48 * n_row))\n",
    "\n",
    "    freq = np.arange(1, rate >> 1)\n",
    "    pos  = 0\n",
    "    # extd = np.append([0, 1, 1], freq[-1])\n",
    "    \n",
    "    for k in sample:\n",
    "        for c in channels:\n",
    "            pos += 1\n",
    "            x   = normalized(data[c][k])\n",
    "            coefficients, _ = pywt.cwt(x, scales = freq, wavelet = 'cmor')\n",
    "\n",
    "            plt.subplot(n_row, n_col, pos)\n",
    "            plt.imshow(np.abs(coefficients), aspect = 'auto', cmap = 'jet') #, extent = extd\n",
    "            # plt.colorbar(label=\"Magnitude\")\n",
    "            # plt.ylabel(\"Scale\")\n",
    "            # plt.xlabel(\"Time\")\n",
    "            # plt.title(\"CWT of a Chirp Signal\")\n",
    "            plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_logMelSpectrogram(data, rate):\n",
    "    sns.heatmap(np.rot90(logMelSpectrogram(data, rate)), cmap = 'inferno')\n",
    "    \n",
    "    # loc, _ = plt.xticks()\n",
    "    # l      = np.round((loc - loc.min()) * len(data) / fe / loc.max(), 2), vmin = -6\n",
    "\n",
    "    # plt.xticks(loc, l)\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Frequency (Mel)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = [list(harmonic(x, bands_coeff).values()) for x in [df_train[c] for c in eeg_Chans]]\n",
    "\n",
    "np.shape(H), np.shape(np.stack(np.stack(H, axis = 1), axis = 2)), np.shape(H[0][1][0])\n",
    "# harmonic(trains['C3'][256], bands_coeff).values())), H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train.drop(columns = ['EventType']),\n",
    "                                                    df_train['EventType'], test_size = .2, random_state = 42)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split([v.tolist() for v in np.array(trains[eeg_Channels])],\n",
    "#                                                     trains['EventType'], test_size = .2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = np.array(X_train)\n",
    "# test_dataset  = X_test\n",
    "\n",
    "# train_dataset = structure(X_train, SAMPLE_RATE, eeg_Chans[:2])\n",
    "# test_dataset  = structure(X_test, SAMPLE_RATE, eeg_Chans[:2])\n",
    "\n",
    "# train_dataset = structure(X_train, SAMPLE_RATE, eeg_Chans[: 2])\n",
    "# test_dataset  = structure(X_test, SAMPLE_RATE, eeg_Chans[: 2])\n",
    "\n",
    "train_dataset = structure(df_train.drop(columns = ['EventType']), SAMPLE_RATE, eeg_Chans[: 2])\n",
    "test_dataset  = structure(df_test.drop(columns = ['EventType']), SAMPLE_RATE, eeg_Chans[: 2])\n",
    "\n",
    "# print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### • Test prédiction\n",
    "\n",
    "# UNITS    : int   = 100\n",
    "BATCHSIZE: int   = 32\n",
    "EPOCH    : int   = 1000\n",
    "ZERO     : int   = 32\n",
    "DROPOUT  : float = 1 / 4\n",
    "\n",
    "OPTIMIZER = 'AdamW'     # adamax, , adafactor, adam, nadam\n",
    "# kl_divergence mean_squared_logarithmic_error mean_absolute_error\n",
    "LOSS      = 'sparse_categorical_crossentropy'\n",
    "ACTIV     = ReLU   # PReLU, LeakyReLU\n",
    "K_SIZE    = (5)\n",
    "\n",
    "model = Sequential([\n",
    "    # - Couche 1 -\n",
    "    Conv1D(filters = ZERO, kernel_size = K_SIZE, dilation_rate = 2,\n",
    "           input_shape = train_dataset.shape),\n",
    "    ACTIV(),\n",
    "    MaxPooling1D(pool_size = 2, strides = 1),\n",
    "    Dropout(rate = DROPOUT),\n",
    "    # - Couche 2 -\n",
    "    Conv1D(filters = ZERO << 1, kernel_size = K_SIZE, dilation_rate = 2),\n",
    "    ACTIV(),\n",
    "    MaxPooling1D(pool_size = 2, strides = 1),\n",
    "    Dropout(rate = DROPOUT),\n",
    "    # - Couche 3 -\n",
    "    Conv1D(filters = ZERO << 2, kernel_size = K_SIZE, dilation_rate = 2),\n",
    "    ACTIV(),\n",
    "    MaxPooling1D(pool_size = 2, strides = 1),\n",
    "    Dropout(rate = DROPOUT),\n",
    "    # - Flatten layer -\n",
    "    Flatten(),\n",
    "    GlobalAveragePooling1D(),\n",
    "    # - Couches de sortie -\n",
    "    Dense(units = ZERO << 2),\n",
    "    ACTIV(),\n",
    "    Dropout(rate = DROPOUT),\n",
    "    Dense(units = len(hands_event), activation = 'softmax'), # sigmoid \n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer = OPTIMIZER, loss = LOSS, metrics = ['acc'])\n",
    "\n",
    "print()\n",
    "\n",
    "stop    = EarlyStopping(monitor = 'val_accuracy', mode = 'max', verbose = 1, patience = 50)\n",
    "history = model.fit(train_dataset, y_train, validation_data = (test_dataset, y_test), verbose = 1,\n",
    "                    batch_size = BATCHSIZE, epochs = EPOCH, callbacks = [stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_dataset)\n",
    "\n",
    "sum([np.where(x > .5)[0][0] for x in pred] == y_test) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values  = history_dict['loss']\n",
    "acc_values   = history_dict['accuracy']\n",
    "absc         = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.figure(figsize = (12, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(absc, loss_values, label = 'Loss')\n",
    "plt.plot(absc, acc_values, label = 'Accuracy')\n",
    "plt.title('Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(absc, history_dict['val_loss'], label = 'Loss')\n",
    "plt.plot(absc, history_dict['val_accuracy'], label = 'Accuracy')\n",
    "plt.title('Testing')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation densité spectrale du Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Densité Spectral du Signal\n",
    "\n",
    "plot_psd(entrants, train_runs, rate = SAMPLE_RATE, Channels = eeg_Chans, titled = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Densité spectrale / échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Densité spectral / échantillon\n",
    "\n",
    "inc   = 40\n",
    "scp = samples(train_samples, inc)\n",
    "boolInt = -2\n",
    "\n",
    "plt.figure(figsize = (15, inc * 1.5))\n",
    "\n",
    "for i in scp:\n",
    "    boolInt += 2\n",
    "\n",
    "    for c in eeg_Chans:\n",
    "        target = train_runs[0][c][i]\n",
    "        yest, Pxx_den = signal.welch(target, SAMPLE_RATE)   # , scaling = 'spectrum'\n",
    "        \n",
    "        plt.subplot(inc, 4, boolInt + 1)\n",
    "        plt.semilogy(yest, Pxx_den, label = c)\n",
    "        plt.title(f\"welch - {i + 1}\", fontsize = 11)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(inc, 4, boolInt + 2)\n",
    "        res, _ = plt.psd(target, Fs = SAMPLE_RATE, label = c) # , NFFT = NFFT\n",
    "        plt.title(f\"psd - {i + 1}\", fontsize = 11)\n",
    "        plt.xlabel('')\n",
    "        plt.ylabel('')\n",
    "        # plt.legend()\n",
    "\n",
    "plt.xlabel('frequency [Hz]')\n",
    "# plt.ylabel('PSD [V**2/Hz]')\n",
    "plt.tight_layout()\n",
    "plt.show();\n",
    "\n",
    "# f, Pxx_den = signal.welch(train_eras[0]['C3'][752], SAMPLE_RATE)\n",
    "\n",
    "# print(len(Pxx_den))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation Epoques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Epoques\n",
    "\n",
    "for i in range(len(files))[:: 3]:\n",
    "    plot_signal(entrants[i], parts[i], train_spots[0][i], train_spots[1][i], channels = eeg_Chans, # \n",
    "                period = CHUNK, lag = LAG, title = headers[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation décomposition des signaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Décomposition des signaux\n",
    "\n",
    "# Test de décomposition des signaux en bandes de fréquences spécifiques compatibles avec les répartitions usuelles\n",
    "# dans le domaine des EEG ['Delta', 'Theta', 'Alpha', 'Beta', 'Gamma']\n",
    "\n",
    "for input, token in zip(train_runs, ['Gauche', 'Droite']):\n",
    "    print(f\"Exemples - Évènement Discriminé Main {token}\")\n",
    "    plot_wavelets(input, bands_coeff, eeg_Chans, scope = 30, headers = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation des spectrogrammes (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in zip(numbers, ['Gauche', 'Droite']):\n",
    "    print(f\"Exemples - Évènement Discriminé Main {t}\")\n",
    "    spectrogram(train_runs[i], SAMPLE_RATE, eeg_Chans, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • MNE époque (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • MNE époque (test)\n",
    "\n",
    "raw_csv = entrants[0][eeg_Chans]\n",
    "info    = mne.create_info(ch_names = eeg_Chans, sfreq = SAMPLE_RATE, ch_types = 'eeg')\n",
    "raw_mne = mne.io.RawArray(raw_csv.T * 1e-6, info)\n",
    "sites     = np.where(entrants[0]['EventStart'] == 1)[0]\n",
    "\n",
    "# display(compare(np.sort(np.concatenate((train_spots[0][0], train_spots[1][0]))), loc))\n",
    "\n",
    "tmin, tmax = -0., 1\n",
    "\n",
    "# loc = mne.find_events(raw_mne, stim_channel = 'C3')\n",
    "# event_id = dict(C3 = 1, aud_r = 2, vis_l = 3, vis_r = 4)\n",
    "# raw = mne.io.Raw(raw_mne, preload = True)\n",
    "# raw.filter(2, None, method = 'iir')           # replace baselining with high-pass\n",
    "# events = mne.read_events(event_fname)\n",
    "\n",
    "# raw.info['bads'] = ['MEG 2443']  # set bad channels\n",
    "# picks = mne.pick_types(info, meg = 'grad', eeg = True, eog = False, exclude = 'bads')\n",
    "# Read epochs\n",
    "absc = mne.Epochs(raw_mne, np.array([sites, sites, sites]).T, None, tmin, tmax, proj = False,\n",
    "                    picks = None, baseline = None, preload = True, verbose = False) # event_id picks\n",
    "\n",
    "# labels = epochs.events[::5, -1]\n",
    "\n",
    "# events\n",
    "\n",
    "# raw_mne.plot();\n",
    "\n",
    "# raw_mne['C3'][0][0], len(df_train_csv[2]['Cz'])\n",
    "\n",
    "display(absc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test de classification - Proposition 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, target = bands_coeff['Theta']\n",
    "\n",
    "i = np.random.randint(np.shape(X)[0])\n",
    "target = df_train['C4'][i] - df_train['C3'][i]\n",
    "# X = [np.sign(s) for s in (trains['C4'] - trains['C3'])]\n",
    "# y = {band: bandpass_filter(X[i], b, a) for band, (b, a) in bands_coeff.items()}\n",
    "\n",
    "plt.figure()\n",
    "# plt.style.use('')\n",
    "\n",
    "plt.plot(normalized(bandpass_filter(target, b, target)), label = 'C4 - C3')\n",
    "plt.plot(normalized(bandpass_filter(np.sign(target), b, target)), label = '[C4 - C3]', c = np.random.rand(1, 3)[0])\n",
    "# plt.plot(pywt.dwt(y, wavelet = 'db4')[0], label = 'C4 - C3')\n",
    "# plt.plot(pywt.dwt(np.sign(y), wavelet = 'db4')[0], label = '[C4 - C3]')\n",
    "# plt.plot(np.zeros(512), ls = '--', c = np.random.rand(1, 3)[0])\n",
    "\n",
    "# print(np.shape(pywt.dwt(y, wavelet = 'db8')))\n",
    "\n",
    "# plt.plot(bandpass_filter(np.sign(trains['C3'][i]), b, a), label = 'C3')\n",
    "# plt.plot(bandpass_filter(np.sign(trains['C4'][i]), b, a), label = 'C4')\n",
    "# plt.plot(bandpass_filter(np.sign(trains['Cz'][i]), b, a), label = 'Cz')\n",
    "\n",
    "# for (band, signal) in reversed(y.items()):\n",
    "#     plt.plot(pd.Series(signal), label = f'{band}', c = np.random.rand(1, 3)[0])\n",
    "\n",
    "plt.title(f\"{i}\")\n",
    "plt.legend(loc = 'upper right')\n",
    "\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    ## K-plus proches voisins\n",
    "    'knn__n_neighbors': range(2),\n",
    "    ## SVM\n",
    "    'svm__C'     : [0.1, 1, 5],\n",
    "    'svm__kernel': ['linear', 'softmax', 'sigmoid', 'rbf'],\n",
    "    ## RandomForest\n",
    "    # 'rf__max_features'     : ['sqrt', 'log2', None],\n",
    "    # 'rf__min_samples_split': range(2, 32, 2),\n",
    "    # , ('rf', clf3), ('rf', clf3)\n",
    "    'estimators': [[('knn', knc), ('svm', svm)], [('knn', knc), ('svm', svm)]] \n",
    "    }\n",
    "\n",
    "grid = model_selection.GridSearchCV(estimator = Voting_clf, param_grid = params, cv = 5) \\\n",
    "    .fit(X_train_scaled, y_train)\n",
    "\n",
    "# parametres = {'max_features': ['log2', 'sqrt', None], 'min_samples_split': range(2, 32, 2)}\n",
    "\n",
    "# vclf = model_selection.GridSearchCV(estimator = clf3, param_grid = parametres, cv = 3) \\\n",
    "#     .fit(X_train_scaled, y_train)\n",
    "\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n",
    "print('score train:', grid.score(X_train_scaled, y_train))\n",
    "print('score test:', grid.score(X_test_scaled, y_test))\n",
    "\n",
    "# print(vclf.best_estimator_, vclf.best_params_, vclf.best_score_)\n",
    "# print('score train:', grid.score(X_train_scaled, y_train), vclf.score(X_train_scaled, y_train))\n",
    "# print('score test :', grid.score(X_test_scaled, y_test), vclf.score(X_test_scaled, y_test))\n",
    "\n",
    "df_train_cpy, event_start = fancy_df(entrants, targets['EventType'], hands_event, CHUNK)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize = (24, 5), sharey = True)\n",
    "sig = .05\n",
    "\n",
    "axes.plot(entrants['C3'])\n",
    "\n",
    "for p in event_start:\n",
    "    axes.axvspan(p[0] - (CHUNK >> 1), p[0] + 1.5 * CHUNK, facecolor = 'orangered', alpha = .5)\n",
    "\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolInt = 16\n",
    "start   = event_start[boolInt][0]\n",
    "entrant = start + CHUNK\n",
    "input   = df_train_cpy['C3_4'][start: entrant]\n",
    "smooth  = input.copy()\n",
    "inc     = 5\n",
    "alpha   = 1 / 3\n",
    "dec     = int(inc / alpha)\n",
    "\n",
    "plt.figure(figsize = (24, 5))\n",
    "plot_window(entrants, ['C3', 'C4', 'C3 + C4'], start, CHUNK)\n",
    "\n",
    "# Lissage des hautes fréquences\n",
    "for _ in range(inc):\n",
    "    smooth = simple_exponential_smoothing(smooth, alpha, 0)\n",
    "\n",
    "smooth = pd.Series(index = range(start, entrant + inc - dec), data = smooth[dec:])\n",
    "\n",
    "# plt.plot(raw - smooth, label = hands[event_start[pos][1]])\n",
    "plt.plot(smooth, '--', label = hands_event[event_start[boolInt][1]])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Apendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"text-align:center\" ><b>EEG</b> - Prédiction des Mouvements Imaginaires de la Main</h2>\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Le projet**\n",
    "- Intoduction  \n",
    "https://github.com/DataScientest-Studio/mar24_cds_eeg/blob/eric/references/Description_projet_EEG.pdf  \n",
    "https://www.bbci.de/competition/iv/desc_2b.pdf\n",
    "- Ressources / Données   \n",
    "https://www.kaggle.com/competitions/ucsd-neural-data-challenge/overview  \n",
    "- Bibliographie  \n",
    "https://www.bbci.de/competition/iv/desc_2b.pdf\n",
    "#### **2. Liens utils**\n",
    "- SciPy - *open-source software for mathematics, science, and engineering*  \n",
    "https://docs.scipy.org/doc/scipy/index.html  \n",
    "https://docs.scipy.org/doc/scipy/reference/signal.html  \n",
    "- MNE - *MEG + EEG Analysis & Visualisation*\n",
    "   - Accueil  \n",
    "   https://mne.tools/stable/index.html\n",
    "\n",
    "   - MNE - Data structures from arbitrary data  \n",
    "   https://mne.tools/stable/auto_tutorials/io/10_reading_meg_data.html#creating-mne-data-structures-from-arbitrary-data-from-memory\n",
    "   \n",
    "   - MNE - EEG Preprocessing  \n",
    "   https://mne.tools/dev/auto_tutorials/preprocessing/index.html  \n",
    "\n",
    "- pyRiemann - *Biosignals classification with Riemannian geometry*  \n",
    "https://pyriemann.readthedocs.io/en/latest/  \n",
    "- neurodsp - *Neuro Digital Signal Processing Toolbox*  \n",
    "https://neurodsp-tools.github.io/neurodsp/index.html#\n",
    "- Rythme Mu  \n",
    "https://fr.wikipedia.org/wiki/Rythme_Mu\n",
    "- Spectrogram from EEG  \n",
    "https://www.kaggle.com/code/cdeotte/how-to-make-spectrogram-from-eeg\n",
    "- Divers  \n",
    "https://signalprocessingsociety.org/  \n",
    "https://fr.wikipedia.org/wiki/Filtre_de_Butterworth  \n",
    "https://fr.wikipedia.org/wiki/Moyenne_mobile  \n",
    "https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html  \n",
    "https://perso.etis-lab.fr/ghaffari/2014_CCMB_Floride_USA.pdf  \n",
    "https://www.youtube.com/watch?v=wB417SAbdak&list=PLXc9qfVbMMN2TAoLHVW5NvNmJtwiHurzw  \n",
    "https://fastercapital.com/fr/sujet/identification-des-artefacts-de-traitement-du-signal-dans-des-sc%C3%A9narios-r%C3%A9els.html#:~:text=L'inspection%20visuelle%20est%20la,des%20pertes%20et%20du%20bruit.  \n",
    "   - Z-Score Normalisation  \n",
    "   https://fr.wikipedia.org/wiki/Cote_Z_(statistiques)  \n",
    "   https://typeset.io/questions/why-is-z-score-normalisation-necessary-in-pre-processing-eeg-1xv5jepyq5  \n",
    "\n",
    "   - Traitement numérique du signal  \n",
    "   https://fr.wikipedia.org/wiki/Traitement_num%C3%A9rique_du_signal  \n",
    "   - Ondelette  \n",
    "      - Wiki  \n",
    "      https://fr.wikipedia.org/wiki/Ondelette  \n",
    "\n",
    "      - L’analyse par ondelettes dans la vie de tous les jours  \n",
    "      https://interstices.info/lanalyse-par-ondelettes-dans-la-vie-de-tous-les-jours/  \n",
    "\n",
    "      - A guide for using the Wavelet Transform in Machine Learning  \n",
    "      https://ataspinar.com/2018/12/21/a-guide-for-using-the-wavelet-transform-in-machine-learning/\n",
    "      \n",
    "      - pyWavelets - *open source wavelet transform*  \n",
    "      https://pywavelets.readthedocs.io/en/latest/\n",
    "\n",
    "      - Ondelettes et applications  \n",
    "      https://www.i2m.univ-amu.fr/~caroline.chaux/GEOMDATA/TI-te5215.pdf\n",
    "\n",
    "   - Maximum de vraisemblance  \n",
    "   https://pmarchand1.github.io/ECL8202/notes_cours/03-Maximum_vraisemblance.html  \n",
    "   https://fr.wikipedia.org/wiki/Maximum_de_vraisemblance#:~:text=En%20statistique%2C%20l'estimateur%20du,maximisant%20la%20fonction%20de%20vraisemblance  \n",
    "\n",
    "   - Transformation de Fourier discrète  \n",
    "   https://fr.wikipedia.org/wiki/Transformation_de_Fourier_discr%C3%A8te  \n",
    "      - La Transformation de Fourier n’est pas adaptée à l’analyse des signaux non stationnaires.\n",
    "   - Neural Data Science in Python  \n",
    "   https://neuraldatascience.io/intro.html\n",
    "\n",
    "   - Preprocessing of EEG  \n",
    "   https://www.frontiersin.org/articles/10.3389/fninf.2015.00016/full#:~:text=The%20depositable%20preprocessing%20pipeline%20consists,with%20a%20low%20recording%20SNR  \n",
    "   https://typeset.io/papers/preprocessing-of-eeg-4go8vhcbty  \n",
    "   https://learn.neurotechedu.com/preprocessing  \n",
    "   https://g0rella.github.io/gorella_mwn/preprocessing_eeg.html  \n",
    "   \n",
    "   - Biblio:  \n",
    "   https://perso.telecom-paristech.fr/bloch/P6Image/ondelettestrsp.pdf  \n",
    "   https://www.math.u-bordeaux.fr/~jbigot/Site/Enseignement_files/ondelettesIMAT.pdf  \n",
    "   http://w3.cran.univ-lorraine.fr/perso/radu.ranta/pdf/cours_deb_ond%28fr%29.pdf\n",
    "   \n",
    "   - Digital Filtering  \n",
    "   http://notebooks.pluxbiosignals.com/notebooks/Categories/Pre-Process/digital_filtering_eeg_rev.html\n",
    "\n",
    "   - Processus stationnaire  \n",
    "   https://fr.wikipedia.org/wiki/Processus_stationnaire\n",
    "\n",
    "   - Analyse en composantes principales  \n",
    "   https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales#:~:text=L'ACP%2C%20d%C3%A9sign%C3%A9e%20en%20g%C3%A9n%C3%A9ral,une%20grandeur%20physique%2C%20comme%20les"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
