{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Chargement des différentes librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Chargement des différentes librairies\n",
    "\n",
    "import sys, os #, math, time\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "from src.thot.sesh import *\n",
    "from src.thot.catch_features import *\n",
    "\n",
    "# from scipy.fft import fft\n",
    "import pywt, librosa            # type: ignore\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "from sklearn import model_selection, preprocessing as sk_p      # type: ignore\n",
    "# Supervised learning\n",
    "from sklearn import ensemble, svm, linear_model, neighbors      # type: ignore\n",
    "# Unsupervised learning\n",
    "# from sklearn import cluster\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.pipeline import Pipeline                           # type: ignore\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler  # type: ignore\n",
    "from sklearn.decomposition import PCA                           # type: ignore\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import VotingClassifier, GradientBoostingClassifier\n",
    "\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate # type: ignore\n",
    "from sklearn.metrics import classification_report               # type: ignore\n",
    "\n",
    "from keras.models import Sequential                             # type: ignore\n",
    "from keras.callbacks import EarlyStopping                       # type: ignore\n",
    "from keras.layers import GlobalAveragePooling1D, MaxPooling1D   # type: ignore\n",
    "from keras.layers import ReLU, Dense, Dropout, Conv1D, Flatten  # type: ignore\n",
    "# from keras.layers import LSTM, LeakyReLU, PReLU, ConvLSTM1D\n",
    "# from keras.layers import Bidirectional, TimeDistributed, RepeatVector, Flatten\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.optimizers import AdamW, Adam            # type: ignore\n",
    "\n",
    "# from pyriemann.spatialfilters import CSP\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Déclaration de constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 512)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### • Déclaration de constantes\n",
    "\n",
    "# Fréquence d'échantillonnage - Hz (Nombre de valeur / sec)\n",
    "SAMPLE_RATE = 250\n",
    "#\n",
    "PW2 : int   = int(np.floor(np.log2(SAMPLE_RATE))) # 2 << SAMPLE_RATE // 32\n",
    "#\n",
    "NFFT : int  = 1 << PW2\n",
    "# Temps additionel pour étendre le domaines d'étude.\n",
    "LAG : int   = 0 # SAMPLE_RATE >> 2 # -62     # Décalage du signal dû signal ~250ms\n",
    "# Epoque en sec donnée en nombre d'échantillon consectutif # 4\" de données (multiple de 2)\n",
    "CHUNK : int = ceil_pow2(SAMPLE_RATE) << 1 # SAMPLE_RATE << 1 # (1 << PW2) * 4\n",
    "# Taille des lots\n",
    "BATCH : int = CHUNK\n",
    "\n",
    "# Deux enregistrements bipolaires + neutre\n",
    "eeg_Chans   = ['C3', 'C4', 'Cz']\n",
    "# Liste des cannaux eeg associés aux évènement 0 et 1\n",
    "eeg_left    = [f'{c}_0' for c in eeg_Chans]\n",
    "eeg_right   = [f'{c}_1' for c in eeg_Chans]\n",
    "full_eeg    = eeg_left + eeg_right\n",
    "# Trois enregistrements musculaires\n",
    "ecg_Chans   = ['EOG:ch01', 'EOG:ch02', 'EOG:ch03']\n",
    "# Liste de tous les cannaux des dataframes\n",
    "all_chans   = eeg_Chans + ecg_Chans\n",
    "# Correspondance pour la classification\n",
    "hands_event = {0: 'Left', 1: 'Right'}\n",
    "# Nombre dévènement à prédire\n",
    "numbers     = range(len(hands_event))\n",
    "# Les bandes de fréquences d'intérêt\n",
    "eeg_bands   = {'Delta' : (.1, 4),\n",
    "               'Theta' : (4, 8),\n",
    "               'Alpha' : (8, 14),\n",
    "               'Beta'  : (14, 31),\n",
    "               'Gamma' : (31, (SAMPLE_RATE >> 2) - 1),}\n",
    "# Coefficients pour filtres Butterworth numérique d'ordre N pour le filtrage passe-bande\n",
    "bands_coeff = {band : butter_bandpass(low, high, SAMPLE_RATE) for band, (low, high) in eeg_bands.items()}\n",
    "# Largeur de bande retenue pour étude de cas\n",
    "band_interest = butter_bandpass(1e-3, eeg_bands['Alpha'][1], SAMPLE_RATE)\n",
    "\n",
    "LAG, CHUNK,  #, *band_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_struct(data : Board, event : int, col : str) -> Board :\n",
    "    room = data[data[col] == event]\n",
    "    b, a = bands_coeff['Delta']\n",
    "    # b, a = band_interest[0], band_interest[1]\n",
    "\n",
    "    vals = [normalized(bandpass_filter(v4 - v3, b, a)) for v3, v4 in zip(room['C4'], room['C3'])]\n",
    "    # vals = [normalized(v4 - v3) for v3, v4 in zip(room['C4'], room['C3'])]\n",
    "    # vals = [bandpass_filter(v4  - v3, b, a)\n",
    "    #         for v3, v4 in zip(room['C4'], room['C3'])]\n",
    "    # vals = [normalized(v4  - v3) for v3, v4 in zip(room['C4'], room['C3'])]\n",
    "    # vals = [v4  - v3 for v3, v4 in zip(room['C4'], room['C3'])]\n",
    "    \n",
    "    # _df = pd.DataFrame([*np.subtract(room['C4'], room['C3'])])\n",
    "    # _df = pd.DataFrame([normalized(v4) - normalized(v3) for v3, v4 in zip(room['C4'], room['C3'])])\n",
    "    _df = pd.DataFrame(vals)\n",
    "\n",
    "    _df[col] = event\n",
    "    \n",
    "    return _df\n",
    "\n",
    "#### • Try catch_22\n",
    "\n",
    "def catch(data : Board, col : str, channels : Clause, event : int,\n",
    "            norm : bool = False) -> Board :\n",
    "    func = [np.min, np.max, np.median]\n",
    "    #, np.mean func = [np.min, np.max, np.std, np.var, np.mean, np.median]\n",
    "    # func = np.append(func, catch_)\n",
    "    room = data[data[col] == event]\n",
    "    name = [f.__name__ for f in func]\n",
    "    # head = [f\"{f}_diff\" for f in name]\n",
    "    head = [f\"{c}_{f}\" for c in channels for f in name] \\\n",
    "         + [f\"{f}_diff\" for f in name]\n",
    "    \n",
    "    if norm :\n",
    "        temp = [[normalized(x) for x in room[c]] for c in channels]\n",
    "        c3, c4 = [[[f(v) for v in s] for f in func] for s in temp]\n",
    "    else :\n",
    "        c3, c4 = [[[f(v) for v in room[c]] for f in func] for c in channels]\n",
    "    \n",
    "    sub = np.subtract(c3, c4)\n",
    "    _df = pd.DataFrame(np.array((*c3, *c4, *sub)).T, columns = head)\n",
    "    # _df = pd.DataFrame(np.array(sub).T, columns = head)\n",
    "\n",
    "    print(np.array((np.stack([c3, c4], axis= 1), *sub)))\n",
    "\n",
    "    # display(_df)\n",
    "    \n",
    "    _df[col] = event\n",
    "    \n",
    "    return _df\n",
    "\n",
    "def pool_(data : Board, event : int) -> Board :\n",
    "    return catch(data, 'EventType', eeg_Chans[: -1], event, norm = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Acquisition des données d'entrainement et de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquisition des données d'entrainement\n",
    "working  = \"../data/data.zip\"\n",
    "size     = len('train/')\n",
    "files    = [x[size :] for x in files_in_zip(working, directory = 'train')]\n",
    "# Acquisition des fichiers du répertoir dans le fichier zip\n",
    "entrants = csv_in_zip(working, directory = 'train', files = files)\n",
    "targets  = csv_in_zip(working, directory = 'y_train_only', files = files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Acquisition des données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = csv_in_zip(working, directory = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Pré-traitement (Segmentation) des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers d'entrainements :\n",
      "  B0103T.csv B0203T.csv B0303T.csv B0403T.csv B0503T.csv B0603T.csv B0703T.csv B0803T.csv B0903T.csv\n",
      "\n",
      "Fichiers tests :\n",
      "  B0101T.csv B0102T.csv B0201T.csv B0202T.csv B0301T.csv B0302T.csv B0401T.csv B0402T.csv B0501T.csv B0502T.csv B0601T.csv B0602T.csv B0701T.csv B0702T.csv B0801T.csv B0802T.csv B0901T.csv B0902T.csv\n",
      "\n",
      "(2, 3, 720, 512)\n",
      "(2, 3, 1080, 512)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y, test_X, test_y     = train_test_init(entrants, targets, files, methode = -1)\n",
    "train_runs, train_spots, train_parts = split_and_merge(train_X, train_y, eeg_Chans, numbers, CHUNK, LAG)\n",
    "test_runs, test_spots, test_parts    = split_and_merge(test_X, test_y, eeg_Chans, numbers, CHUNK, LAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ CUSTOME 'TRAIN_TEST_SPLIT' -------\n",
      "---------------- TRAIN ------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1440 entries, 93 to 249\n",
      "Columns: 513 entries, 0 to EventType\n",
      "dtypes: float64(512), int64(1)\n",
      "memory usage: 5.6 MB\n",
      "None\n",
      "----------------- TEST ------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2160 entries, 786 to 1362\n",
      "Columns: 513 entries, 0 to EventType\n",
      "dtypes: float64(512), int64(1)\n",
      "memory usage: 8.5 MB\n",
      "None\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df_trains = pd.concat(train_runs, ignore_index = True)\n",
    "df_test = pd.concat(test_runs, ignore_index = True)\n",
    "X_train = pd.concat((simple_struct(df_trains, i, 'EventType') for i in numbers), ignore_index = True)\n",
    "X_test  = pd.concat((simple_struct(df_test, i, 'EventType') for i in numbers), ignore_index = True)\n",
    "# Pour éviter les biais d'apprentissage\n",
    "X_train = X_train.sample(frac = 1)\n",
    "X_test  = X_test.sample(frac = 1)\n",
    "\n",
    "print(titre(\"Custome 'train_test_split'\", 40))\n",
    "print(titre('train', 40))\n",
    "print(X_train.info())\n",
    "print(titre('test', 40))\n",
    "print(X_test.info())\n",
    "print('-' * 40)\n",
    "\n",
    "y_train = X_train['EventType']\n",
    "y_test  = X_test ['EventType']\n",
    "\n",
    "X_train.drop(columns = ['EventType'], inplace = True)\n",
    "X_test.drop(columns = ['EventType'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test de classification - Proposition 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• RandomForestClassifier(n_jobs=-1) : Accuracy -> 70.1% (±0.028, max : 74.0%)\n",
      "\t-> Classification report [ Test-score / : 60.1% ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.603     0.589     0.596      1080\n",
      "           1      0.599     0.613     0.606      1080\n",
      "\n",
      "    accuracy                          0.601      2160\n",
      "   macro avg      0.601     0.601     0.601      2160\n",
      "weighted avg      0.601     0.601     0.601      2160\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prono</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vrai</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>636</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>418</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Prono    0    1\n",
       "Vrai           \n",
       "0      636  444\n",
       "1      418  662"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• SVC(kernel='poly') : Accuracy -> 71.0% (±0.018, max : 72.9%)\n",
      "\t-> Classification report [ Test-score / : 61.0% ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.622     0.563     0.591      1080\n",
      "           1      0.601     0.657     0.628      1080\n",
      "\n",
      "    accuracy                          0.610      2160\n",
      "   macro avg      0.611     0.610     0.609      2160\n",
      "weighted avg      0.611     0.610     0.609      2160\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prono</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vrai</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>608</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>370</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Prono    0    1\n",
       "Vrai           \n",
       "0      608  472\n",
       "1      370  710"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• LogisticRegression(solver='newton-cholesky') : Accuracy -> 72.4% (±0.034, max : 76.0%)\n",
      "\t-> Classification report [ Test-score / : 62.7% ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.636     0.592     0.613      1080\n",
      "           1      0.619     0.662     0.640      1080\n",
      "\n",
      "    accuracy                          0.627      2160\n",
      "   macro avg      0.627     0.627     0.626      2160\n",
      "weighted avg      0.627     0.627     0.626      2160\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prono</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vrai</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>639</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>365</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Prono    0    1\n",
       "Vrai           \n",
       "0      639  441\n",
       "1      365  715"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# csp = CSP(nfilter = 2)\n",
    "# std = RobustScaler()\n",
    "pca = PCA(CHUNK >> 4)\n",
    "std = StandardScaler()\n",
    "\n",
    "# X_train_scaled = std.fit_transform(X_train)\n",
    "# X_test_scaled  = std.transform(X_test)\n",
    "\n",
    "# X_train_scaled = pca.fit_transform(X_train)\n",
    "# X_test_scaled  = pca.transform(X_test) \n",
    "\n",
    "# print(*pca.singular_values_.shape)\n",
    "\n",
    "X_train_scaled = X_train\n",
    "X_test_scaled  = X_test\n",
    "\n",
    "#\n",
    "clf = svm.SVC(gamma = 'scale', kernel = 'poly')\n",
    "# \n",
    "rfc = ensemble.RandomForestClassifier(n_jobs = -1)\n",
    "#\n",
    "lrg = linear_model.LogisticRegression(solver = 'newton-cholesky') # 'saga' ''\n",
    "# #\n",
    "# knc = neighbors.KNeighborsClassifier() # .RadiusNeighborsRegressor()\n",
    "# #\n",
    "# lsg = linear_model.SGDClassifier()\n",
    "# # \n",
    "# gbc = ensemble.GradientBoostingClassifier()\n",
    "# # \n",
    "# nnp = MLPClassifier(solver = 'sgd', learning_rate = 'invscaling')\n",
    "# # \n",
    "# kmn = cluster.KMeans(n_clusters = 2, algorithm='elkan')\n",
    "\n",
    "for reg in [rfc, clf, lrg] :\n",
    "    scores : dict = cross_validate(reg, X_train_scaled, y_train, scoring = ['accuracy'])\n",
    "    \n",
    "    reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "    res  = scores['test_accuracy']\n",
    "    pred = reg.predict(X_test_scaled)\n",
    "    \n",
    "    print(f\"• {reg} : Accuracy -> {res.mean():.1%} (±{res.std():.2}, max : {res.max():.1%})\")\n",
    "    print(f\"\\t-> Classification report [ Test-score / : {reg.score(X_test_scaled, y_test):.1%} ]\")\n",
    "    print(classification_report(y_test, pred, digits = 3))\n",
    "    # print(f\"\\t-> ● Accuracy score : {accuracy_score(y_test, pred):.1%}\")\n",
    "    display(pd.crosstab(y_test, pred, rownames = ['Vrai'], colnames = ['Prono']))\n",
    "\n",
    "#ExtraTreesClassifier \n",
    "# Voting_clf = VotingClassifier(estimators = [('knn', clf1), ('svm', clf2), ('rf', clf3)], voting = 'hard')\n",
    "# cv3        = model_selection.KFold(n_splits = 3, random_state = 42, shuffle = True), clf4\n",
    "\n",
    "# Create a pipeline\n",
    "# pip = Pipeline([('RFC', rfc), ('SVM', clf)])    # ('CSP', csp), \n",
    "\n",
    "# pip.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = pip.predict(X_test)\n",
    "\n",
    "# y_train = np.array(y_train)\n",
    "\n",
    "    # if 'cluster' in type(reg).__name__ :\n",
    "    #     pred = reg.fit_predict(X_test_scaled)\n",
    "    # else :\n",
    "    #     pred = reg.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://raphaelvallat.com/bandpower.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test EEG Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch                                        # type: ignore\n",
    "\n",
    "from torchvision import datasets                    # type: ignore\n",
    "from torch.utils.data import Dataset, DataLoader    # type: ignore\n",
    "from torchvision.transforms import ToTensor         # type: ignore\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/banggiangle/cnn-eeg-pytorch\n",
    "\n",
    "def resample_data(gt, chunk_size : int = CHUNK) :\n",
    "    \"\"\"\n",
    "    split long signals to smaller chunks, discard no-events chunks  \n",
    "    \"\"\"\n",
    "    total_discard_chunks = 0\n",
    "    threshold = 1e-2\n",
    "    mean_val = []\n",
    "    index = []\n",
    "    \n",
    "    for i in range(len(gt)) :\n",
    "        for j in range(0, gt[i].shape[1], chunk_size):\n",
    "            mn = min(gt[i].shape[1], j + chunk_size)\n",
    "\n",
    "            mean_val.append(np.mean(gt[i][:, j : mn]))\n",
    "\n",
    "            if mean_val[-1] < threshold :  # discard chunks with low events time\n",
    "                total_discard_chunks += 1\n",
    "            else :\n",
    "                index.extend([(i, k) for k in range(j, mn)]) # min(gt[i].shape[1], j + chunk_size)\n",
    "\n",
    "    nb = len(mean_val)\n",
    "\n",
    "    plt.plot([0, nb], [threshold, threshold], color = 'r')\n",
    "    plt.scatter(range(nb), mean_val, s = 1)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Total number of chunks discarded: {total_discard_chunks} chunks\")\n",
    "    print(f\"{total_discard_chunks / len(mean_val)}% data\")\n",
    "\n",
    "    del mean_val\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return index\n",
    "\n",
    "%time\n",
    "class EEGSignalDataset(Dataset) :\n",
    "    def __init__(self, data, gt, m = m, s =s , soft_label = True, train = True) :\n",
    "        self.data   = data\n",
    "        self.gt     = gt\n",
    "        self.train  = train\n",
    "        self.soft_label = soft_label\n",
    "        self.eps = 1e-7\n",
    "        \n",
    "        self.index = resample_data(gt) if train else \\\n",
    "            [(i, j) for i in range(len(data)) for j in range(data[i].shape[1])]\n",
    "\n",
    "        for dt in self.data :\n",
    "            dt -= m\n",
    "            dt /= s + self.eps\n",
    "    \n",
    "    def __getitem__(self, i) :\n",
    "        i, j     = self.index[i]\n",
    "        raw_data = self.data[i][:, max(0, j + 1 - opt.in_len) : j + 1]\n",
    "        label    = self.gt[i][:, j]\n",
    "        \n",
    "        pad      = opt.in_len - raw_data.shape[1]\n",
    "\n",
    "        if pad :\n",
    "            raw_data = np.pad(raw_data, ((0, 0), (pad, 0)), 'constant', constant_values = 0)\n",
    "\n",
    "        raw_data = torch.from_numpy(raw_data.astype(np.float32))\n",
    "        label    = torch.from_numpy(label.astype(np.float32))\n",
    "\n",
    "        if self.soft_label : label[label < .02] = .02\n",
    "\n",
    "        return raw_data, label\n",
    "\n",
    "    def __len__(self) : return len(self.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ CUSTOME 'TRAIN_TEST_SPLIT' -------\n",
      "Torseur train\t: 2 720 3 512\n",
      "Torseur test\t: 2 1080 3 512\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_parts = torch_split(train_X, train_y, eeg_Chans, numbers, CHUNK, LAG)\n",
    "test_parts  = torch_split(test_X, test_y, eeg_Chans, numbers, CHUNK, LAG)\n",
    "\n",
    "print(titre(\"Custome 'train_test_split'\", 40))\n",
    "print('Torseur train\\t:', *train_parts.shape)\n",
    "print('Torseur test\\t:', *test_parts.shape)\n",
    "print('-' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation des spectrogrammes / Test\n",
    "\n",
    "def logMelSpectrogram(data : Vector, rate : int, dt : float = 1e-2) -> Vector :\n",
    "    tps = 1 << int(np.floor(np.log2(rate * dt)))\n",
    "    # print(tps)\n",
    "    # Spectrogramme\n",
    "    stfts = np.abs(librosa.stft(y = data, n_fft = tps, hop_length = 1 << 2, center = True)).T\n",
    "    # Filtre de MEL\n",
    "    liny  = librosa.filters.mel(sr = rate, n_fft = tps + 1, n_mels = stfts.shape[-1]).T\n",
    "    # Application du filtre au spectrogramme\n",
    "    mel_  = np.tensordot(stfts, liny, 1)\n",
    "\n",
    "    return np.log(mel_ + 1e-6)\n",
    "    \n",
    "def structure(data : Board | Vector, rate : int, whr : Clause) -> Vector :\n",
    "    # return np.array([logMelSpectrogram(X, rate, 2) for X in data[whr]])\n",
    "    # return np.stack([[signal.welch(X, rate)[1] for X in data[c]] for c in whr], axis = 2)\n",
    "    return np.stack(data['C4'] - data['C3'], axis = 0)\n",
    "    # return np.stack([[X for X in data[c]] for c in whr], axis = 2)\n",
    "\n",
    "def img_spectrogram(raw : Vector, rate : int, nfft : int = 1 << 10) -> Vector :\n",
    "    return librosa.feature.melspectrogram(y = raw, sr = rate, hop_length = 1, \n",
    "                            n_fft = nfft, n_mels = 32, fmin = 0, fmax = 20, win_length = 32)\n",
    "\n",
    "def spectrogram_dep(data : Board, rate : int, channels : Clause, n_row : int = 5, n_col : int = 12) :\n",
    "    sample = np.random.default_rng().integers(data.shape[0], size = n_row)\n",
    "\n",
    "    sample.sort()\n",
    "\n",
    "    plt.figure(figsize = (18, 2 * .48 * n_row))\n",
    "\n",
    "    pos = 0\n",
    "\n",
    "    for k in sample :\n",
    "        for c in channels :\n",
    "            # x   = normalized(data[c][k])\n",
    "            raw = img_spectrogram(raw = data[c][k], rate = rate)\n",
    "            pos += 1\n",
    "            \n",
    "            plt.subplot(n_row, n_col, pos)\n",
    "            plt.title(f\"{((pos - 1) // 3) + 1} . {k} - {c}\", fontsize = 8)\n",
    "            librosa.display.specshow(data = 1 - raw, sr = rate, hop_length = 1)\n",
    "            \n",
    "            # pos = n_col * (i >> 1) + j\n",
    "            # f, t, Sxx = signal.spectrogram(x, rate)\n",
    "            # plt.subplot(n_row, n_col, pos + 4)\n",
    "            # plt.pcolormesh(t, f, 1 - Sxx, shading = 'gouraud')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show();\n",
    "\n",
    "def spectrogram(data : Board, rate : int, channels : Clause, n_row : int = 5, n_col : int = 12) :\n",
    "    sample = np.random.default_rng().integers(data.shape[0], size = n_row)\n",
    "\n",
    "    sample.sort()\n",
    "\n",
    "    plt.figure(figsize = (18, 2 * .48 * n_row))\n",
    "\n",
    "    freq = np.arange(1, rate >> 1)\n",
    "    pos  = 0\n",
    "    # extd = np.append([0, 1, 1], freq[-1])\n",
    "    \n",
    "    for k in sample :\n",
    "        for c in channels :\n",
    "            pos += 1\n",
    "            x   = normalized(data[c][k])\n",
    "            coefficients, _ = pywt.cwt(x, scales = freq, wavelet = 'cmor')\n",
    "\n",
    "            plt.subplot(n_row, n_col, pos)\n",
    "            plt.imshow(np.abs(coefficients), aspect = 'auto', cmap = 'jet') #, extent = extd\n",
    "            # plt.colorbar(label=\"Magnitude\")\n",
    "            # plt.ylabel(\"Scale\")\n",
    "            # plt.xlabel(\"Time\")\n",
    "            # plt.title(\"CWT of a Chirp Signal\")\n",
    "            plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_logMelSpectrogram(data, rate) :\n",
    "    sns.heatmap(np.rot90(logMelSpectrogram(data, rate)), cmap = 'inferno')\n",
    "    \n",
    "    # loc, _ = plt.xticks()\n",
    "    # l      = np.round((loc - loc.min()) * len(data) / fe / loc.max(), 2), vmin = -6\n",
    "\n",
    "    # plt.xticks(loc, l)\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Frequency (Mel)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = [list(harmonic(x, bands_coeff).values()) for x in [df_trains[c] for c in eeg_Chans]]\n",
    "\n",
    "np.shape(H), np.shape(np.stack(np.stack(H, axis = 1), axis = 2)), np.shape(H[0][1][0])\n",
    "# harmonic(trains['C3'][256], bands_coeff).values())), H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_trains.drop(columns = ['EventType']),\n",
    "                                                    df_trains['EventType'], test_size = .2, random_state = 42)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split([v.tolist() for v in np.array(trains[eeg_Channels])],\n",
    "#                                                     trains['EventType'], test_size = .2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = np.array(X_train)\n",
    "# test_dataset  = X_test\n",
    "\n",
    "# train_dataset = structure(X_train, SAMPLE_RATE, eeg_Chans[:2])\n",
    "# test_dataset  = structure(X_test, SAMPLE_RATE, eeg_Chans[:2])\n",
    "\n",
    "# train_dataset = structure(X_train, SAMPLE_RATE, eeg_Chans[: 2])\n",
    "# test_dataset  = structure(X_test, SAMPLE_RATE, eeg_Chans[: 2])\n",
    "\n",
    "train_dataset = structure(df_trains.drop(columns = ['EventType']), SAMPLE_RATE, eeg_Chans[: 2])\n",
    "test_dataset  = structure(df_test.drop(columns = ['EventType']), SAMPLE_RATE, eeg_Chans[: 2])\n",
    "\n",
    "# print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### • Test prédiction\n",
    "\n",
    "# UNITS     : int   = 100\n",
    "BATCHSIZE : int   = 32\n",
    "EPOCH     : int   = 1000\n",
    "ZERO      : int   = 32\n",
    "DROPOUT   : float = 1 / 4\n",
    "\n",
    "OPTIMIZER = 'AdamW'     # adamax, , adafactor, adam, nadam\n",
    "# kl_divergence mean_squared_logarithmic_error mean_absolute_error\n",
    "LOSS      = 'sparse_categorical_crossentropy'\n",
    "ACTIV     = ReLU   # PReLU, LeakyReLU\n",
    "K_SIZE    = (5)\n",
    "\n",
    "model = Sequential([\n",
    "    # - Couche 1 -\n",
    "    Conv1D(filters = ZERO, kernel_size = K_SIZE, dilation_rate = 2,\n",
    "           input_shape = train_dataset.shape),\n",
    "    ACTIV(),\n",
    "    MaxPooling1D(pool_size = 2, strides = 1),\n",
    "    Dropout(rate = DROPOUT),\n",
    "    # - Couche 2 -\n",
    "    Conv1D(filters = ZERO << 1, kernel_size = K_SIZE, dilation_rate = 2),\n",
    "    ACTIV(),\n",
    "    MaxPooling1D(pool_size = 2, strides = 1),\n",
    "    Dropout(rate = DROPOUT),\n",
    "    # - Couche 3 -\n",
    "    Conv1D(filters = ZERO << 2, kernel_size = K_SIZE, dilation_rate = 2),\n",
    "    ACTIV(),\n",
    "    MaxPooling1D(pool_size = 2, strides = 1),\n",
    "    Dropout(rate = DROPOUT),\n",
    "    # - Flatten layer -\n",
    "    Flatten(),\n",
    "    GlobalAveragePooling1D(),\n",
    "    # - Couches de sortie -\n",
    "    Dense(units = ZERO << 2),\n",
    "    ACTIV(),\n",
    "    Dropout(rate = DROPOUT),\n",
    "    Dense(units = len(hands_event), activation = 'softmax'), # sigmoid \n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer = OPTIMIZER, loss = LOSS, metrics = ['acc'])\n",
    "\n",
    "print()\n",
    "\n",
    "stop    = EarlyStopping(monitor = 'val_accuracy', mode = 'max', verbose = 1, patience = 50)\n",
    "history = model.fit(train_dataset, y_train, validation_data = (test_dataset, y_test), verbose = 1,\n",
    "                    batch_size = BATCHSIZE, epochs = EPOCH, callbacks = [stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_dataset)\n",
    "\n",
    "sum([np.where(x > .5)[0][0] for x in pred] == y_test) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values  = history_dict['loss']\n",
    "acc_values   = history_dict['accuracy']\n",
    "absc         = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.figure(figsize = (12, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(absc, loss_values, label = 'Loss')\n",
    "plt.plot(absc, acc_values, label = 'Accuracy')\n",
    "plt.title('Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(absc, history_dict['val_loss'], label = 'Loss')\n",
    "plt.plot(absc, history_dict['val_accuracy'], label = 'Accuracy')\n",
    "plt.title('Testing')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation densité spectrale du Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Densité Spectral du Signal\n",
    "\n",
    "plot_psd(entrants, train_runs, rate = SAMPLE_RATE, Channels = eeg_Chans, titled = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Densité spectrale / échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Densité spectral / échantillon\n",
    "\n",
    "inc   = 40\n",
    "scp = samples(train_samples, inc)\n",
    "boolInt = -2\n",
    "\n",
    "plt.figure(figsize = (15, inc * 1.5))\n",
    "\n",
    "for i in scp :\n",
    "    boolInt += 2\n",
    "\n",
    "    for c in eeg_Chans :\n",
    "        y = train_runs[0][c][i]\n",
    "        yest, Pxx_den = signal.welch(y, SAMPLE_RATE)   # , scaling = 'spectrum'\n",
    "        \n",
    "        plt.subplot(inc, 4, boolInt + 1)\n",
    "        plt.semilogy(yest, Pxx_den, label = c)\n",
    "        plt.title(f\"welch - {i + 1}\", fontsize = 11)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(inc, 4, boolInt + 2)\n",
    "        res, _ = plt.psd(y, Fs = SAMPLE_RATE, label = c) # , NFFT = NFFT\n",
    "        plt.title(f\"psd - {i + 1}\", fontsize = 11)\n",
    "        plt.xlabel('')\n",
    "        plt.ylabel('')\n",
    "        # plt.legend()\n",
    "\n",
    "plt.xlabel('frequency [Hz]')\n",
    "# plt.ylabel('PSD [V**2/Hz]')\n",
    "plt.tight_layout()\n",
    "plt.show();\n",
    "\n",
    "# f, Pxx_den = signal.welch(train_eras[0]['C3'][752], SAMPLE_RATE)\n",
    "\n",
    "# print(len(Pxx_den))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation Epoques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Epoques\n",
    "\n",
    "for i in range(len(files))[:: 3] :\n",
    "    plot_signal(entrants[i], train_parts[i], train_spots[0][i], train_spots[1][i], channels = eeg_Chans, # \n",
    "                period = CHUNK, lag = LAG, title = headers[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation décomposition des signaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Décomposition des signaux\n",
    "\n",
    "# Test de décomposition des signaux en bandes de fréquences spécifiques compatibles avec les répartitions usuelles\n",
    "# dans le domaine des EEG ['Delta', 'Theta', 'Alpha', 'Beta', 'Gamma']\n",
    "\n",
    "for input, token in zip(train_runs, ['Gauche', 'Droite']) :\n",
    "    print(f\"Exemples - Évènement Discriminé Main {token}\")\n",
    "    plot_wavelets(input, bands_coeff, eeg_Chans, scope = 30, headers = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation des spectrogrammes (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in zip(numbers, ['Gauche', 'Droite']) :\n",
    "    print(f\"Exemples - Évènement Discriminé Main {t}\")\n",
    "    spectrogram(train_runs[i], SAMPLE_RATE, eeg_Chans, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • MNE époque (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • MNE époque (test)\n",
    "\n",
    "raw_csv = entrants[0][eeg_Chans]\n",
    "info    = mne.create_info(ch_names = eeg_Chans, sfreq = SAMPLE_RATE, ch_types = 'eeg')\n",
    "raw_mne = mne.io.RawArray(raw_csv.T * 1e-6, info)\n",
    "sites     = np.where(entrants[0]['EventStart'] == 1)[0]\n",
    "\n",
    "# display(compare(np.sort(np.concatenate((train_spots[0][0], train_spots[1][0]))), loc))\n",
    "\n",
    "tmin, tmax = -0., 1\n",
    "\n",
    "# loc = mne.find_events(raw_mne, stim_channel = 'C3')\n",
    "# event_id = dict(C3 = 1, aud_r = 2, vis_l = 3, vis_r = 4)\n",
    "# raw = mne.io.Raw(raw_mne, preload = True)\n",
    "# raw.filter(2, None, method = 'iir')           # replace baselining with high-pass\n",
    "# events = mne.read_events(event_fname)\n",
    "\n",
    "# raw.info['bads'] = ['MEG 2443']  # set bad channels\n",
    "# picks = mne.pick_types(info, meg = 'grad', eeg = True, eog = False, exclude = 'bads')\n",
    "# Read epochs\n",
    "absc = mne.Epochs(raw_mne, np.array([sites, sites, sites]).T, None, tmin, tmax, proj = False,\n",
    "                    picks = None, baseline = None, preload = True, verbose = False) # event_id picks\n",
    "\n",
    "# labels = epochs.events[::5, -1]\n",
    "\n",
    "# events\n",
    "\n",
    "# raw_mne.plot();\n",
    "\n",
    "# raw_mne['C3'][0][0], len(df_train_csv[2]['Cz'])\n",
    "\n",
    "display(absc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test de classification - Proposition 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, y = bands_coeff['Theta']\n",
    "\n",
    "i = np.random.randint(np.shape(X)[0])\n",
    "y = df_trains['C4'][i] - df_trains['C3'][i]\n",
    "# X = [np.sign(s) for s in (trains['C4'] - trains['C3'])]\n",
    "# y = {band: bandpass_filter(X[i], b, a) for band, (b, a) in bands_coeff.items()}\n",
    "\n",
    "plt.figure()\n",
    "# plt.style.use('')\n",
    "\n",
    "plt.plot(normalized(bandpass_filter(y, b, y)), label = 'C4 - C3')\n",
    "plt.plot(normalized(bandpass_filter(np.sign(y), b, y)), label = '[C4 - C3]', c = np.random.rand(1, 3)[0])\n",
    "# plt.plot(pywt.dwt(y, wavelet = 'db4')[0], label = 'C4 - C3')\n",
    "# plt.plot(pywt.dwt(np.sign(y), wavelet = 'db4')[0], label = '[C4 - C3]')\n",
    "# plt.plot(np.zeros(512), ls = '--', c = np.random.rand(1, 3)[0])\n",
    "\n",
    "# print(np.shape(pywt.dwt(y, wavelet = 'db8')))\n",
    "\n",
    "# plt.plot(bandpass_filter(np.sign(trains['C3'][i]), b, a), label = 'C3')\n",
    "# plt.plot(bandpass_filter(np.sign(trains['C4'][i]), b, a), label = 'C4')\n",
    "# plt.plot(bandpass_filter(np.sign(trains['Cz'][i]), b, a), label = 'Cz')\n",
    "\n",
    "# for (band, signal) in reversed(y.items()) :\n",
    "#     plt.plot(pd.Series(signal), label = f'{band}', c = np.random.rand(1, 3)[0])\n",
    "\n",
    "plt.title(f\"{i}\")\n",
    "plt.legend(loc = 'upper right')\n",
    "\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    ## K-plus proches voisins\n",
    "    'knn__n_neighbors' : range(2),\n",
    "    ## SVM\n",
    "    'svm__C'      : [0.1, 1, 5],\n",
    "    'svm__kernel' : ['linear', 'softmax', 'sigmoid', 'rbf'],\n",
    "    ## RandomForest\n",
    "    # 'rf__max_features'      : ['sqrt', 'log2', None],\n",
    "    # 'rf__min_samples_split' : range(2, 32, 2),\n",
    "    # , ('rf', clf3), ('rf', clf3)\n",
    "    'estimators': [[('knn', knc), ('svm', svm)], [('knn', knc), ('svm', svm)]] \n",
    "    }\n",
    "\n",
    "grid = model_selection.GridSearchCV(estimator = Voting_clf, param_grid = params, cv = 5) \\\n",
    "    .fit(X_train_scaled, y_train)\n",
    "\n",
    "# parametres = {'max_features' : ['log2', 'sqrt', None], 'min_samples_split' : range(2, 32, 2)}\n",
    "\n",
    "# vclf = model_selection.GridSearchCV(estimator = clf3, param_grid = parametres, cv = 3) \\\n",
    "#     .fit(X_train_scaled, y_train)\n",
    "\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n",
    "print('score train :', grid.score(X_train_scaled, y_train))\n",
    "print('score test :', grid.score(X_test_scaled, y_test))\n",
    "\n",
    "# print(vclf.best_estimator_, vclf.best_params_, vclf.best_score_)\n",
    "# print('score train :', grid.score(X_train_scaled, y_train), vclf.score(X_train_scaled, y_train))\n",
    "# print('score test  :', grid.score(X_test_scaled, y_test), vclf.score(X_test_scaled, y_test))\n",
    "\n",
    "df_train_cpy, event_start = fancy_df(entrants, targets['EventType'], hands_event, CHUNK)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize = (24, 5), sharey = True)\n",
    "sig = .05\n",
    "\n",
    "axes.plot(entrants['C3'])\n",
    "\n",
    "for p in event_start :\n",
    "    axes.axvspan(p[0] - (CHUNK >> 1), p[0] + 1.5 * CHUNK, facecolor = 'orangered', alpha = .5)\n",
    "\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolInt = 16\n",
    "start   = event_start[boolInt][0]\n",
    "entrant = start + CHUNK\n",
    "input   = df_train_cpy['C3_4'][start : entrant]\n",
    "smooth  = input.copy()\n",
    "inc     = 5\n",
    "alpha   = 1 / 3\n",
    "dec     = int(inc / alpha)\n",
    "\n",
    "plt.figure(figsize = (24, 5))\n",
    "plot_window(entrants, ['C3', 'C4', 'C3 + C4'], start, CHUNK)\n",
    "\n",
    "# Lissage des hautes fréquences\n",
    "for _ in range(inc) :\n",
    "    smooth = simple_exponential_smoothing(smooth, alpha, 0)\n",
    "\n",
    "smooth = pd.Series(index = range(start, entrant + inc - dec), data = smooth[dec :])\n",
    "\n",
    "# plt.plot(raw - smooth, label = hands[event_start[pos][1]])\n",
    "plt.plot(smooth, '--', label = hands_event[event_start[boolInt][1]])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Apendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"text-align:center\" ><b>EEG</b> - Prédiction des Mouvements Imaginaires de la Main</h2>\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Le projet**\n",
    "- Intoduction  \n",
    "https://github.com/DataScientest-Studio/mar24_cds_eeg/blob/eric/references/Description_projet_EEG.pdf  \n",
    "https://www.bbci.de/competition/iv/desc_2b.pdf\n",
    "- Ressources / Données   \n",
    "https://www.kaggle.com/competitions/ucsd-neural-data-challenge/overview  \n",
    "- Bibliographie  \n",
    "https://www.bbci.de/competition/iv/desc_2b.pdf\n",
    "#### **2. Liens utils**\n",
    "- SciPy - *open-source software for mathematics, science, and engineering*  \n",
    "https://docs.scipy.org/doc/scipy/index.html  \n",
    "https://docs.scipy.org/doc/scipy/reference/signal.html  \n",
    "- MNE - *MEG + EEG Analysis & Visualisation*\n",
    "   - Accueil  \n",
    "   https://mne.tools/stable/index.html\n",
    "\n",
    "   - MNE - Data structures from arbitrary data  \n",
    "   https://mne.tools/stable/auto_tutorials/io/10_reading_meg_data.html#creating-mne-data-structures-from-arbitrary-data-from-memory\n",
    "   \n",
    "   - MNE - EEG Preprocessing  \n",
    "   https://mne.tools/dev/auto_tutorials/preprocessing/index.html  \n",
    "\n",
    "- pyRiemann - *Biosignals classification with Riemannian geometry*  \n",
    "https://pyriemann.readthedocs.io/en/latest/  \n",
    "- neurodsp - *Neuro Digital Signal Processing Toolbox*  \n",
    "https://neurodsp-tools.github.io/neurodsp/index.html#\n",
    "- Rythme Mu  \n",
    "https://fr.wikipedia.org/wiki/Rythme_Mu\n",
    "- Spectrogram from EEG  \n",
    "https://www.kaggle.com/code/cdeotte/how-to-make-spectrogram-from-eeg\n",
    "- Divers  \n",
    "https://signalprocessingsociety.org/  \n",
    "https://fr.wikipedia.org/wiki/Filtre_de_Butterworth  \n",
    "https://fr.wikipedia.org/wiki/Moyenne_mobile  \n",
    "https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html  \n",
    "https://perso.etis-lab.fr/ghaffari/2014_CCMB_Floride_USA.pdf  \n",
    "https://www.youtube.com/watch?v=wB417SAbdak&list=PLXc9qfVbMMN2TAoLHVW5NvNmJtwiHurzw  \n",
    "https://fastercapital.com/fr/sujet/identification-des-artefacts-de-traitement-du-signal-dans-des-sc%C3%A9narios-r%C3%A9els.html#:~:text=L'inspection%20visuelle%20est%20la,des%20pertes%20et%20du%20bruit.  \n",
    "   - Z-Score Normalisation  \n",
    "   https://fr.wikipedia.org/wiki/Cote_Z_(statistiques)  \n",
    "   https://typeset.io/questions/why-is-z-score-normalisation-necessary-in-pre-processing-eeg-1xv5jepyq5  \n",
    "\n",
    "   - Traitement numérique du signal  \n",
    "   https://fr.wikipedia.org/wiki/Traitement_num%C3%A9rique_du_signal  \n",
    "   - Ondelette  \n",
    "      - Wiki  \n",
    "      https://fr.wikipedia.org/wiki/Ondelette  \n",
    "\n",
    "      - L’analyse par ondelettes dans la vie de tous les jours  \n",
    "      https://interstices.info/lanalyse-par-ondelettes-dans-la-vie-de-tous-les-jours/  \n",
    "\n",
    "      - A guide for using the Wavelet Transform in Machine Learning  \n",
    "      https://ataspinar.com/2018/12/21/a-guide-for-using-the-wavelet-transform-in-machine-learning/\n",
    "      \n",
    "      - pyWavelets - *open source wavelet transform*  \n",
    "      https://pywavelets.readthedocs.io/en/latest/\n",
    "\n",
    "      - Ondelettes et applications  \n",
    "      https://www.i2m.univ-amu.fr/~caroline.chaux/GEOMDATA/TI-te5215.pdf\n",
    "\n",
    "   - Maximum de vraisemblance  \n",
    "   https://pmarchand1.github.io/ECL8202/notes_cours/03-Maximum_vraisemblance.html  \n",
    "   https://fr.wikipedia.org/wiki/Maximum_de_vraisemblance#:~:text=En%20statistique%2C%20l'estimateur%20du,maximisant%20la%20fonction%20de%20vraisemblance  \n",
    "\n",
    "   - Transformation de Fourier discrète  \n",
    "   https://fr.wikipedia.org/wiki/Transformation_de_Fourier_discr%C3%A8te  \n",
    "      - La Transformation de Fourier n’est pas adaptée à l’analyse des signaux non stationnaires.\n",
    "   - Neural Data Science in Python  \n",
    "   https://neuraldatascience.io/intro.html\n",
    "\n",
    "   - Preprocessing of EEG  \n",
    "   https://www.frontiersin.org/articles/10.3389/fninf.2015.00016/full#:~:text=The%20depositable%20preprocessing%20pipeline%20consists,with%20a%20low%20recording%20SNR  \n",
    "   https://typeset.io/papers/preprocessing-of-eeg-4go8vhcbty  \n",
    "   https://learn.neurotechedu.com/preprocessing  \n",
    "   https://g0rella.github.io/gorella_mwn/preprocessing_eeg.html  \n",
    "   \n",
    "   - Biblio :  \n",
    "   https://perso.telecom-paristech.fr/bloch/P6Image/ondelettestrsp.pdf  \n",
    "   https://www.math.u-bordeaux.fr/~jbigot/Site/Enseignement_files/ondelettesIMAT.pdf  \n",
    "   http://w3.cran.univ-lorraine.fr/perso/radu.ranta/pdf/cours_deb_ond%28fr%29.pdf\n",
    "   \n",
    "   - Digital Filtering  \n",
    "   http://notebooks.pluxbiosignals.com/notebooks/Categories/Pre-Process/digital_filtering_eeg_rev.html\n",
    "\n",
    "   - Processus stationnaire  \n",
    "   https://fr.wikipedia.org/wiki/Processus_stationnaire\n",
    "\n",
    "   - Analyse en composantes principales  \n",
    "   https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales#:~:text=L'ACP%2C%20d%C3%A9sign%C3%A9e%20en%20g%C3%A9n%C3%A9ral,une%20grandeur%20physique%2C%20comme%20les"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
