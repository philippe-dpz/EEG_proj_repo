{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"text-align:center\" ><b>EEG</b> - Prédiction des Mouvements Imaginaires de la Main</h2>\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Le projet**\n",
    "- Intoduction  \n",
    "https://github.com/DataScientest-Studio/mar24_cds_eeg/blob/eric/references/Description_projet_EEG.pdf  \n",
    "https://www.bbci.de/competition/iv/desc_2b.pdf\n",
    "- Ressources / Données   \n",
    "https://www.kaggle.com/competitions/ucsd-neural-data-challenge/overview  \n",
    "- Bibliographie  \n",
    "https://www.bbci.de/competition/iv/desc_2b.pdf\n",
    "#### **2. Liens utils**\n",
    "- SciPy - *open-source software for mathematics, science, and engineering*  \n",
    "https://docs.scipy.org/doc/scipy/index.html  \n",
    "https://docs.scipy.org/doc/scipy/reference/signal.html  \n",
    "- MNE - *MEG + EEG Analysis & Visualisation*\n",
    "   - Accueil  \n",
    "   https://mne.tools/stable/index.html\n",
    "\n",
    "   - MNE - Data structures from arbitrary data  \n",
    "   https://mne.tools/stable/auto_tutorials/io/10_reading_meg_data.html#creating-mne-data-structures-from-arbitrary-data-from-memory\n",
    "   \n",
    "   - MNE - EEG Preprocessing  \n",
    "   https://mne.tools/dev/auto_tutorials/preprocessing/index.html  \n",
    "\n",
    "- pyRiemann - *Biosignals classification with Riemannian geometry*  \n",
    "https://pyriemann.readthedocs.io/en/latest/  \n",
    "- neurodsp - *Neuro Digital Signal Processing Toolbox*  \n",
    "https://neurodsp-tools.github.io/neurodsp/index.html#\n",
    "- Rythme Mu  \n",
    "https://fr.wikipedia.org/wiki/Rythme_Mu\n",
    "- Spectrogram from EEG  \n",
    "https://www.kaggle.com/code/cdeotte/how-to-make-spectrogram-from-eeg\n",
    "- Divers  \n",
    "https://signalprocessingsociety.org/  \n",
    "https://fr.wikipedia.org/wiki/Filtre_de_Butterworth  \n",
    "https://fr.wikipedia.org/wiki/Moyenne_mobile  \n",
    "https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html  \n",
    "https://perso.etis-lab.fr/ghaffari/2014_CCMB_Floride_USA.pdf  \n",
    "https://www.youtube.com/watch?v=wB417SAbdak&list=PLXc9qfVbMMN2TAoLHVW5NvNmJtwiHurzw  \n",
    "https://fastercapital.com/fr/sujet/identification-des-artefacts-de-traitement-du-signal-dans-des-sc%C3%A9narios-r%C3%A9els.html#:~:text=L'inspection%20visuelle%20est%20la,des%20pertes%20et%20du%20bruit.  \n",
    "   - Z-Score Normalisation  \n",
    "   https://fr.wikipedia.org/wiki/Cote_Z_(statistiques)  \n",
    "   https://typeset.io/questions/why-is-z-score-normalisation-necessary-in-pre-processing-eeg-1xv5jepyq5  \n",
    "\n",
    "   - Traitement numérique du signal  \n",
    "   https://fr.wikipedia.org/wiki/Traitement_num%C3%A9rique_du_signal  \n",
    "   - Ondelette  \n",
    "      - Wiki  \n",
    "      https://fr.wikipedia.org/wiki/Ondelette  \n",
    "\n",
    "      - L’analyse par ondelettes dans la vie de tous les jours  \n",
    "      https://interstices.info/lanalyse-par-ondelettes-dans-la-vie-de-tous-les-jours/  \n",
    "\n",
    "      - A guide for using the Wavelet Transform in Machine Learning  \n",
    "      https://ataspinar.com/2018/12/21/a-guide-for-using-the-wavelet-transform-in-machine-learning/\n",
    "      \n",
    "      - pyWavelets - *open source wavelet transform*  \n",
    "      https://pywavelets.readthedocs.io/en/latest/\n",
    "\n",
    "      - Ondelettes et applications  \n",
    "      https://www.i2m.univ-amu.fr/~caroline.chaux/GEOMDATA/TI-te5215.pdf\n",
    "\n",
    "   - Maximum de vraisemblance  \n",
    "   https://pmarchand1.github.io/ECL8202/notes_cours/03-Maximum_vraisemblance.html  \n",
    "   https://fr.wikipedia.org/wiki/Maximum_de_vraisemblance#:~:text=En%20statistique%2C%20l'estimateur%20du,maximisant%20la%20fonction%20de%20vraisemblance  \n",
    "\n",
    "   - Transformation de Fourier discrète  \n",
    "   https://fr.wikipedia.org/wiki/Transformation_de_Fourier_discr%C3%A8te  \n",
    "      - La Transformation de Fourier n’est pas adaptée à l’analyse des signaux non stationnaires.\n",
    "   - Neural Data Science in Python  \n",
    "   https://neuraldatascience.io/intro.html\n",
    "\n",
    "   - Preprocessing of EEG  \n",
    "   https://www.frontiersin.org/articles/10.3389/fninf.2015.00016/full#:~:text=The%20depositable%20preprocessing%20pipeline%20consists,with%20a%20low%20recording%20SNR  \n",
    "   https://typeset.io/papers/preprocessing-of-eeg-4go8vhcbty  \n",
    "   https://learn.neurotechedu.com/preprocessing  \n",
    "   https://g0rella.github.io/gorella_mwn/preprocessing_eeg.html  \n",
    "   \n",
    "   - Biblio :  \n",
    "   https://perso.telecom-paristech.fr/bloch/P6Image/ondelettestrsp.pdf  \n",
    "   https://www.math.u-bordeaux.fr/~jbigot/Site/Enseignement_files/ondelettesIMAT.pdf  \n",
    "   http://w3.cran.univ-lorraine.fr/perso/radu.ranta/pdf/cours_deb_ond%28fr%29.pdf\n",
    "   \n",
    "   - Digital Filtering  \n",
    "   http://notebooks.pluxbiosignals.com/notebooks/Categories/Pre-Process/digital_filtering_eeg_rev.html\n",
    "\n",
    "   - Processus stationnaire  \n",
    "   https://fr.wikipedia.org/wiki/Processus_stationnaire\n",
    "\n",
    "   - Analyse en composantes principales  \n",
    "   https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales#:~:text=L'ACP%2C%20d%C3%A9sign%C3%A9e%20en%20g%C3%A9n%C3%A9ral,une%20grandeur%20physique%2C%20comme%20les"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Chargement des différentes librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Chargement des différentes librairies\n",
    "\n",
    "import sys, os, gc\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "from src.thot.sesh import *\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import ensemble, svm, neighbors\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from keras.models import Sequential                             # type: ignore\n",
    "from keras.callbacks import EarlyStopping                       # type: ignore\n",
    "from keras.layers import GlobalAveragePooling1D                 # type: ignore\n",
    "from keras.layers import Dense, Dropout, Conv1D, LSTM           # type: ignore\n",
    "from keras.layers import LeakyReLU, ReLU, PReLU, ConvLSTM1D     # type: ignore\n",
    "# from keras.layers import Bidirectional, TimeDistributed, RepeatVector, Flatten\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.optimizers import AdamW, Adam            # type: ignore\n",
    "\n",
    "# from pyriemann.spatialfilters import CSP\n",
    "\n",
    "import pywt, librosa\n",
    "import seaborn as sns\n",
    "\n",
    "# from scipy.fft import fft\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Déclaration de constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Déclaration de constantes\n",
    "\n",
    "# Fréquence d'échantillonnage - Hz (Nombre de valeur / sec)\n",
    "SAMPLE_RATE  = 250\n",
    "# Temps additionel pour étendre le domaines d'étude.\n",
    "LAG : int    = -64     # Décalage du signal dû signal acoustique ~500ms\n",
    "#\n",
    "PW2 : int    = int(np.floor(np.log2(SAMPLE_RATE)))\n",
    "#\n",
    "NFFT : int   = 1 << PW2\n",
    "# Epoque en sec donnée en nombre d'échantillon consectutif # 4\" de données (multiple de 2)\n",
    "SCOPE : int  = (1 << PW2) << 2\n",
    "# Deux enregistrements bipolaires + neutre\n",
    "eeg_Chans    = ['C3', 'C4', 'Cz']\n",
    "# Liste des cannaux eeg associés aux évènement 0 et 1\n",
    "eeg_left     = [f'{c}_0' for c in eeg_Chans]\n",
    "eeg_right    = [f'{c}_1' for c in eeg_Chans]\n",
    "full_eeg     = eeg_left + eeg_right\n",
    "# Trois enregistrements musculaires\n",
    "ecg_Chans    = ['EOG:ch01', 'EOG:ch02', 'EOG:ch03']\n",
    "# Liste de tous les cannaux des dataframes\n",
    "all_chans    = eeg_Chans + ecg_Chans\n",
    "# Correspondance pour la classification\n",
    "hands_event  = {0: 'Left', 1: 'Right'}\n",
    "# Les bandes de fréquences d'intérêt\n",
    "eeg_bands    = {'Delta' : (0.1, 4.0),\n",
    "                'Theta' : (4.1, 8.0),\n",
    "                'Alpha' : (8.1, 14.0),\n",
    "                'Beta'  : (14.1, 30.0),\n",
    "                'Gamma' : (30.1, (SAMPLE_RATE >> 1) - 1),}\n",
    "# Coefficients pour filtres Butterworth numérique d'ordre N pour le filtrage passe-bande\n",
    "bands_coeff  = {band : butter_bandpass(low, high, SAMPLE_RATE) for band, (low, high) in eeg_bands.items()}\n",
    "# Nombre dévènement à prédir\n",
    "num_events   = range(len(hands_event))\n",
    "\n",
    "LAG, SCOPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Acquisition des données d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Acquisition des données d'entrainement\n",
    "\n",
    "target  = \"../data/data.zip\"\n",
    "count   = len('train/')\n",
    "fics    = [x[count :] for x in files_in_zip(target, directory = 'train')]\n",
    "\n",
    "[fics.remove(x) for x in fics[:: -3]]\n",
    "\n",
    "# Acquisition des fichiers du répertoir dans le fichier zip\n",
    "train_csv = csv_in_zip(target, directory = 'train', files = fics)\n",
    "label_csv = csv_in_zip(target, directory = 'y_train_only', files = fics)\n",
    "\n",
    "notes   = filename(fics)\n",
    "headers = [f\"{t} . {i + 1}\" for i, t in enumerate(notes)]\n",
    "count   = range(len(train_csv))\n",
    "\n",
    "# fics    = [f'B0{i}0{j}T.csv' for i in range(1, 9) for j in range(1, 4)]\n",
    "# df_train_pkl = pkl_in_zip(path, fichier_specifique = 'epoched_train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Acquisition des données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Acquisition des données de test\n",
    "\n",
    "test_csv = csv_in_zip(target, directory = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Pré-traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Pré-traitement des données\n",
    "\n",
    "def data_spliting(datas : list[Board], Channels : Clause, number_event : int | Index,\n",
    "                  labels : list[Board] | None = None, merge : bool = False, level : bool = True) :\n",
    "    count = range(len(datas))\n",
    "    parts = []        #\n",
    "    temp  = [[], []]  # Les époques pour tous les cannaux et tous les évènements.\n",
    "    spots = [[], []]  # Apparitions des évènements\n",
    "    # Pour la standardisation du nombre d'échantillon max conservé\n",
    "    ceil  = min([len(labels[i]['EventType']) for i in count])\n",
    "\n",
    "    if type(number_event) == int : number_event = range(number_event)\n",
    "\n",
    "    # Extraction des données relavitives à l'apparition des évènements.\n",
    "    if level :\n",
    "        for i in count :\n",
    "            df   = datas[i]\n",
    "            kind = labels[i]['EventType'][: ceil]\n",
    "            loc  = np.where(df['EventStart'] == 1)[: ceil]\n",
    "\n",
    "            parts.append(zero_removal(df['C3'], 75))\n",
    "\n",
    "            for j in number_event :\n",
    "                spots[j].append(np.array(*loc)[*np.where(kind == j)])\n",
    "\n",
    "                # [tf.convert_to_tensor(X) for X in event_epochs(spots[j][-1], SCOPE, LAG)]\n",
    "                room = event_epochs(spots[j][-1], SCOPE, LAG)\n",
    "\n",
    "                temp[j].append([full_event(df[c], room, merge) for c in Channels])\n",
    "    else :\n",
    "        for i in count :\n",
    "            df   = datas[i]\n",
    "            kind = label_csv[i]['EventType']\n",
    "            loc  = np.where(df['EventStart'] == 1)\n",
    "\n",
    "            parts.append(zero_removal(df['C3'], 75))\n",
    "\n",
    "            for j in number_event :\n",
    "                spots[j].append(np.array(*loc)[*np.where(kind == j)])\n",
    "\n",
    "                room = event_epochs(spots[j][-1], SCOPE, LAG)\n",
    "\n",
    "                temp[j].append([full_event(df[c], room, merge) for c in Channels])\n",
    "\n",
    "    # Regroupement des données en fonction du type de l'évènement et du cannal d'observation\n",
    "    if merge :\n",
    "        temp = [[[np.append([], T[j :: 3]) for T in temp[i]] for j in range(3)] for i in number_event]\n",
    "    else :\n",
    "        store = [[[], [], []], [[], [], []]]\n",
    "        \n",
    "        [[[[store[i][j].append(G) for G in X] for j, X in enumerate(T)] for T in temp[i]] for i in number_event]\n",
    "\n",
    "        temp = store\n",
    "\n",
    "    eras = [pd.DataFrame({**dict(zip(Channels, [pd.Series(X) for X in temp[i]])), 'EventType': i}) for i in number_event]\n",
    "\n",
    "    return eras, spots, parts\n",
    "\n",
    "train_eras, train_spots, train_parts = data_spliting(train_csv, eeg_Chans, num_events, label_csv)\n",
    "\n",
    "train_samples = samples(len(train_eras[0]))\n",
    "trains        = pd.concat(train_eras, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation des spectrogrammes / Test\n",
    "\n",
    "def logMelSpectrogram(data : Vector, rate : int, dt : float = 1e-2) -> Vector :\n",
    "    tps = 1 << int(np.floor(np.log2(rate * dt)))\n",
    "    # print(tps)\n",
    "    # Spectrogramme\n",
    "    stfts = np.abs(librosa.stft(y = data, n_fft = tps, hop_length = 1 << 2, center = True)).T\n",
    "    # Filtre de MEL\n",
    "    liny  = librosa.filters.mel(sr = rate, n_fft = tps + 1, n_mels = stfts.shape[-1]).T\n",
    "    # Application du filtre au spectrogramme\n",
    "    mel_  = np.tensordot(stfts, liny, 1)\n",
    "\n",
    "    return np.log(mel_ + 1e-6)\n",
    "    \n",
    "def structure(data : Board | Vector, rate : int, whr : Clause) -> Vector :\n",
    "    # return np.array([logMelSpectrogram(X, rate, 2) for X in data[whr]])\n",
    "    # return np.stack([[signal.welch(X, rate)[1] for X in data[c]] for c in whr], axis = 2)\n",
    "    return np.stack(trains['C4'] - trains['C3'], axis = 1)\n",
    "    # return np.stack([[X for X in data[c]] for c in whr], axis = 2)\n",
    "\n",
    "def img_spectrogram(raw : Vector, rate : int, nfft : int = 1 << 10) -> Vector :\n",
    "    return librosa.feature.melspectrogram(y = raw, sr = rate, hop_length = 1, \n",
    "                            n_fft = nfft, n_mels = 32, fmin = 0, fmax = 20, win_length = 32)\n",
    "\n",
    "def spectrogram(data : Board, rate : int, channels : Clause, n_row : int = 5, n_col : int = 12) :\n",
    "    sample = np.random.default_rng().integers(data.shape[0], size = n_row)\n",
    "\n",
    "    sample.sort()\n",
    "\n",
    "    plt.figure(figsize = (18, 2 * .48 * n_row))\n",
    "\n",
    "    pos = 0\n",
    "\n",
    "    for k in sample :\n",
    "        for c in channels :\n",
    "            x   = normalized(data[c][k])\n",
    "            raw = img_spectrogram(raw = x, rate = rate)\n",
    "            pos += 1\n",
    "            \n",
    "            plt.subplot(n_row, n_col, pos)\n",
    "            plt.title(f\"{((pos - 1) // 3) + 1} . {k} - {c}\", fontsize = 8)\n",
    "            librosa.display.specshow(data = 1 - raw, sr = rate, hop_length = 1)\n",
    "            \n",
    "            # pos = n_col * (i >> 1) + j\n",
    "            # f, t, Sxx = signal.spectrogram(x, rate)\n",
    "            # plt.subplot(n_row, n_col, pos + 4)\n",
    "            # plt.pcolormesh(t, f, 1 - Sxx, shading = 'gouraud')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show();\n",
    "\n",
    "def plot_logMelSpectrogram(data, rate) :\n",
    "    sns.heatmap(np.rot90(logMelSpectrogram(data, rate)), cmap = 'inferno')\n",
    "    \n",
    "    # loc, _ = plt.xticks()\n",
    "    # l      = np.round((loc - loc.min()) * len(data) / fe / loc.max(), 2), vmin = -6\n",
    "\n",
    "    # plt.xticks(loc, l)\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Frequency (Mel)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = [list(harmonic(x, bands_coeff).values()) for x in [trains[c] for c in eeg_Chans]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(H), np.shape(np.stack(np.stack(H, axis = 1), axis = 2)), np.shape(H[0][1][0])\n",
    "# harmonic(trains['C3'][256], bands_coeff).values())), H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(trains.drop(columns = ['EventType']),\n",
    "                                                    trains['EventType'], test_size = .2, random_state = 42)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split([v.tolist() for v in np.array(trains[eeg_Channels])],\n",
    "#                                                     trains['EventType'], test_size = .2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = np.array(X_train)\n",
    "# test_dataset  = X_test\n",
    "\n",
    "# train_dataset = structure(X_train, SAMPLE_RATE, eeg_Chans[:2])\n",
    "# test_dataset  = structure(X_test, SAMPLE_RATE, eeg_Chans[:2])\n",
    "\n",
    "train_dataset = structure(X_train, SAMPLE_RATE, eeg_Chans[:2])\n",
    "test_dataset  = structure(X_test, SAMPLE_RATE, eeg_Chans[:2])\n",
    "\n",
    "# print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### • Test prédiction\n",
    "\n",
    "# UNITS     : int   = 100\n",
    "BATCHSIZE : int   = 30\n",
    "EPOCH     : int   = 1000\n",
    "ZERO      : int   = 64\n",
    "DROPOUT   : float = .2\n",
    "# kl_divergence mean_squared_logarithmic_error mean_absolute_error\n",
    "LOSS      : None  = 'sparse_categorical_crossentropy'\n",
    "ACTIV     : None  = LeakyReLU   # PReLU, \n",
    "OPTIMIZER : None  = 'AdamW'     # adamax, , adafactor, adam, nadam\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# - 1 -\n",
    "model.add(Conv1D(filters = ZERO, kernel_size = (5), dilation_rate = 2,\n",
    "                 input_shape = train_dataset.shape[1: ]))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(ACTIV())\n",
    "\n",
    "# - 2 -\n",
    "model.add(Conv1D(filters = ZERO << 1, kernel_size = (5), dilation_rate = 2))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(ACTIV())\n",
    "\n",
    "# - 3 -\n",
    "model.add(Conv1D(filters = ZERO << 2, kernel_size = (5), dilation_rate = 2))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(ACTIV())\n",
    "model.add(GlobalAveragePooling1D()) # \n",
    "\n",
    "# Classification\n",
    "model.add(Dense(ZERO << 2))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(ACTIV())\n",
    "model.add(Dense(len(hands_event), activation = 'softmax')) # sigmoid  \n",
    "\n",
    "'''\n",
    "# Ajout de la premiere couche lstm\n",
    "model.add(LSTM(ZERO, input_shape = train_dataset.shape[1:], activation = ACTIV(), return_sequences = True)) #\n",
    "model.add(LSTM(ZERO, dropout = DROPOUT, return_sequences = False))\n",
    "\n",
    "# Ajout de la couche de sortie\n",
    "model.add(Dense(len(hands_event), activation = 'softmax'))\n",
    "'''\n",
    "\n",
    "model.compile(optimizer = OPTIMIZER, loss = LOSS, metrics = ['accuracy'])\n",
    "# model.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop    = EarlyStopping(monitor = 'val_accuracy', mode = 'max', verbose = 1, patience = 50)\n",
    "history = model.fit(train_dataset, y_train, validation_data = (test_dataset, y_test), verbose = 1,\n",
    "                    batch_size = BATCHSIZE, epochs = EPOCH, callbacks = [stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_dataset)\n",
    "\n",
    "sum([np.where(x > .5)[0][0] for x in pred] == y_test) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values  = history_dict['loss']\n",
    "acc_values   = history_dict['accuracy']\n",
    "absc         = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.figure(figsize = (12, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(absc, loss_values, label = 'Loss')\n",
    "plt.plot(absc, acc_values, label = 'Accuracy')\n",
    "plt.title('Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(absc, history_dict['val_loss'], label = 'Loss')\n",
    "plt.plot(absc, history_dict['val_accuracy'], label = 'Accuracy')\n",
    "plt.title('Testing')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation densité spectrale du Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Densité Spectral du Signal\n",
    "\n",
    "plot_psd(train_csv, train_eras, rate = SAMPLE_RATE, Channels = eeg_Chans, titled = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Densité spectrale / échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Densité spectral / échantillon\n",
    "\n",
    "n   = 40\n",
    "scp = samples(train_samples, n)\n",
    "pos = -2\n",
    "\n",
    "plt.figure(figsize = (15, n * 1.5))\n",
    "\n",
    "for i in scp :\n",
    "    pos += 2\n",
    "\n",
    "    for c in eeg_Chans :\n",
    "        x = train_eras[0][c][i]\n",
    "        f, Pxx_den = signal.welch(x, SAMPLE_RATE)   # , scaling = 'spectrum'\n",
    "        \n",
    "        plt.subplot(n, 4, pos + 1)\n",
    "        plt.semilogy(f, Pxx_den, label = c)\n",
    "        plt.title(f\"welch - {i + 1}\", fontsize = 11)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(n, 4, pos + 2)\n",
    "        r, _ = plt.psd(x, Fs = SAMPLE_RATE, label = c) # , NFFT = NFFT\n",
    "        plt.title(f\"psd - {i + 1}\", fontsize = 11)\n",
    "        plt.xlabel('')\n",
    "        plt.ylabel('')\n",
    "        # plt.legend()\n",
    "\n",
    "plt.xlabel('frequency [Hz]')\n",
    "# plt.ylabel('PSD [V**2/Hz]')\n",
    "plt.tight_layout()\n",
    "plt.show();\n",
    "\n",
    "# f, Pxx_den = signal.welch(train_eras[0]['C3'][752], SAMPLE_RATE)\n",
    "\n",
    "# print(len(Pxx_den))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation Epoques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Epoques\n",
    "\n",
    "for i in range(len(fics))[:: 3] :\n",
    "    plot_signal(train_csv[i], train_parts[i], train_spots[0][i], train_spots[1][i], channels = eeg_Chans, # \n",
    "                period = SCOPE, lag = LAG, title = headers[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation décomposition des signaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Décomposition des signaux\n",
    "\n",
    "# Test de décomposition des signaux en bandes de fréquences spécifiques compatibles avec les répartitions usuelles\n",
    "# dans le domaine des EEG ['Delta', 'Theta', 'Alpha', 'Beta', 'Gamma']\n",
    "\n",
    "for df, token in zip(train_eras, ['Gauche', 'Droite']) :\n",
    "    print(f\"Exemples - Évènement Discriminé Main {token}\")\n",
    "    plot_wavelets(df, bands_coeff, eeg_Chans, scope = 30, headers = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation des spectrogrammes (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in zip(num_events, ['Gauche', 'Droite']) :\n",
    "    print(f\"Exemples - Évènement Discriminé Main {t}\")\n",
    "    spectrogram(train_eras[i], SAMPLE_RATE, eeg_Chans, 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test PCA - (Non cloncluant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Test PCA - (Non cloncluant)\n",
    "\n",
    "nca = SCOPE >> 2\n",
    "pca = PCA(nca)\n",
    "\n",
    "_, ax = plt.subplots(nrows = 2, ncols = 3, figsize = (15, 5))\n",
    "\n",
    "for i, d in enumerate(train_eras) :\n",
    "    for j, c in enumerate(eeg_Chans) :\n",
    "        Z = sc.fit_transform(list(d[c].to_list())) # \n",
    "        principal_components = pca.fit_transform(Z)\n",
    "        \n",
    "        ax[i, j].plot(range(nca), np.cumsum(pca.explained_variance_ratio_))\n",
    "        ax[i, j].set_title(f'{c} . {i}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • MNE époque (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • MNE époque (test)\n",
    "\n",
    "raw_csv = train_csv[0][eeg_Chans]\n",
    "info    = mne.create_info(ch_names = eeg_Chans, sfreq = SAMPLE_RATE, ch_types = 'eeg')\n",
    "raw_mne = mne.io.RawArray(raw_csv.T * 1e-6, info)\n",
    "loc     = np.where(train_csv[0]['EventStart'] == 1)[0]\n",
    "\n",
    "# display(compare(np.sort(np.concatenate((train_spots[0][0], train_spots[1][0]))), loc))\n",
    "\n",
    "tmin, tmax = -0., 1\n",
    "\n",
    "# loc = mne.find_events(raw_mne, stim_channel = 'C3')\n",
    "# event_id = dict(C3 = 1, aud_r = 2, vis_l = 3, vis_r = 4)\n",
    "# raw = mne.io.Raw(raw_mne, preload = True)\n",
    "# raw.filter(2, None, method = 'iir')           # replace baselining with high-pass\n",
    "# events = mne.read_events(event_fname)\n",
    "\n",
    "# raw.info['bads'] = ['MEG 2443']  # set bad channels\n",
    "# picks = mne.pick_types(info, meg = 'grad', eeg = True, eog = False, exclude = 'bads')\n",
    "# Read epochs\n",
    "absc = mne.Epochs(raw_mne, np.array([loc, loc, loc]).T, None, tmin, tmax, proj = False,\n",
    "                    picks = None, baseline = None, preload = True, verbose = False) # event_id picks\n",
    "\n",
    "# labels = epochs.events[::5, -1]\n",
    "\n",
    "# events\n",
    "\n",
    "# raw_mne.plot();\n",
    "\n",
    "# raw_mne['C3'][0][0], len(df_train_csv[2]['Cz'])\n",
    "\n",
    "display(absc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test de classification - Proposition inputs 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### • Test de classification - Proposition inputs 01\n",
    "\n",
    "# X1 = np.where(trains['EventType'] == 0, trains['C3'], trains['Cz']) \n",
    "# X2 = np.where(trains['EventType'] == 0, trains['Cz'], trains['C4']) \n",
    "# X = [signal.welch(list(v), SAMPLE_RATE)[1] for v in X1 + X2]\n",
    "\n",
    "# X = [np.append([], list(harmonic(v, bands_coeff).values())) for v in X]\n",
    "# X = np.where(trains['EventType'] == 0, trains['C3'], 2 * trains['Cz'])\n",
    "# X = [signal.welch(list(v), SAMPLE_RATE)[1] for v in X], trains['C3'] + trains['Cz'], trains['C4'] + trains['Cz']\n",
    "\n",
    "diff = np.array([trains['C3'] - trains['C4']])\n",
    "X = [np.append([], v.tolist()) for v in diff.T]\n",
    "\n",
    "# X = [np.append([], v.tolist()) for v in np.array(trains[eeg_Chans])]\n",
    "# diff = \n",
    "# X = np.where(trains['EventType'] == 0, trains['C3'], 2 * trains['Cz'])[:2]\n",
    "# X = [np.append([], list(harmonic(v, bands_coeff).values())) for v in X]\n",
    "# # X = [list(v) for v in X]\n",
    "\n",
    "# pca = PCA()\n",
    "# X = pca.fit_transform(X)\n",
    "# X = [signal.welch(v, SAMPLE_RATE)[1] for v in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diff.T.shape), np.array(trains[eeg_Chans]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test de classification - Proposition inputs 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Test de classification\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, trains['EventType'],\n",
    "                                                    test_size = .2, random_state = 42)\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled  = sc.transform(X_test)\n",
    "\n",
    "# X_train_scaled = X_train\n",
    "# X_test_scaled  = X_test\n",
    "\n",
    "# \n",
    "csp = CSP(nfilter = 2)\n",
    "# K-plus proches voisins\n",
    "knc = neighbors.KNeighborsClassifier()\n",
    "# SVM (support vector machine)[, 'auto', kernel = 'rbf']\n",
    "clf = svm.SVC(gamma = 'scale')\n",
    "# RandomForest \n",
    "rfc = ensemble.RandomForestClassifier(n_jobs = -1)\n",
    "#\n",
    "lrg = LogisticRegression()\n",
    "#ExtraTreesClassifier \n",
    "# Voting_clf = VotingClassifier(estimators = [('knn', clf1), ('svm', clf2), ('rf', clf3)], voting = 'hard')\n",
    "# cv3        = model_selection.KFold(n_splits = 3, random_state = 42, shuffle = True), clf4\n",
    "\n",
    "# Create a pipeline\n",
    "pip = Pipeline([('CSP', csp), ('RFC', rfc), ('SVM', clf)])\n",
    "\n",
    "# pip.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = pip.predict(X_test)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n",
    "# print(\"Accuracy score:\", accuracy_score(y_test, y_pred)), lrg\n",
    "\n",
    "for clf in [knc, clf, rfc, csp] :\n",
    "    print(f\"● {clf} :\")\n",
    "\n",
    "    scores : dict = model_selection.cross_validate(clf, X_train, y_train, cv = 5, scoring = ['accuracy'])\n",
    "    \n",
    "    r = scores['test_accuracy']\n",
    "    \n",
    "    print(f\"  - Accuracy : {r.mean():.1%} (±{r.std(): .2})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# print('score train :', clf3.score(X_train_scaled, y_train))\n",
    "print('score test :', svm.score(X_test_scaled, y_test))\n",
    "\n",
    "pred = svm.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    ## K-plus proches voisins\n",
    "    'knn__n_neighbors' : range(2),\n",
    "    ## SVM\n",
    "    'svm__C'      : [0.1, 1, 5],\n",
    "    'svm__kernel' : ['linear', 'softmax', 'sigmoid', 'rbf'],\n",
    "    ## RandomForest\n",
    "    # 'rf__max_features'      : ['sqrt', 'log2', None],\n",
    "    # 'rf__min_samples_split' : range(2, 32, 2),\n",
    "    # , ('rf', clf3), ('rf', clf3)\n",
    "    'estimators': [[('knn', knc), ('svm', svm)], [('knn', knc), ('svm', svm)]] \n",
    "    }\n",
    "\n",
    "grid = model_selection.GridSearchCV(estimator = Voting_clf, param_grid = params, cv = 5) \\\n",
    "    .fit(X_train_scaled, y_train)\n",
    "\n",
    "# parametres = {'max_features' : ['log2', 'sqrt', None], 'min_samples_split' : range(2, 32, 2)}\n",
    "\n",
    "# vclf = model_selection.GridSearchCV(estimator = clf3, param_grid = parametres, cv = 3) \\\n",
    "#     .fit(X_train_scaled, y_train)\n",
    "\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n",
    "print('score train :', grid.score(X_train_scaled, y_train))\n",
    "print('score test :', grid.score(X_test_scaled, y_test))\n",
    "\n",
    "# print(vclf.best_estimator_, vclf.best_params_, vclf.best_score_)\n",
    "# print('score train :', grid.score(X_train_scaled, y_train), vclf.score(X_train_scaled, y_train))\n",
    "# print('score test  :', grid.score(X_test_scaled, y_test), vclf.score(X_test_scaled, y_test))\n",
    "\n",
    "df_train_cpy, event_start = fancy_df(train_csv, label_csv['EventType'], hands_event, SCOPE)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize = (24, 5), sharey = True)\n",
    "sig = .05\n",
    "\n",
    "axes.plot(train_csv['C3'])\n",
    "\n",
    "for p in event_start :\n",
    "    axes.axvspan(p[0] - (SCOPE >> 1), p[0] + 1.5 * SCOPE, facecolor = 'orangered', alpha = .5)\n",
    "\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos    = 16\n",
    "start  = event_start[pos][0]\n",
    "entrant  = start + SCOPE\n",
    "df   = df_train_cpy['C3_4'][start : entrant]\n",
    "smooth = df.copy()\n",
    "n      = 5\n",
    "alpha  = 1 / 3\n",
    "dec    = int(n / alpha)\n",
    "\n",
    "plt.figure(figsize = (24, 5))\n",
    "plot_window(train_csv, ['C3', 'C4', 'C3 + C4'], start, SCOPE)\n",
    "\n",
    "# Lissage des hautes fréquences\n",
    "for _ in range(n) :\n",
    "    smooth = simple_exponential_smoothing(smooth, alpha, 0)\n",
    "\n",
    "smooth = pd.Series(index = range(start, entrant + n - dec), data = smooth[dec :])\n",
    "\n",
    "# plt.plot(raw - smooth, label = hands[event_start[pos][1]])\n",
    "plt.plot(smooth, '--', label = hands_event[event_start[pos][1]])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- Introduction :  \n",
    "L’électroencéphalogramme (EEG) est une technique d’imagerie cérébrale utilisée pour étudier les activités du cerveau.  \n",
    "En plaçant des capteurs sur le cuir chevelu, l’activité électrique du cerveau est enregistrée,  \n",
    "   ce qui permet de comprendre les fonctionnements cérébraux et d’identifier certains schémas que l’on peut ensuite attribuer à des comportements précis.  \n",
    "Un des schémas d'EEG qui a été beaucoup étudié est l’imagerie motrice (IM), ou le mouvement imaginaire de la main.  \n",
    "Les IM créent des schémas bien définis qui peuvent être détectés.  \n",
    "Le but de ce projet est de créer et d’entraîner un programme permettant de prédire si l’IM d’une personne correspond à un mouvement de la main droite ou de la main gauche.  \n",
    "# **2. Étapes du projet**\n",
    "- Prétraitement des Données :  \n",
    "Les données EEG sont sujettes à des artefacts ou des erreurs de collecte dues à des mouvements parasites ou des interférences.  \n",
    "Il est donc nécessaire d'appliquer un système de prétraitement des données pour réduire le bruit et extraire les bandes de fréquences pertinentes.\n",
    "\n",
    "- Segmentation des données et extraction des caractéristiques :  \n",
    "Les données EEG sont présentées comme un flux continu. Il est donc important, pour une meilleure analyse, de diviser les données en segments temporels correspondant à l’IM.  \n",
    "Ensuite, identifier et extraire les caractéristiques pertinentes des signaux EEG associées aux IM est essentiel.  \n",
    "Cela comprend la puissance et d'autres spécificités de l’activité électrique qui définissent les IM.\n",
    "\n",
    "- Analyse statistique exploratoire :  \n",
    "Utiliser les outils d’analyse exploratoire pour mieux comprendre les données et identifier les tendances ou les patterns significatifs.\n",
    "\n",
    "- Entraînement du modèle :  \n",
    "Entraîner un modèle permettant de distinguer les différences entre les IM des mains droite et gauche.  \n",
    "Optimiser le modèle et évaluer sa performance sur un ensemble de test.\n",
    "\n",
    "- Conclusion :  \n",
    "Ces étapes sont cruciales pour développer un programme efficace de prédiction des mouvements imaginaires de la main basé sur les données EEG.\n",
    "'''\n",
    "\n",
    "'''\n",
    "parts = []        #\n",
    "temp = [[], []]  # Les époques pour tous les cannaux et tous les évènements.\n",
    "spots = [[], []]  # Apparitions des évènements\n",
    "\n",
    "# Pour la standardisation du nombre d'échantillon max conservé\n",
    "ceil   = min([len(label_csv[i]['EventType']) for i in count])\n",
    "merge  = False\n",
    "\n",
    "# Extraction des données relavitives à l'apparition des évènements.\n",
    "for i in count :\n",
    "    df   = train_csv[i]\n",
    "    kind  = label_csv[i]['EventType'][: ceil]\n",
    "    loc = np.where(df['EventStart'] == 1)[: ceil]\n",
    "\n",
    "    parts.append(zero_removal(df['C3'], 75))\n",
    "\n",
    "    for i in num_events :\n",
    "        spots[i].append(np.array(*loc)[*np.where(kind == i)])\n",
    "\n",
    "        room = event_epochs(spots[i][-1], SCOPE, LAG)\n",
    "\n",
    "        temp[i].append([full_event(df[c], room, merge)for c in eeg_Channels])\n",
    "\n",
    "# Regroupement des données en fonction du type de l'évènement et du cannal d'observation\n",
    "if merge :\n",
    "    temp = [[[np.append([], T[j :: 3]) for T in temp[i]] for j in range(3)] for i in num_events]\n",
    "else :\n",
    "    store = [[[], [], []], [[], [], []]]\n",
    "    \n",
    "    [[[[store[i][j].append(G) for G in X] for j, X in enumerate(T)] for T in temp[i]] for i in num_events]\n",
    "\n",
    "    temp = store\n",
    "    # res = [[np.concatenate(np.stack(tries[i], axis = 1)[j], axis = 0) for j in range(3)] for i in n_type]\n",
    "\n",
    "eras    = [pd.DataFrame(dict(zip(eeg_Channels, [pd.Series(X) for X in temp[i]]))) for i in num_events]\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
