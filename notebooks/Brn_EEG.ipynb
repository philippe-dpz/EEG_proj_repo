{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Chargement des différentes librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Chargement des différentes librairies\n",
    "\n",
    "import sys, os, math, time\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "from src.thot.sesh import *\n",
    "from src.thot.catch_features import *\n",
    "\n",
    "# from scipy.fft import fft\n",
    "import pywt, librosa            # type: ignore\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "from sklearn import model_selection, preprocessing as sk_p      # type: ignore\n",
    "\n",
    "# Supervised learning\n",
    "from sklearn import ensemble, svm, neighbors, linear_model      # type: ignore\n",
    "# Unsupervised learning\n",
    "from sklearn import cluster\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline                           # type: ignore\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler  # type: ignore\n",
    "from sklearn.decomposition import PCA                           # type: ignore\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import VotingClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate # type: ignore\n",
    "from sklearn.metrics import classification_report               # type: ignore\n",
    "\n",
    "from keras.models import Sequential                             # type: ignore\n",
    "from keras.callbacks import EarlyStopping                       # type: ignore\n",
    "from keras.layers import GlobalAveragePooling1D, MaxPooling1D   # type: ignore\n",
    "from keras.layers import Dense, Dropout, Conv1D, LSTM, Flatten  # type: ignore\n",
    "from keras.layers import LeakyReLU, ReLU, PReLU, ConvLSTM1D\n",
    "# from keras.layers import Bidirectional, TimeDistributed, RepeatVector, Flatten\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.optimizers import AdamW, Adam            # type: ignore\n",
    "\n",
    "# from pyriemann.spatialfilters import CSP\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Déclaration de constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 250)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### • Déclaration de constantes\n",
    "\n",
    "# Fréquence d'échantillonnage - Hz (Nombre de valeur / sec)\n",
    "SAMPLE_RATE  = 250\n",
    "# Temps additionel pour étendre le domaines d'étude.\n",
    "LAG : int    = 0 # SAMPLE_RATE >> 2 # -62     # Décalage du signal dû signal ~250ms\n",
    "#\n",
    "PW2 : int    = int(np.floor(np.log2(SAMPLE_RATE))) # 2 << SAMPLE_RATE // 32\n",
    "#\n",
    "NFFT : int   = 1 << PW2\n",
    "# Epoque en sec donnée en nombre d'échantillon consectutif # 4\" de données (multiple de 2)\n",
    "SCOPE : int  = SAMPLE_RATE >> 0 # << 1 # (1 << PW2) * 4\n",
    "# Deux enregistrements bipolaires + neutre\n",
    "eeg_Chans    = ['C3', 'C4', 'Cz']\n",
    "# Liste des cannaux eeg associés aux évènement 0 et 1\n",
    "eeg_left     = [f'{c}_0' for c in eeg_Chans]\n",
    "eeg_right    = [f'{c}_1' for c in eeg_Chans]\n",
    "full_eeg     = eeg_left + eeg_right\n",
    "# Trois enregistrements musculaires\n",
    "ecg_Chans    = ['EOG:ch01', 'EOG:ch02', 'EOG:ch03']\n",
    "# Liste de tous les cannaux des dataframes\n",
    "all_chans    = eeg_Chans + ecg_Chans\n",
    "# Correspondance pour la classification\n",
    "hands_event  = {0: 'Left', 1: 'Right'}\n",
    "# Nombre dévènement à prédire\n",
    "num_events   = range(len(hands_event))\n",
    "# Les bandes de fréquences d'intérêt\n",
    "eeg_bands    = {'Delta' : (0.1, 4.0),\n",
    "                'Theta' : (4.1, 8.0),\n",
    "                'Alpha' : (8.1, 14.0),\n",
    "                'Beta'  : (14.1, 30.0),\n",
    "                'Gamma' : (30.1, (SAMPLE_RATE >> 1) - 1),}\n",
    "# Coefficients pour filtres Butterworth numérique d'ordre N pour le filtrage passe-bande\n",
    "bands_coeff   = {band : butter_bandpass(low, high, SAMPLE_RATE) for band, (low, high) in eeg_bands.items()}\n",
    "# Largeur de bande retenue pour étude de cas\n",
    "band_interest = butter_bandpass(1e-3, eeg_bands['Alpha'][1], SAMPLE_RATE)\n",
    "\n",
    "LAG, SCOPE #, *band_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time\n",
    "def split_and_merge(datas : list[Board], labels : list[Board] | None, Channels : Clause,\n",
    "                    events : int | Index, level : bool = True,\n",
    "                    merge : bool = False) -> tuple[Board, Index, Index] :\n",
    "    parts = []        #\n",
    "    temp  = [[], []]  # Les époques pour tous les cannaux et tous les évènements.\n",
    "    spots = [[], []]  # Apparitions des évènements\n",
    "    \n",
    "    # Pour la standardisation du nombre d'échantillon max conservé\n",
    "    loop = [len(x['EventType']) for x in labels]\n",
    "    ceil = [min(loop)] * len(datas) if level else loop\n",
    "\n",
    "    if type(events) == int : events = range(events)\n",
    "\n",
    "    # Extraction des données relavitives à l'apparition des évènements.\n",
    "    for i in range(len(datas)) :\n",
    "        input = datas[i]\n",
    "        types = labels[i]['EventType'][: ceil[i]]\n",
    "        sites = np.where(input['EventStart'] == 1)[: ceil[i]]\n",
    "\n",
    "        parts.append(zero_removal(input[Channels[0]], 75))\n",
    "\n",
    "        for j in events :\n",
    "            spots[j].append(np.array(*sites)[*np.where(types == j)])\n",
    "\n",
    "            room = event_epochs(spots[j][-1], SCOPE, LAG)\n",
    "\n",
    "            temp[j].append([full_event(input[c], room, merge) for c in Channels])\n",
    "\n",
    "    del loop, ceil\n",
    "\n",
    "    # Regroupement des données en fonction du type de l'évènement et du cannal d'observation\n",
    "    if merge :\n",
    "        n    = len(Channels)\n",
    "        temp = [[[np.append([], T[j :: n]) for T in temp[i]] for j in range(n)]\n",
    "                for i in events]\n",
    "    else :\n",
    "        pool = [[[], [], []], [[], [], []]]\n",
    "        \n",
    "        [[[[pool[i][j].append(x) for x in A] for j, A in enumerate(T)] for T in temp[i]]\n",
    "         for i in events]\n",
    "        \n",
    "        temp = pool\n",
    "    \n",
    "    eras = [pd.DataFrame({**dict(zip(Channels, [pd.Series(X) for X in temp[i]])), 'EventType': i})\n",
    "            for i in events]\n",
    "    \n",
    "    del pool, temp, room, types\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    return eras, spots, parts\n",
    "\n",
    "def simple_struct(data : Board, event : int, col : str) -> Board :\n",
    "    room = data[data[col] == event]\n",
    "    b, a = bands_coeff['Delta']\n",
    "    # b, a = band_interest[0], band_interest[1]\n",
    "\n",
    "    vals = [normalized(bandpass_filter(v4 - v3, b, a)) for v3, v4 in zip(room['C4'], room['C3'])]\n",
    "    # vals = [bandpass_filter(v4  - v3, b, a)\n",
    "    #         for v3, v4 in zip(room['C4'], room['C3'])]\n",
    "    # vals = [normalized(v4  - v3) for v3, v4 in zip(room['C4'], room['C3'])]\n",
    "    # vals = [v4  - v3 for v3, v4 in zip(room['C4'], room['C3'])]\n",
    "    \n",
    "    # _df = pd.DataFrame([*np.subtract(room['C4'], room['C3'])])\n",
    "    # _df = pd.DataFrame([normalized(v4) - normalized(v3) for v3, v4 in zip(room['C4'], room['C3'])])\n",
    "    _df = pd.DataFrame(vals)\n",
    "\n",
    "    _df[col] = event\n",
    "    \n",
    "    return _df\n",
    "\n",
    "#### • Try catch_22\n",
    "\n",
    "def titre(txt : str, size : int) -> str :\n",
    "    n   = len(txt)\n",
    "    avt = ((size - n) >> 1) - 1\n",
    "\n",
    "    return f\"{'-' * avt} {txt.upper()} {'-' * (size - (avt + n + 1))}\"\n",
    "\n",
    "def catch(data : Board, col : str, channels : Clause, event : int,\n",
    "            norm : bool = False) -> Board :\n",
    "    func = [np.min, np.max, np.median]\n",
    "    #, np.mean func = [np.min, np.max, np.std, np.var, np.mean, np.median]\n",
    "    # func = np.append(func, catch_)\n",
    "    room = data[data[col] == event]\n",
    "    name = [f.__name__ for f in func]\n",
    "    # head = [f\"{f}_diff\" for f in name]\n",
    "    head = [f\"{c}_{f}\" for c in channels for f in name] \\\n",
    "         + [f\"{f}_diff\" for f in name]\n",
    "    \n",
    "    if norm :\n",
    "        temp = [[normalized(x) for x in room[c]] for c in channels]\n",
    "        c3, c4 = [[[f(v) for v in s] for f in func] for s in temp]\n",
    "    else :\n",
    "        c3, c4 = [[[f(v) for v in room[c]] for f in func] for c in channels]\n",
    "    \n",
    "    sub = np.subtract(c3, c4)\n",
    "    _df = pd.DataFrame(np.array((*c3, *c4, *sub)).T, columns = head)\n",
    "    # _df = pd.DataFrame(np.array(sub).T, columns = head)\n",
    "\n",
    "    print(np.array((np.stack([c3, c4], axis= 1), *sub)))\n",
    "\n",
    "    # display(_df)\n",
    "    \n",
    "    _df[col] = event\n",
    "    \n",
    "    return _df\n",
    "\n",
    "def pool_(data : Board, event : int) -> Board :\n",
    "    return catch(data, 'EventType', eeg_Chans[: -1], event, norm = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Acquisition des données d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Acquisition des données d'entrainement\n",
    "\n",
    "target   = \"../data/data.zip\"\n",
    "size     = len('train/')\n",
    "files    = [x[size :] for x in files_in_zip(target, directory = 'train')]\n",
    "# files    = [f for f in files if f not in files[:: -3]]\n",
    "# files    = files[:: -3]\n",
    "\n",
    "# Acquisition des fichiers du répertoir dans le fichier zip\n",
    "entrants  = csv_in_zip(target, directory = 'train', files = files)\n",
    "label_tmp = csv_in_zip(target, directory = 'y_train_only', files = files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Acquisition des données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Acquisition des données de test\n",
    "\n",
    "test_csv = csv_in_zip(target, directory = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Pré-traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers d'entrainements :\n",
      "  B0103T.csv B0203T.csv B0303T.csv B0403T.csv B0503T.csv B0603T.csv B0703T.csv B0803T.csv B0903T.csv\n",
      "\n",
      "Fichiers tests :\n",
      "  B0101T.csv B0102T.csv B0201T.csv B0202T.csv B0301T.csv B0302T.csv B0401T.csv B0402T.csv B0501T.csv B0502T.csv B0601T.csv B0602T.csv B0701T.csv B0702T.csv B0801T.csv B0802T.csv B0901T.csv B0902T.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_files = len(files)\n",
    "size    = range(n_files)\n",
    "files   = np.array(files)\n",
    "unic    = len(np.unique([x[2] for x in files]))\n",
    "step    = n_files // unic\n",
    "offre   = -1\n",
    "\n",
    "match offre :\n",
    "    case 1 | 2 :\n",
    "        n_test    = np.array([1, 3]) if offre == 2 else \\\n",
    "                    single_draw(1, unic, math.ceil(.2 * unic))\n",
    "        test_pos  = [range(i, i + step) for i in (n_test - 1) * step]\n",
    "        test_pos  = np.append([], test_pos).astype(int)\n",
    "        train_pos = [i for i in size if i not in test_pos]\n",
    "\n",
    "        print(*n_test, '\\n')\n",
    "    case 3 :\n",
    "        n_test    = np.random.randint(1, unic) - 1\n",
    "        i         = n_test * step\n",
    "        test_pos  = range(i, i + step)\n",
    "        train_pos = [i for i in size[:: -3] if i not in test_pos]\n",
    "        test_pos  = test_pos[: step - 1]\n",
    "    case 4 :\n",
    "        train_pos, test_pos, _, _ = train_test_split(size, size, test_size = .2, random_state = 42)\n",
    "    case _ :\n",
    "        train_pos = np.flip(size[:: -3])\n",
    "        test_pos  = [i for i in size if i not in train_pos]\n",
    "\n",
    "# -------------------- Train --------------------\n",
    "train_files = files[train_pos]\n",
    "train_csv   = [entrants[i] for i in train_pos]\n",
    "train_label = [label_tmp[i] for i in train_pos]\n",
    "# --------------------- Test --------------------\n",
    "test_files  = files[test_pos]\n",
    "test_csv    = [entrants[i] for i in test_pos]\n",
    "test_label  = [label_tmp[i] for i in test_pos]\n",
    "\n",
    "print(\"Fichiers d'entrainements :\\n \", *train_files)\n",
    "print()\n",
    "print(\"Fichiers tests :\\n \", *test_files)\n",
    "print()\n",
    "\n",
    "### • Pré-traitement des données\n",
    "\n",
    "train_runs, train_spots, train_parts = split_and_merge(train_csv, train_label, eeg_Chans, num_events)\n",
    "test_runs, test_spots, test_parts    = split_and_merge(test_csv, test_label, eeg_Chans, num_events)\n",
    "\n",
    "df_trains = pd.concat(train_runs, ignore_index = True)\n",
    "df_test   = pd.concat(test_runs, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ CUSTOME 'TRAIN_TEST_SPLIT' -------\n",
      "---------------- TRAIN ------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1440 entries, 232 to 651\n",
      "Columns: 251 entries, 0 to EventType\n",
      "dtypes: float64(250), int64(1)\n",
      "memory usage: 2.8 MB\n",
      "None\n",
      "----------------- TEST ------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2160 entries, 2026 to 144\n",
      "Columns: 251 entries, 0 to EventType\n",
      "dtypes: float64(250), int64(1)\n",
      "memory usage: 4.2 MB\n",
      "None\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.concat((simple_struct(df_trains, i, 'EventType') for i in num_events), ignore_index = True)\n",
    "X_test  = pd.concat((simple_struct(df_test, i, 'EventType') for i in num_events), ignore_index = True)\n",
    "\n",
    "# X_train = pd.concat((pool_(df_trains, i) for i in num_events), ignore_index = True)\n",
    "# X_test  = pd.concat((pool_(df_test, i) for i in num_events), ignore_index = True)\n",
    "\n",
    "# Pour éviter les biais d'apprentissage\n",
    "X_train = X_train.sample(frac = 1)\n",
    "X_test  = X_test.sample(frac = 1)\n",
    "\n",
    "print(titre(\"Custome 'train_test_split'\", 40))\n",
    "print(titre('train', 40))\n",
    "print(X_train.info())\n",
    "print(titre('test', 40))\n",
    "print(X_test.info())\n",
    "print('-' * 40)\n",
    "\n",
    "y_train = X_train['EventType']\n",
    "y_test  = X_test ['EventType']\n",
    "\n",
    "X_train.drop(columns = ['EventType'], inplace = True)\n",
    "X_test.drop (columns = ['EventType'], inplace = True)\n",
    "\n",
    "# display(X_train)\n",
    "# display(X_test)\n",
    "# print(sum(X_test.isna()))\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_trains.drop(columns = ['EventType']),\n",
    "#                                                     df_trains['EventType'], test_size = .2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test de classification - Proposition 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• RandomForestClassifier(n_jobs=-1) : Accuracy -> 70.0% (±0.049, max : 73.6%)\n",
      "\t-> Classification report [ Test-score / : 62.2% ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.625     0.609     0.617      1080\n",
      "           1      0.619     0.635     0.627      1080\n",
      "\n",
      "    accuracy                          0.622      2160\n",
      "   macro avg      0.622     0.622     0.622      2160\n",
      "weighted avg      0.622     0.622     0.622      2160\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prono</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vrai</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>658</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>394</td>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Prono    0    1\n",
       "Vrai           \n",
       "0      658  422\n",
       "1      394  686"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• SVC(kernel='poly') : Accuracy -> 71.9% (±0.032, max : 75.0%)\n",
      "\t-> Classification report [ Test-score / : 62.7% ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.634     0.600     0.617      1080\n",
      "           1      0.620     0.654     0.637      1080\n",
      "\n",
      "    accuracy                          0.627      2160\n",
      "   macro avg      0.627     0.627     0.627      2160\n",
      "weighted avg      0.627     0.627     0.627      2160\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prono</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vrai</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>648</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>374</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Prono    0    1\n",
       "Vrai           \n",
       "0      648  432\n",
       "1      374  706"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• LogisticRegression(solver='newton-cholesky') : Accuracy -> 72.8% (±0.024, max : 75.0%)\n",
      "\t-> Classification report [ Test-score / : 64.4% ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.653     0.616     0.634      1080\n",
      "           1      0.636     0.672     0.654      1080\n",
      "\n",
      "    accuracy                          0.644      2160\n",
      "   macro avg      0.644     0.644     0.644      2160\n",
      "weighted avg      0.644     0.644     0.644      2160\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prono</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vrai</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>665</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>354</td>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Prono    0    1\n",
       "Vrai           \n",
       "0      665  415\n",
       "1      354  726"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# csp = CSP(nfilter = 2)\n",
    "# std = RobustScaler()\n",
    "pca = PCA(SCOPE >> 4)\n",
    "std = StandardScaler()\n",
    "\n",
    "# X_train_scaled = std.fit_transform(X_train)\n",
    "# X_test_scaled  = std.transform(X_test)\n",
    "\n",
    "# X_train_scaled = pca.fit_transform(X_train)\n",
    "# X_test_scaled  = pca.transform(X_test) \n",
    "\n",
    "# print(*pca.singular_values_.shape)\n",
    "\n",
    "X_train_scaled = X_train\n",
    "X_test_scaled  = X_test\n",
    "\n",
    "#\n",
    "clf = svm.SVC(gamma = 'scale', kernel = 'poly')\n",
    "# \n",
    "rfc = ensemble.RandomForestClassifier(n_jobs = -1)\n",
    "#\n",
    "lrg = linear_model.LogisticRegression(solver = 'newton-cholesky') # 'saga' ''\n",
    "# #\n",
    "# knc = neighbors.KNeighborsClassifier() # .RadiusNeighborsRegressor()\n",
    "# #\n",
    "# lsg = linear_model.SGDClassifier()\n",
    "# # \n",
    "# gbc = ensemble.GradientBoostingClassifier()\n",
    "# # \n",
    "# nnp = MLPClassifier(solver = 'sgd', learning_rate = 'invscaling')\n",
    "# # \n",
    "# kmn = cluster.KMeans(n_clusters = 2, algorithm='elkan')\n",
    "\n",
    "for reg in [rfc, clf, lrg] :\n",
    "    scores : dict = cross_validate(reg, X_train_scaled, y_train, scoring = ['accuracy'])\n",
    "    \n",
    "    reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "    res  = scores['test_accuracy']\n",
    "    pred = reg.predict(X_test_scaled)\n",
    "    \n",
    "    print(f\"• {reg} : Accuracy -> {res.mean():.1%} (±{res.std():.2}, max : {res.max():.1%})\")\n",
    "    print(f\"\\t-> Classification report [ Test-score / : {reg.score(X_test_scaled, y_test):.1%} ]\")\n",
    "    print(classification_report(y_test, pred, digits = 3))\n",
    "    # print(f\"\\t-> ● Accuracy score : {accuracy_score(y_test, pred):.1%}\")\n",
    "    display(pd.crosstab(y_test, pred, rownames = ['Vrai'], colnames = ['Prono']))\n",
    "\n",
    "#ExtraTreesClassifier \n",
    "# Voting_clf = VotingClassifier(estimators = [('knn', clf1), ('svm', clf2), ('rf', clf3)], voting = 'hard')\n",
    "# cv3        = model_selection.KFold(n_splits = 3, random_state = 42, shuffle = True), clf4\n",
    "\n",
    "# Create a pipeline\n",
    "# pip = Pipeline([('RFC', rfc), ('SVM', clf)])    # ('CSP', csp), \n",
    "\n",
    "# pip.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = pip.predict(X_test)\n",
    "\n",
    "# y_train = np.array(y_train)\n",
    "\n",
    "    # if 'cluster' in type(reg).__name__ :\n",
    "    #     pred = reg.fit_predict(X_test_scaled)\n",
    "    # else :\n",
    "    #     pred = reg.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://raphaelvallat.com/bandpower.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes   = filename(train_files)\n",
    "headers = [f\"{t} . {i + 1}\" for i, t in enumerate(notes)]\n",
    "count   = range(len(train_files))\n",
    "\n",
    "pd.DataFrame(np.stack(df_trains['C4'] - df_trains['C3'], axis = 0))\n",
    "\n",
    "df_trains['C4'] @ df_trains['C3'] # Produit scalaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation des spectrogrammes / Test\n",
    "\n",
    "def logMelSpectrogram(data : Vector, rate : int, dt : float = 1e-2) -> Vector :\n",
    "    tps = 1 << int(np.floor(np.log2(rate * dt)))\n",
    "    # print(tps)\n",
    "    # Spectrogramme\n",
    "    stfts = np.abs(librosa.stft(y = data, n_fft = tps, hop_length = 1 << 2, center = True)).T\n",
    "    # Filtre de MEL\n",
    "    liny  = librosa.filters.mel(sr = rate, n_fft = tps + 1, n_mels = stfts.shape[-1]).T\n",
    "    # Application du filtre au spectrogramme\n",
    "    mel_  = np.tensordot(stfts, liny, 1)\n",
    "\n",
    "    return np.log(mel_ + 1e-6)\n",
    "    \n",
    "def structure(data : Board | Vector, rate : int, whr : Clause) -> Vector :\n",
    "    # return np.array([logMelSpectrogram(X, rate, 2) for X in data[whr]])\n",
    "    # return np.stack([[signal.welch(X, rate)[1] for X in data[c]] for c in whr], axis = 2)\n",
    "    return np.stack(df_trains['C4'] - df_trains['C3'], axis = 0)\n",
    "    # return np.stack([[X for X in data[c]] for c in whr], axis = 2)\n",
    "\n",
    "def img_spectrogram(raw : Vector, rate : int, nfft : int = 1 << 10) -> Vector :\n",
    "    return librosa.feature.melspectrogram(y = raw, sr = rate, hop_length = 1, \n",
    "                            n_fft = nfft, n_mels = 32, fmin = 0, fmax = 20, win_length = 32)\n",
    "\n",
    "def spectrogram_dep(data : Board, rate : int, channels : Clause, n_row : int = 5, n_col : int = 12) :\n",
    "    sample = np.random.default_rng().integers(data.shape[0], size = n_row)\n",
    "\n",
    "    sample.sort()\n",
    "\n",
    "    plt.figure(figsize = (18, 2 * .48 * n_row))\n",
    "\n",
    "    pos = 0\n",
    "\n",
    "    for k in sample :\n",
    "        for c in channels :\n",
    "            # x   = normalized(data[c][k])\n",
    "            raw = img_spectrogram(raw = data[c][k], rate = rate)\n",
    "            pos += 1\n",
    "            \n",
    "            plt.subplot(n_row, n_col, pos)\n",
    "            plt.title(f\"{((pos - 1) // 3) + 1} . {k} - {c}\", fontsize = 8)\n",
    "            librosa.display.specshow(data = 1 - raw, sr = rate, hop_length = 1)\n",
    "            \n",
    "            # pos = n_col * (i >> 1) + j\n",
    "            # f, t, Sxx = signal.spectrogram(x, rate)\n",
    "            # plt.subplot(n_row, n_col, pos + 4)\n",
    "            # plt.pcolormesh(t, f, 1 - Sxx, shading = 'gouraud')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show();\n",
    "\n",
    "def spectrogram(data : Board, rate : int, channels : Clause, n_row : int = 5, n_col : int = 12) :\n",
    "    sample = np.random.default_rng().integers(data.shape[0], size = n_row)\n",
    "\n",
    "    sample.sort()\n",
    "\n",
    "    plt.figure(figsize = (18, 2 * .48 * n_row))\n",
    "\n",
    "    freq = np.arange(1, rate >> 1)\n",
    "    pos  = 0\n",
    "    # extd = np.append([0, 1, 1], freq[-1])\n",
    "    \n",
    "    for k in sample :\n",
    "        for c in channels :\n",
    "            pos += 1\n",
    "            x   = normalized(data[c][k])\n",
    "            coefficients, _ = pywt.cwt(x, scales = freq, wavelet = 'cmor')\n",
    "\n",
    "            plt.subplot(n_row, n_col, pos)\n",
    "            plt.imshow(np.abs(coefficients), aspect = 'auto', cmap = 'jet') #, extent = extd\n",
    "            # plt.colorbar(label=\"Magnitude\")\n",
    "            # plt.ylabel(\"Scale\")\n",
    "            # plt.xlabel(\"Time\")\n",
    "            # plt.title(\"CWT of a Chirp Signal\")\n",
    "            plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_logMelSpectrogram(data, rate) :\n",
    "    sns.heatmap(np.rot90(logMelSpectrogram(data, rate)), cmap = 'inferno')\n",
    "    \n",
    "    # loc, _ = plt.xticks()\n",
    "    # l      = np.round((loc - loc.min()) * len(data) / fe / loc.max(), 2), vmin = -6\n",
    "\n",
    "    # plt.xticks(loc, l)\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Frequency (Mel)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients, frequencies = pywt.cwt(train_runs[0]['C3'][0], scales = np.arange(1, SAMPLE_RATE >> 1), wavelet = 'cmor')\n",
    "\n",
    "1 / frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = [list(harmonic(x, bands_coeff).values()) for x in [df_trains[c] for c in eeg_Chans]]\n",
    "\n",
    "np.shape(H), np.shape(np.stack(np.stack(H, axis = 1), axis = 2)), np.shape(H[0][1][0])\n",
    "# harmonic(trains['C3'][256], bands_coeff).values())), H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_trains.drop(columns = ['EventType']),\n",
    "                                                    df_trains['EventType'], test_size = .2, random_state = 42)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split([v.tolist() for v in np.array(trains[eeg_Channels])],\n",
    "#                                                     trains['EventType'], test_size = .2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = np.array(X_train)\n",
    "# test_dataset  = X_test\n",
    "\n",
    "# train_dataset = structure(X_train, SAMPLE_RATE, eeg_Chans[:2])\n",
    "# test_dataset  = structure(X_test, SAMPLE_RATE, eeg_Chans[:2])\n",
    "\n",
    "# train_dataset = structure(X_train, SAMPLE_RATE, eeg_Chans[: 2])\n",
    "# test_dataset  = structure(X_test, SAMPLE_RATE, eeg_Chans[: 2])\n",
    "\n",
    "train_dataset = structure(df_trains.drop(columns = ['EventType']), SAMPLE_RATE, eeg_Chans[: 2])\n",
    "test_dataset  = structure(df_test.drop(columns = ['EventType']), SAMPLE_RATE, eeg_Chans[: 2])\n",
    "\n",
    "# print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### • Test prédiction\n",
    "\n",
    "# UNITS     : int   = 100\n",
    "BATCHSIZE : int   = 32\n",
    "EPOCH     : int   = 1000\n",
    "ZERO      : int   = 32\n",
    "DROPOUT   : float = 1 / 4\n",
    "\n",
    "OPTIMIZER = 'AdamW'     # adamax, , adafactor, adam, nadam\n",
    "# kl_divergence mean_squared_logarithmic_error mean_absolute_error\n",
    "LOSS      = 'sparse_categorical_crossentropy'\n",
    "ACTIV     = ReLU   # PReLU, LeakyReLU\n",
    "K_SIZE    = (5)\n",
    "\n",
    "model = Sequential([\n",
    "    # - Couche 1 -\n",
    "    Conv1D(filters = ZERO, kernel_size = K_SIZE, dilation_rate = 2,\n",
    "           input_shape = train_dataset.shape),\n",
    "    ACTIV(),\n",
    "    MaxPooling1D(pool_size = 2, strides = 1),\n",
    "    Dropout(rate = DROPOUT),\n",
    "    # - Couche 2 -\n",
    "    Conv1D(filters = ZERO << 1, kernel_size = K_SIZE, dilation_rate = 2),\n",
    "    ACTIV(),\n",
    "    MaxPooling1D(pool_size = 2, strides = 1),\n",
    "    Dropout(rate = DROPOUT),\n",
    "    # - Couche 3 -\n",
    "    Conv1D(filters = ZERO << 2, kernel_size = K_SIZE, dilation_rate = 2),\n",
    "    ACTIV(),\n",
    "    MaxPooling1D(pool_size = 2, strides = 1),\n",
    "    Dropout(rate = DROPOUT),\n",
    "    # - Flatten layer -\n",
    "    Flatten(),\n",
    "    GlobalAveragePooling1D(),\n",
    "    # - Couches de sortie -\n",
    "    Dense(units = ZERO << 2),\n",
    "    ACTIV(),\n",
    "    Dropout(rate = DROPOUT),\n",
    "    Dense(units = len(hands_event), activation = 'softmax'), # sigmoid \n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer = OPTIMIZER, loss = LOSS, metrics = ['acc'])\n",
    "\n",
    "print()\n",
    "\n",
    "stop    = EarlyStopping(monitor = 'val_accuracy', mode = 'max', verbose = 1, patience = 50)\n",
    "history = model.fit(train_dataset, y_train, validation_data = (test_dataset, y_test), verbose = 1,\n",
    "                    batch_size = BATCHSIZE, epochs = EPOCH, callbacks = [stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_dataset)\n",
    "\n",
    "sum([np.where(x > .5)[0][0] for x in pred] == y_test) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values  = history_dict['loss']\n",
    "acc_values   = history_dict['accuracy']\n",
    "absc         = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.figure(figsize = (12, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(absc, loss_values, label = 'Loss')\n",
    "plt.plot(absc, acc_values, label = 'Accuracy')\n",
    "plt.title('Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(absc, history_dict['val_loss'], label = 'Loss')\n",
    "plt.plot(absc, history_dict['val_accuracy'], label = 'Accuracy')\n",
    "plt.title('Testing')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation densité spectrale du Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Densité Spectral du Signal\n",
    "\n",
    "plot_psd(entrants, train_runs, rate = SAMPLE_RATE, Channels = eeg_Chans, titled = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Densité spectrale / échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Densité spectral / échantillon\n",
    "\n",
    "inc   = 40\n",
    "scp = samples(train_samples, inc)\n",
    "boolInt = -2\n",
    "\n",
    "plt.figure(figsize = (15, inc * 1.5))\n",
    "\n",
    "for i in scp :\n",
    "    boolInt += 2\n",
    "\n",
    "    for c in eeg_Chans :\n",
    "        y = train_runs[0][c][i]\n",
    "        yest, Pxx_den = signal.welch(y, SAMPLE_RATE)   # , scaling = 'spectrum'\n",
    "        \n",
    "        plt.subplot(inc, 4, boolInt + 1)\n",
    "        plt.semilogy(yest, Pxx_den, label = c)\n",
    "        plt.title(f\"welch - {i + 1}\", fontsize = 11)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(inc, 4, boolInt + 2)\n",
    "        res, _ = plt.psd(y, Fs = SAMPLE_RATE, label = c) # , NFFT = NFFT\n",
    "        plt.title(f\"psd - {i + 1}\", fontsize = 11)\n",
    "        plt.xlabel('')\n",
    "        plt.ylabel('')\n",
    "        # plt.legend()\n",
    "\n",
    "plt.xlabel('frequency [Hz]')\n",
    "# plt.ylabel('PSD [V**2/Hz]')\n",
    "plt.tight_layout()\n",
    "plt.show();\n",
    "\n",
    "# f, Pxx_den = signal.welch(train_eras[0]['C3'][752], SAMPLE_RATE)\n",
    "\n",
    "# print(len(Pxx_den))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation Epoques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Epoques\n",
    "\n",
    "for i in range(len(files))[:: 3] :\n",
    "    plot_signal(entrants[i], train_parts[i], train_spots[0][i], train_spots[1][i], channels = eeg_Chans, # \n",
    "                period = SCOPE, lag = LAG, title = headers[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation décomposition des signaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Décomposition des signaux\n",
    "\n",
    "# Test de décomposition des signaux en bandes de fréquences spécifiques compatibles avec les répartitions usuelles\n",
    "# dans le domaine des EEG ['Delta', 'Theta', 'Alpha', 'Beta', 'Gamma']\n",
    "\n",
    "for input, token in zip(train_runs, ['Gauche', 'Droite']) :\n",
    "    print(f\"Exemples - Évènement Discriminé Main {token}\")\n",
    "    plot_wavelets(input, bands_coeff, eeg_Chans, scope = 30, headers = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation des spectrogrammes (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in zip(num_events, ['Gauche', 'Droite']) :\n",
    "    print(f\"Exemples - Évènement Discriminé Main {t}\")\n",
    "    spectrogram(train_runs[i], SAMPLE_RATE, eeg_Chans, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test PCA - (Non cloncluant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Test PCA - (Non cloncluant)\n",
    "\n",
    "nca = SCOPE >> 2\n",
    "pca = PCA(nca)\n",
    "\n",
    "_, ax = plt.subplots(nrows = 2, ncols = 3, figsize = (15, 5))\n",
    "\n",
    "for i, d in enumerate(train_runs) :\n",
    "    for j, c in enumerate(eeg_Chans) :\n",
    "        Z = std.fit_transform(list(d[c].to_list())) # \n",
    "        principal_components = pca.fit_transform(Z)\n",
    "        \n",
    "        ax[i, j].plot(range(nca), np.cumsum(pca.explained_variance_ratio_))\n",
    "        ax[i, j].set_title(f'{c} . {i}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • MNE époque (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • MNE époque (test)\n",
    "\n",
    "raw_csv = entrants[0][eeg_Chans]\n",
    "info    = mne.create_info(ch_names = eeg_Chans, sfreq = SAMPLE_RATE, ch_types = 'eeg')\n",
    "raw_mne = mne.io.RawArray(raw_csv.T * 1e-6, info)\n",
    "sites     = np.where(entrants[0]['EventStart'] == 1)[0]\n",
    "\n",
    "# display(compare(np.sort(np.concatenate((train_spots[0][0], train_spots[1][0]))), loc))\n",
    "\n",
    "tmin, tmax = -0., 1\n",
    "\n",
    "# loc = mne.find_events(raw_mne, stim_channel = 'C3')\n",
    "# event_id = dict(C3 = 1, aud_r = 2, vis_l = 3, vis_r = 4)\n",
    "# raw = mne.io.Raw(raw_mne, preload = True)\n",
    "# raw.filter(2, None, method = 'iir')           # replace baselining with high-pass\n",
    "# events = mne.read_events(event_fname)\n",
    "\n",
    "# raw.info['bads'] = ['MEG 2443']  # set bad channels\n",
    "# picks = mne.pick_types(info, meg = 'grad', eeg = True, eog = False, exclude = 'bads')\n",
    "# Read epochs\n",
    "absc = mne.Epochs(raw_mne, np.array([sites, sites, sites]).T, None, tmin, tmax, proj = False,\n",
    "                    picks = None, baseline = None, preload = True, verbose = False) # event_id picks\n",
    "\n",
    "# labels = epochs.events[::5, -1]\n",
    "\n",
    "# events\n",
    "\n",
    "# raw_mne.plot();\n",
    "\n",
    "# raw_mne['C3'][0][0], len(df_train_csv[2]['Cz'])\n",
    "\n",
    "display(absc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test de classification - Proposition 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### • Test de classification - Proposition inputs 01\n",
    "\n",
    "# X1 = np.where(trains['EventType'] == 0, trains['C3'], trains['Cz']) \n",
    "# X2 = np.where(trains['EventType'] == 0, trains['Cz'], trains['C4']) \n",
    "# X = [signal.welch(list(v), SAMPLE_RATE)[1] for v in X1 + X2]\n",
    "\n",
    "# X = [np.append([], list(harmonic(v, bands_coeff).values())) for v in X]\n",
    "# X = np.where(trains['EventType'] == 0, trains['C3'], 2 * trains['Cz'])\n",
    "# X = [signal.welch(list(v), SAMPLE_RATE)[1] for v in X], trains['C3'] + trains['Cz'], trains['C4'] + trains['Cz']\n",
    "\n",
    "# diff = np.array([np.cos(x) for x in (trains['C3'] ** 2 + trains['C4'] ** 2)])  + 2 * trains['Cz']\n",
    "\n",
    "# dist =np.array([list(np.sqrt(x)) for x in (trains['C3'] ** 2 + trains['C4'] ** 2)])\n",
    "# display(np.shape(np.array()))\n",
    "\n",
    "# diff = np.array([d - v for v, d in zip(, dist)]) - trains['Cz'] * [x.mean() for x in trains['Cz']]\n",
    "\n",
    "# cz_min = [v.min() for v in trains['Cz']]\n",
    "# cz_max = [v.max() for v in trains['Cz']]\n",
    "\n",
    "# display(compare(cz_min, cz_max), )\n",
    "\n",
    "# trio = zip(trains['C4'], trains['C3'], trains['Cz'])\n",
    "\n",
    "# prd = [(c3 - c4) * cz for c3, c4, cz in trio] \n",
    "\n",
    "# 'Delta', 'Theta', 'Alpha', 'Beta', 'Gamma'\n",
    "\n",
    "# b, a = bands_coeff['Delta']\n",
    "\n",
    "# c3 = np.array([bandpass_filter(bw, b, a) for bw in trains['C3']])\n",
    "# c4 = np.array([bandpass_filter(bw, b, a) for bw in trains['C4']])\n",
    "\n",
    "# diff = (c4 - c3) / trains['Cz'].max() #/ cz.max() # [for v in cz]\n",
    "# X = [np.sign(s) for s in trains['C4'] - trains['C3']]\n",
    "# X = np.array([normalized(bandpass_filter(bw, b, a)) for bw in X])\n",
    "\n",
    "# X = [sk_p.minmax_scale(pywt.dwt(c4, wavelet = 'db4')[0] - pywt.dwt(c3, wavelet = 'db4')[0]) for c3, c4 in zip(trains['C3'], trains['C4'])]\n",
    "# X = [v) for v in X]\n",
    "\n",
    "# X = [pywt.dwt(v, wavelet = 'db4')[0] for v in X]\n",
    "# X = [normalized(pywt.dwt(v, wavelet = 'db4')[0]) for v in X]\n",
    "# X = [normalized(pywt.dwt(v, wavelet = 'db4')[0]) for v in X]\n",
    "# X = [normalized(v) for v in X]\n",
    "\n",
    "# X = [np.append([], v.tolist()) for v in X]\n",
    "\n",
    "# ret = pywt.dwt(data, wavelet = 'db1') #, level = 4, mode = 'antisymmetric'\n",
    "\n",
    "# for o in ret :\n",
    "#     print(np.shape(o))\n",
    "\n",
    "# # np.shape(ret[3])[0] / np.shape(train_csv[0]['Cz'])[0]\n",
    "# # (ret)\n",
    "\n",
    "# plt.figure(figsize = (20, 4))\n",
    "\n",
    "# # plt.subplot(1, 3, 1)\n",
    "# plt.plot(data)\n",
    "\n",
    "# for o in ret :\n",
    "#     plt.plot(o)\n",
    "\n",
    "# # plt.title(\"Original Signal\")\n",
    "# # plt.subplot(1, 3, 2)\n",
    "# # plt.title(\"Approximation Coefficients\")\n",
    "# # plt.subplot(1, 3, 3)\n",
    "# # plt.plot(cD)\n",
    "# # plt.title(\"Detail Coefficients\")\n",
    "# # plt.tight_layout()\n",
    "# plt.show();\n",
    "\n",
    "# train_files, train_csv, train_label = [], [], []\n",
    "# test_files, test_csv, test_label    = [], [], []\n",
    "\n",
    "# for i in range(n_files) :\n",
    "#     sites = np.where(entrants[i]['EventStart'] == 1)\n",
    "\n",
    "#     if i // step in n_test :\n",
    "#         test_files.append(files[i])\n",
    "#         test_csv  .append(entrants[i])\n",
    "#         test_label.append(label_tmp[i])\n",
    "#     else :\n",
    "#         train_files.append(files[i])\n",
    "#         train_csv  .append(entrants[i])\n",
    "#         train_label.append(label_tmp[i])\n",
    "\n",
    "\n",
    "\n",
    "# print((1, *X_train_scaled.shape[:1]))\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add()\n",
    "# model.add()\n",
    "# model.add()\n",
    "\n",
    "# model.add()\n",
    "# model.add()\n",
    "# model.add()\n",
    "\n",
    "# model.add()\n",
    "# model.add()\n",
    "# model.add()\n",
    "# model.add()\n",
    "\n",
    "# model.add()\n",
    "# model.add()\n",
    "# model.add()\n",
    "# model.add() \n",
    "\n",
    "'''\n",
    "# Ajout de la premiere couche lstm\n",
    "model.add(LSTM(ZERO, input_shape = train_dataset.shape[1:], activation = ACTIV(), return_sequences = True)) #\n",
    "model.add(LSTM(ZERO, dropout = DROPOUT, return_sequences = False))\n",
    "\n",
    "# Ajout de la couche de sortie\n",
    "model.add(Dense(len(hands_event), activation = 'softmax'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, y = bands_coeff['Theta']\n",
    "\n",
    "i = np.random.randint(np.shape(X)[0])\n",
    "y = df_trains['C4'][i] - df_trains['C3'][i]\n",
    "# X = [np.sign(s) for s in (trains['C4'] - trains['C3'])]\n",
    "# y = {band: bandpass_filter(X[i], b, a) for band, (b, a) in bands_coeff.items()}\n",
    "\n",
    "plt.figure()\n",
    "# plt.style.use('')\n",
    "\n",
    "plt.plot(normalized(bandpass_filter(y, b, y)), label = 'C4 - C3')\n",
    "plt.plot(normalized(bandpass_filter(np.sign(y), b, y)), label = '[C4 - C3]', c = np.random.rand(1, 3)[0])\n",
    "# plt.plot(pywt.dwt(y, wavelet = 'db4')[0], label = 'C4 - C3')\n",
    "# plt.plot(pywt.dwt(np.sign(y), wavelet = 'db4')[0], label = '[C4 - C3]')\n",
    "# plt.plot(np.zeros(512), ls = '--', c = np.random.rand(1, 3)[0])\n",
    "\n",
    "# print(np.shape(pywt.dwt(y, wavelet = 'db8')))\n",
    "\n",
    "# plt.plot(bandpass_filter(np.sign(trains['C3'][i]), b, a), label = 'C3')\n",
    "# plt.plot(bandpass_filter(np.sign(trains['C4'][i]), b, a), label = 'C4')\n",
    "# plt.plot(bandpass_filter(np.sign(trains['Cz'][i]), b, a), label = 'Cz')\n",
    "\n",
    "# for (band, signal) in reversed(y.items()) :\n",
    "#     plt.plot(pd.Series(signal), label = f'{band}', c = np.random.rand(1, 3)[0])\n",
    "\n",
    "plt.title(f\"{i}\")\n",
    "plt.legend(loc = 'upper right')\n",
    "\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    ## K-plus proches voisins\n",
    "    'knn__n_neighbors' : range(2),\n",
    "    ## SVM\n",
    "    'svm__C'      : [0.1, 1, 5],\n",
    "    'svm__kernel' : ['linear', 'softmax', 'sigmoid', 'rbf'],\n",
    "    ## RandomForest\n",
    "    # 'rf__max_features'      : ['sqrt', 'log2', None],\n",
    "    # 'rf__min_samples_split' : range(2, 32, 2),\n",
    "    # , ('rf', clf3), ('rf', clf3)\n",
    "    'estimators': [[('knn', knc), ('svm', svm)], [('knn', knc), ('svm', svm)]] \n",
    "    }\n",
    "\n",
    "grid = model_selection.GridSearchCV(estimator = Voting_clf, param_grid = params, cv = 5) \\\n",
    "    .fit(X_train_scaled, y_train)\n",
    "\n",
    "# parametres = {'max_features' : ['log2', 'sqrt', None], 'min_samples_split' : range(2, 32, 2)}\n",
    "\n",
    "# vclf = model_selection.GridSearchCV(estimator = clf3, param_grid = parametres, cv = 3) \\\n",
    "#     .fit(X_train_scaled, y_train)\n",
    "\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n",
    "print('score train :', grid.score(X_train_scaled, y_train))\n",
    "print('score test :', grid.score(X_test_scaled, y_test))\n",
    "\n",
    "# print(vclf.best_estimator_, vclf.best_params_, vclf.best_score_)\n",
    "# print('score train :', grid.score(X_train_scaled, y_train), vclf.score(X_train_scaled, y_train))\n",
    "# print('score test  :', grid.score(X_test_scaled, y_test), vclf.score(X_test_scaled, y_test))\n",
    "\n",
    "df_train_cpy, event_start = fancy_df(entrants, label_tmp['EventType'], hands_event, SCOPE)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize = (24, 5), sharey = True)\n",
    "sig = .05\n",
    "\n",
    "axes.plot(entrants['C3'])\n",
    "\n",
    "for p in event_start :\n",
    "    axes.axvspan(p[0] - (SCOPE >> 1), p[0] + 1.5 * SCOPE, facecolor = 'orangered', alpha = .5)\n",
    "\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolInt = 16\n",
    "start   = event_start[boolInt][0]\n",
    "entrant = start + SCOPE\n",
    "input   = df_train_cpy['C3_4'][start : entrant]\n",
    "smooth  = input.copy()\n",
    "inc     = 5\n",
    "alpha   = 1 / 3\n",
    "dec     = int(inc / alpha)\n",
    "\n",
    "plt.figure(figsize = (24, 5))\n",
    "plot_window(entrants, ['C3', 'C4', 'C3 + C4'], start, SCOPE)\n",
    "\n",
    "# Lissage des hautes fréquences\n",
    "for _ in range(inc) :\n",
    "    smooth = simple_exponential_smoothing(smooth, alpha, 0)\n",
    "\n",
    "smooth = pd.Series(index = range(start, entrant + inc - dec), data = smooth[dec :])\n",
    "\n",
    "# plt.plot(raw - smooth, label = hands[event_start[pos][1]])\n",
    "plt.plot(smooth, '--', label = hands_event[event_start[boolInt][1]])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Apendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"text-align:center\" ><b>EEG</b> - Prédiction des Mouvements Imaginaires de la Main</h2>\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Le projet**\n",
    "- Intoduction  \n",
    "https://github.com/DataScientest-Studio/mar24_cds_eeg/blob/eric/references/Description_projet_EEG.pdf  \n",
    "https://www.bbci.de/competition/iv/desc_2b.pdf\n",
    "- Ressources / Données   \n",
    "https://www.kaggle.com/competitions/ucsd-neural-data-challenge/overview  \n",
    "- Bibliographie  \n",
    "https://www.bbci.de/competition/iv/desc_2b.pdf\n",
    "#### **2. Liens utils**\n",
    "- SciPy - *open-source software for mathematics, science, and engineering*  \n",
    "https://docs.scipy.org/doc/scipy/index.html  \n",
    "https://docs.scipy.org/doc/scipy/reference/signal.html  \n",
    "- MNE - *MEG + EEG Analysis & Visualisation*\n",
    "   - Accueil  \n",
    "   https://mne.tools/stable/index.html\n",
    "\n",
    "   - MNE - Data structures from arbitrary data  \n",
    "   https://mne.tools/stable/auto_tutorials/io/10_reading_meg_data.html#creating-mne-data-structures-from-arbitrary-data-from-memory\n",
    "   \n",
    "   - MNE - EEG Preprocessing  \n",
    "   https://mne.tools/dev/auto_tutorials/preprocessing/index.html  \n",
    "\n",
    "- pyRiemann - *Biosignals classification with Riemannian geometry*  \n",
    "https://pyriemann.readthedocs.io/en/latest/  \n",
    "- neurodsp - *Neuro Digital Signal Processing Toolbox*  \n",
    "https://neurodsp-tools.github.io/neurodsp/index.html#\n",
    "- Rythme Mu  \n",
    "https://fr.wikipedia.org/wiki/Rythme_Mu\n",
    "- Spectrogram from EEG  \n",
    "https://www.kaggle.com/code/cdeotte/how-to-make-spectrogram-from-eeg\n",
    "- Divers  \n",
    "https://signalprocessingsociety.org/  \n",
    "https://fr.wikipedia.org/wiki/Filtre_de_Butterworth  \n",
    "https://fr.wikipedia.org/wiki/Moyenne_mobile  \n",
    "https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html  \n",
    "https://perso.etis-lab.fr/ghaffari/2014_CCMB_Floride_USA.pdf  \n",
    "https://www.youtube.com/watch?v=wB417SAbdak&list=PLXc9qfVbMMN2TAoLHVW5NvNmJtwiHurzw  \n",
    "https://fastercapital.com/fr/sujet/identification-des-artefacts-de-traitement-du-signal-dans-des-sc%C3%A9narios-r%C3%A9els.html#:~:text=L'inspection%20visuelle%20est%20la,des%20pertes%20et%20du%20bruit.  \n",
    "   - Z-Score Normalisation  \n",
    "   https://fr.wikipedia.org/wiki/Cote_Z_(statistiques)  \n",
    "   https://typeset.io/questions/why-is-z-score-normalisation-necessary-in-pre-processing-eeg-1xv5jepyq5  \n",
    "\n",
    "   - Traitement numérique du signal  \n",
    "   https://fr.wikipedia.org/wiki/Traitement_num%C3%A9rique_du_signal  \n",
    "   - Ondelette  \n",
    "      - Wiki  \n",
    "      https://fr.wikipedia.org/wiki/Ondelette  \n",
    "\n",
    "      - L’analyse par ondelettes dans la vie de tous les jours  \n",
    "      https://interstices.info/lanalyse-par-ondelettes-dans-la-vie-de-tous-les-jours/  \n",
    "\n",
    "      - A guide for using the Wavelet Transform in Machine Learning  \n",
    "      https://ataspinar.com/2018/12/21/a-guide-for-using-the-wavelet-transform-in-machine-learning/\n",
    "      \n",
    "      - pyWavelets - *open source wavelet transform*  \n",
    "      https://pywavelets.readthedocs.io/en/latest/\n",
    "\n",
    "      - Ondelettes et applications  \n",
    "      https://www.i2m.univ-amu.fr/~caroline.chaux/GEOMDATA/TI-te5215.pdf\n",
    "\n",
    "   - Maximum de vraisemblance  \n",
    "   https://pmarchand1.github.io/ECL8202/notes_cours/03-Maximum_vraisemblance.html  \n",
    "   https://fr.wikipedia.org/wiki/Maximum_de_vraisemblance#:~:text=En%20statistique%2C%20l'estimateur%20du,maximisant%20la%20fonction%20de%20vraisemblance  \n",
    "\n",
    "   - Transformation de Fourier discrète  \n",
    "   https://fr.wikipedia.org/wiki/Transformation_de_Fourier_discr%C3%A8te  \n",
    "      - La Transformation de Fourier n’est pas adaptée à l’analyse des signaux non stationnaires.\n",
    "   - Neural Data Science in Python  \n",
    "   https://neuraldatascience.io/intro.html\n",
    "\n",
    "   - Preprocessing of EEG  \n",
    "   https://www.frontiersin.org/articles/10.3389/fninf.2015.00016/full#:~:text=The%20depositable%20preprocessing%20pipeline%20consists,with%20a%20low%20recording%20SNR  \n",
    "   https://typeset.io/papers/preprocessing-of-eeg-4go8vhcbty  \n",
    "   https://learn.neurotechedu.com/preprocessing  \n",
    "   https://g0rella.github.io/gorella_mwn/preprocessing_eeg.html  \n",
    "   \n",
    "   - Biblio :  \n",
    "   https://perso.telecom-paristech.fr/bloch/P6Image/ondelettestrsp.pdf  \n",
    "   https://www.math.u-bordeaux.fr/~jbigot/Site/Enseignement_files/ondelettesIMAT.pdf  \n",
    "   http://w3.cran.univ-lorraine.fr/perso/radu.ranta/pdf/cours_deb_ond%28fr%29.pdf\n",
    "   \n",
    "   - Digital Filtering  \n",
    "   http://notebooks.pluxbiosignals.com/notebooks/Categories/Pre-Process/digital_filtering_eeg_rev.html\n",
    "\n",
    "   - Processus stationnaire  \n",
    "   https://fr.wikipedia.org/wiki/Processus_stationnaire\n",
    "\n",
    "   - Analyse en composantes principales  \n",
    "   https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales#:~:text=L'ACP%2C%20d%C3%A9sign%C3%A9e%20en%20g%C3%A9n%C3%A9ral,une%20grandeur%20physique%2C%20comme%20les"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
