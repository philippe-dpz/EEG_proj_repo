{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"text-align:center\" ><b>EEG</b> - Prédiction des Mouvements Imaginaires de la Main</h2>\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Le projet**\n",
    "- Intoduction  \n",
    "https://github.com/DataScientest-Studio/mar24_cds_eeg/blob/eric/references/Description_projet_EEG.pdf  \n",
    "https://www.bbci.de/competition/iv/desc_2b.pdf\n",
    "- Ressources / Données   \n",
    "https://www.kaggle.com/competitions/ucsd-neural-data-challenge/overview  \n",
    "- Bibliographie  \n",
    "https://www.bbci.de/competition/iv/desc_2b.pdf\n",
    "#### **2. Liens utils**\n",
    "- SciPy - *open-source software for mathematics, science, and engineering*  \n",
    "https://docs.scipy.org/doc/scipy/index.html  \n",
    "https://docs.scipy.org/doc/scipy/reference/signal.html  \n",
    "- MNE - *MEG + EEG Analysis & Visualisation*\n",
    "   - Accueil  \n",
    "   https://mne.tools/stable/index.html\n",
    "\n",
    "   - MNE - Data structures from arbitrary data  \n",
    "   https://mne.tools/stable/auto_tutorials/io/10_reading_meg_data.html#creating-mne-data-structures-from-arbitrary-data-from-memory\n",
    "   \n",
    "   - MNE - EEG Preprocessing  \n",
    "   https://mne.tools/dev/auto_tutorials/preprocessing/index.html  \n",
    "\n",
    "- pyRiemann - *Biosignals classification with Riemannian geometry*  \n",
    "https://pyriemann.readthedocs.io/en/latest/  \n",
    "- neurodsp - *Neuro Digital Signal Processing Toolbox*  \n",
    "https://neurodsp-tools.github.io/neurodsp/index.html#\n",
    "- Rythme Mu  \n",
    "https://fr.wikipedia.org/wiki/Rythme_Mu\n",
    "- Spectrogram from EEG  \n",
    "https://www.kaggle.com/code/cdeotte/how-to-make-spectrogram-from-eeg\n",
    "- Divers  \n",
    "https://signalprocessingsociety.org/  \n",
    "https://fr.wikipedia.org/wiki/Filtre_de_Butterworth  \n",
    "https://fr.wikipedia.org/wiki/Moyenne_mobile  \n",
    "https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html  \n",
    "https://perso.etis-lab.fr/ghaffari/2014_CCMB_Floride_USA.pdf  \n",
    "https://www.youtube.com/watch?v=wB417SAbdak&list=PLXc9qfVbMMN2TAoLHVW5NvNmJtwiHurzw  \n",
    "https://fastercapital.com/fr/sujet/identification-des-artefacts-de-traitement-du-signal-dans-des-sc%C3%A9narios-r%C3%A9els.html#:~:text=L'inspection%20visuelle%20est%20la,des%20pertes%20et%20du%20bruit.  \n",
    "   - Z-Score Normalisation  \n",
    "   https://fr.wikipedia.org/wiki/Cote_Z_(statistiques)  \n",
    "   https://typeset.io/questions/why-is-z-score-normalisation-necessary-in-pre-processing-eeg-1xv5jepyq5  \n",
    "\n",
    "   - Traitement numérique du signal  \n",
    "   https://fr.wikipedia.org/wiki/Traitement_num%C3%A9rique_du_signal  \n",
    "   - Ondelette  \n",
    "      - Wiki  \n",
    "      https://fr.wikipedia.org/wiki/Ondelette  \n",
    "\n",
    "      - L’analyse par ondelettes dans la vie de tous les jours  \n",
    "      https://interstices.info/lanalyse-par-ondelettes-dans-la-vie-de-tous-les-jours/  \n",
    "\n",
    "      - A guide for using the Wavelet Transform in Machine Learning  \n",
    "      https://ataspinar.com/2018/12/21/a-guide-for-using-the-wavelet-transform-in-machine-learning/\n",
    "      \n",
    "      - pyWavelets - *open source wavelet transform*  \n",
    "      https://pywavelets.readthedocs.io/en/latest/\n",
    "\n",
    "      - Ondelettes et applications  \n",
    "      https://www.i2m.univ-amu.fr/~caroline.chaux/GEOMDATA/TI-te5215.pdf\n",
    "\n",
    "   - Maximum de vraisemblance  \n",
    "   https://pmarchand1.github.io/ECL8202/notes_cours/03-Maximum_vraisemblance.html  \n",
    "   https://fr.wikipedia.org/wiki/Maximum_de_vraisemblance#:~:text=En%20statistique%2C%20l'estimateur%20du,maximisant%20la%20fonction%20de%20vraisemblance  \n",
    "\n",
    "   - Transformation de Fourier discrète  \n",
    "   https://fr.wikipedia.org/wiki/Transformation_de_Fourier_discr%C3%A8te  \n",
    "      - La Transformation de Fourier n’est pas adaptée à l’analyse des signaux non stationnaires.\n",
    "   - Neural Data Science in Python  \n",
    "   https://neuraldatascience.io/intro.html\n",
    "\n",
    "   - Preprocessing of EEG  \n",
    "   https://www.frontiersin.org/articles/10.3389/fninf.2015.00016/full#:~:text=The%20depositable%20preprocessing%20pipeline%20consists,with%20a%20low%20recording%20SNR  \n",
    "   https://typeset.io/papers/preprocessing-of-eeg-4go8vhcbty  \n",
    "   https://learn.neurotechedu.com/preprocessing  \n",
    "   https://g0rella.github.io/gorella_mwn/preprocessing_eeg.html  \n",
    "   \n",
    "   - Biblio :  \n",
    "   https://perso.telecom-paristech.fr/bloch/P6Image/ondelettestrsp.pdf  \n",
    "   https://www.math.u-bordeaux.fr/~jbigot/Site/Enseignement_files/ondelettesIMAT.pdf  \n",
    "   http://w3.cran.univ-lorraine.fr/perso/radu.ranta/pdf/cours_deb_ond%28fr%29.pdf\n",
    "   \n",
    "   - Digital Filtering  \n",
    "   http://notebooks.pluxbiosignals.com/notebooks/Categories/Pre-Process/digital_filtering_eeg_rev.html\n",
    "\n",
    "   - Processus stationnaire  \n",
    "   https://fr.wikipedia.org/wiki/Processus_stationnaire\n",
    "\n",
    "   - Analyse en composantes principales  \n",
    "   https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales#:~:text=L'ACP%2C%20d%C3%A9sign%C3%A9e%20en%20g%C3%A9n%C3%A9ral,une%20grandeur%20physique%2C%20comme%20les"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Chargement des différentes librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Chargement des différentes librairies\n",
    "\n",
    "import sys, os, gc, time, math\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "from src.thot.sesh import *\n",
    "\n",
    "from sklearn import model_selection, preprocessing as sk_p\n",
    "from sklearn import ensemble, svm, neighbors\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from keras.models import Sequential                             # type: ignore\n",
    "from keras.callbacks import EarlyStopping                       # type: ignore\n",
    "from keras.layers import GlobalAveragePooling1D                 # type: ignore\n",
    "from keras.layers import Dense, Dropout, Conv1D, LSTM           # type: ignore\n",
    "from keras.layers import LeakyReLU, ReLU, PReLU, ConvLSTM1D     # type: ignore\n",
    "# from keras.layers import Bidirectional, TimeDistributed, RepeatVector, Flatten\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.optimizers import AdamW, Adam            # type: ignore\n",
    "\n",
    "# from pyriemann.spatialfilters import CSP\n",
    "\n",
    "import pywt, librosa\n",
    "import seaborn as sns\n",
    "\n",
    "# from scipy.fft import fft\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Déclaration de constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-62, 500)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### • Déclaration de constantes\n",
    "\n",
    "# Fréquence d'échantillonnage - Hz (Nombre de valeur / sec)\n",
    "SAMPLE_RATE  = 250\n",
    "# Temps additionel pour étendre le domaines d'étude.\n",
    "LAG : int    = -62     # Décalage du signal dû signal ~250ms\n",
    "#\n",
    "PW2 : int    = int(np.floor(np.log2(SAMPLE_RATE))) # 2 << SAMPLE_RATE // 32\n",
    "#\n",
    "NFFT : int   = 1 << PW2\n",
    "# Epoque en sec donnée en nombre d'échantillon consectutif # 4\" de données (multiple de 2)\n",
    "SCOPE : int  = SAMPLE_RATE << 1 # (1 << PW2) * 4\n",
    "# Deux enregistrements bipolaires + neutre\n",
    "eeg_Chans    = ['C3', 'C4', 'Cz']\n",
    "# Liste des cannaux eeg associés aux évènement 0 et 1\n",
    "eeg_left     = [f'{c}_0' for c in eeg_Chans]\n",
    "eeg_right    = [f'{c}_1' for c in eeg_Chans]\n",
    "full_eeg     = eeg_left + eeg_right\n",
    "# Trois enregistrements musculaires\n",
    "ecg_Chans    = ['EOG:ch01', 'EOG:ch02', 'EOG:ch03']\n",
    "# Liste de tous les cannaux des dataframes\n",
    "all_chans    = eeg_Chans + ecg_Chans\n",
    "# Correspondance pour la classification\n",
    "hands_event  = {0: 'Left', 1: 'Right'}\n",
    "# Les bandes de fréquences d'intérêt\n",
    "eeg_bands    = {'Delta' : (0.1, 4.0),\n",
    "                'Theta' : (4.1, 8.0),\n",
    "                'Alpha' : (8.1, 14.0),\n",
    "                'Beta'  : (14.1, 30.0),\n",
    "                'Gamma' : (30.1, (SAMPLE_RATE >> 1) - 1),}\n",
    "# Coefficients pour filtres Butterworth numérique d'ordre N pour le filtrage passe-bande\n",
    "bands_coeff  = {band : butter_bandpass(low, high, SAMPLE_RATE) for band, (low, high) in eeg_bands.items()}\n",
    "# Nombre dévènement à prédir\n",
    "num_events   = range(len(hands_event))\n",
    "\n",
    "LAG, SCOPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Acquisition des données d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Acquisition des données d'entrainement\n",
    "\n",
    "target  = \"../data/data.zip\"\n",
    "count   = len('train/')\n",
    "fics    = [x[count :] for x in files_in_zip(target, directory = 'train')]\n",
    "\n",
    "[fics.remove(x) for x in fics[:: -3]]\n",
    "\n",
    "# Acquisition des fichiers du répertoir dans le fichier zip\n",
    "train_csv = csv_in_zip(target, directory = 'train', files = fics)\n",
    "label_csv = csv_in_zip(target, directory = 'y_train_only', files = fics)\n",
    "\n",
    "notes   = filename(fics)\n",
    "headers = [f\"{t} . {i + 1}\" for i, t in enumerate(notes)]\n",
    "count   = range(len(train_csv))\n",
    "\n",
    "# fics    = [f'B0{i}0{j}T.csv' for i in range(1, 9) for j in range(1, 4)]\n",
    "# df_train_pkl = pkl_in_zip(path, fichier_specifique = 'epoched_train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Acquisition des données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Acquisition des données de test\n",
    "\n",
    "test_csv = csv_in_zip(target, directory = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Catch22 émulateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SB_BinaryStats_mean_longstretch1(y : Vector) -> int :\n",
    "    nb    = len(y)\n",
    "    yMean = np.mean(y)\n",
    "    yBin  = [int(v - yMean <= 0) for v in y]\n",
    "    \n",
    "    max_stretch : int = 0\n",
    "    last1 : int       = 0\n",
    "\n",
    "    for i in range(nb) :\n",
    "        if (yBin[i] == 0 | i == (nb - 2)) :\n",
    "            stretch = i - last1\n",
    "\n",
    "            if(stretch > max_stretch) : max_stretch = stretch\n",
    "            \n",
    "            last1 = i\n",
    "    \n",
    "    return max_stretch\n",
    "\n",
    "def DN_OutlierInclude_np_001_mdrmd(y : Vector, sign : int) -> float :\n",
    "    nb : int    = len(y)\n",
    "    tot : int   = 0\n",
    "    inc : float = 1e-2 # 0.01\n",
    "    yWork = []\n",
    "\n",
    "    constantFlag : int = 1\n",
    "\n",
    "    # apply sign and check constant time series\n",
    "    for x in y :\n",
    "        if (x != y[0]) : constantFlag = 0\n",
    "\n",
    "        # apply sign, save in new variable\n",
    "        yWork.append(sign * x)\n",
    "        \n",
    "        # count pos / negs\n",
    "        if (yWork[-1] >= 0) : tot += 1\n",
    "\n",
    "    if (constantFlag == 1) : return 0\n",
    "    \n",
    "    # find maximum (or minimum, depending on sign)\n",
    "    maxVal = max(yWork)\n",
    "    \n",
    "    #  maximum value too small ? return 0\n",
    "    if (maxVal < inc) : return 0\n",
    "    \n",
    "    # save the indices where y > threshold\n",
    "    r = []\n",
    "    #  save the median over indices with absolute value > threshold\n",
    "    msDti1 = []\n",
    "    msDti3 = []\n",
    "    msDti4 = []\n",
    "    \n",
    "    k = 100 / tot\n",
    "    s = 2 / nb\n",
    "    \n",
    "    nThresh = int(maxVal / inc) + 1\n",
    "    \n",
    "    for j in range(nThresh) :        \n",
    "        for i in range(nb) :\n",
    "            if (yWork[i] >= j * inc) : r.append(i + 1)\n",
    "\n",
    "        #  intervals between high-values\n",
    "        tmp = np.array(r[1:]) - r[:-1]\n",
    "\n",
    "        highSize = len(r) - 1\n",
    "         \n",
    "        msDti1.append(np.mean(tmp[: highSize]))\n",
    "        msDti3.append(k * highSize)\n",
    "        msDti4.append(np.median(r) * s - 1)\n",
    "    \n",
    "    trimthr : int = 2\n",
    "    mj : int      = 0\n",
    "    fbi : int     = nThresh - 1\n",
    "\n",
    "    for i in range(nThresh) :\n",
    "        if (msDti3[i] > trimthr) : mj = i\n",
    "\n",
    "        k = nThresh - 1 - i\n",
    "\n",
    "        if (math.isnan(msDti1[k])) : fbi = k\n",
    "    \n",
    "    return np.median(msDti4[: (mj if mj < fbi else fbi) + 1])\n",
    "\n",
    "def nextpow2(n : int) -> int :\n",
    "    n -= 1\n",
    "\n",
    "    n |= n >> 1\n",
    "    n |= n >> 2\n",
    "    n |= n >> 4\n",
    "    n |= n >> 8\n",
    "    n |= n >> 16\n",
    "\n",
    "    return n + 1\n",
    "\n",
    "def co_autocorrs(y : Vector) -> Vector :\n",
    "    nb : int = len(y)\n",
    "    m = np.mean(y)\n",
    "    nFFT : int = nextpow2(nb) << 1\n",
    "    \n",
    "    F  = [{x - m, 0.0 } for x in y]\n",
    "\n",
    "    for _ in range(nb) : F.append({ 0.0, 0.0 })\n",
    "    \n",
    "    tw = twiddles(tw, nFFT)\n",
    "\n",
    "    fft(F, nFFT, tw)\n",
    "    dot_multiply(F, F, nFFT)\n",
    "    fft(F, nFFT, tw)\n",
    "\n",
    "    divisor = F[0]\n",
    "\n",
    "    F = [_Cdivcc(x, divisor) for x in F]\n",
    "\n",
    "    return [creal(x) for x in F]\n",
    "\n",
    "def CO_f1ecac(y : Vector) -> float :\n",
    "    # Compute autocorrelations\n",
    "    corr = co_autocorrs(y)\n",
    "\n",
    "    # Threshold to cross\n",
    "    thresh = 1.0 / math.exp(1)\n",
    "\n",
    "    for i in range(len(y)) :\n",
    "        x = corr[i + 1]\n",
    "\n",
    "        if (x < thresh) :\n",
    "            dx  = (thresh - corr[i]) / (x - corr[i])\n",
    "            out = i + dx\n",
    "            \n",
    "            return out\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Pré-traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Pré-traitement des données\n",
    "\n",
    "def data_spliting(datas : list[Board], Channels : Clause, number_event : int | Index,\n",
    "                  labels : list[Board] | None = None, merge : bool = False, level : bool = True) :\n",
    "    count = range(len(datas))\n",
    "    parts = []        #\n",
    "    temp  = [[], []]  # Les époques pour tous les cannaux et tous les évènements.\n",
    "    spots = [[], []]  # Apparitions des évènements\n",
    "    # Pour la standardisation du nombre d'échantillon max conservé\n",
    "    ceil  = min([len(labels[i]['EventType']) for i in count])\n",
    "\n",
    "    if type(number_event) == int : number_event = range(number_event)\n",
    "\n",
    "    # Extraction des données relavitives à l'apparition des évènements.\n",
    "    if level :\n",
    "        for i in count :\n",
    "            df   = datas[i]\n",
    "            kind = labels[i]['EventType'][: ceil]\n",
    "            loc  = np.where(df['EventStart'] == 1)[: ceil]\n",
    "\n",
    "            parts.append(zero_removal(df['C3'], 75))\n",
    "\n",
    "            for j in number_event :\n",
    "                spots[j].append(np.array(*loc)[*np.where(kind == j)])\n",
    "\n",
    "                # [tf.convert_to_tensor(X) for X in event_epochs(spots[j][-1], SCOPE, LAG)]\n",
    "                room = event_epochs(spots[j][-1], SCOPE, LAG)\n",
    "\n",
    "                temp[j].append([full_event(df[c], room, merge) for c in Channels])\n",
    "    else :\n",
    "        for i in count :\n",
    "            df   = datas[i]\n",
    "            kind = label_csv[i]['EventType']\n",
    "            loc  = np.where(df['EventStart'] == 1)\n",
    "\n",
    "            parts.append(zero_removal(df['C3'], 75))\n",
    "\n",
    "            for j in number_event :\n",
    "                spots[j].append(np.array(*loc)[*np.where(kind == j)])\n",
    "\n",
    "                room = event_epochs(spots[j][-1], SCOPE, LAG)\n",
    "\n",
    "                temp[j].append([full_event(df[c], room, merge) for c in Channels])\n",
    "\n",
    "    # Regroupement des données en fonction du type de l'évènement et du cannal d'observation\n",
    "    if merge :\n",
    "        temp = [[[np.append([], T[j :: 3]) for T in temp[i]] for j in range(3)] for i in number_event]\n",
    "    else :\n",
    "        store = [[[], [], []], [[], [], []]]\n",
    "        \n",
    "        [[[[store[i][j].append(G) for G in X] for j, X in enumerate(T)] for T in temp[i]] for i in number_event]\n",
    "\n",
    "        temp = store\n",
    "\n",
    "    eras = [pd.DataFrame({**dict(zip(Channels, [pd.Series(X) for X in temp[i]])), 'EventType': i}) for i in number_event]\n",
    "\n",
    "    return eras, spots, parts\n",
    "\n",
    "# %time\n",
    "train_eras, train_spots, train_parts = data_spliting(train_csv, eeg_Chans, num_events, label_csv)\n",
    "\n",
    "# train_samples = samples(len(train_eras[0]))\n",
    "trains        = pd.concat(train_eras, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DN_OutlierInclude_np_001_mdrmd(trains['C3'][0], 1)\n",
    "SB_BinaryStats_mean_longstretch1(trains['C3'][1])\n",
    "# np.array([1, 2, 3, 4]) + [5, 6, 7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C3_min</th>\n",
       "      <th>C3_max</th>\n",
       "      <th>C3_ptp</th>\n",
       "      <th>C3_std</th>\n",
       "      <th>C3_var</th>\n",
       "      <th>C3_mean</th>\n",
       "      <th>C3_median</th>\n",
       "      <th>C3_average</th>\n",
       "      <th>C4_min</th>\n",
       "      <th>C4_max</th>\n",
       "      <th>...</th>\n",
       "      <th>Cz_average</th>\n",
       "      <th>min_diff</th>\n",
       "      <th>max_diff</th>\n",
       "      <th>ptp_diff</th>\n",
       "      <th>std_diff</th>\n",
       "      <th>var_diff</th>\n",
       "      <th>mean_diff</th>\n",
       "      <th>median_diff</th>\n",
       "      <th>average_diff</th>\n",
       "      <th>EventType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.945907</td>\n",
       "      <td>7.385367</td>\n",
       "      <td>14.331273</td>\n",
       "      <td>2.601630</td>\n",
       "      <td>6.768480</td>\n",
       "      <td>0.146694</td>\n",
       "      <td>0.204471</td>\n",
       "      <td>0.146694</td>\n",
       "      <td>-6.704814</td>\n",
       "      <td>10.965133</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165359</td>\n",
       "      <td>0.241093</td>\n",
       "      <td>3.579767</td>\n",
       "      <td>3.338674</td>\n",
       "      <td>0.150621</td>\n",
       "      <td>0.806409</td>\n",
       "      <td>-0.371752</td>\n",
       "      <td>-0.645457</td>\n",
       "      <td>-0.371752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.897078</td>\n",
       "      <td>7.125963</td>\n",
       "      <td>14.023041</td>\n",
       "      <td>2.893735</td>\n",
       "      <td>8.373701</td>\n",
       "      <td>0.302238</td>\n",
       "      <td>0.399786</td>\n",
       "      <td>0.302238</td>\n",
       "      <td>-6.149386</td>\n",
       "      <td>8.517586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922939</td>\n",
       "      <td>0.747692</td>\n",
       "      <td>1.391623</td>\n",
       "      <td>0.643931</td>\n",
       "      <td>-0.327252</td>\n",
       "      <td>-1.786865</td>\n",
       "      <td>-0.157034</td>\n",
       "      <td>-0.473030</td>\n",
       "      <td>-0.157034</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-6.289769</td>\n",
       "      <td>10.486000</td>\n",
       "      <td>16.775769</td>\n",
       "      <td>2.652434</td>\n",
       "      <td>7.035407</td>\n",
       "      <td>0.746673</td>\n",
       "      <td>0.735485</td>\n",
       "      <td>0.746673</td>\n",
       "      <td>-6.018158</td>\n",
       "      <td>7.171740</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.322930</td>\n",
       "      <td>0.271611</td>\n",
       "      <td>-3.314260</td>\n",
       "      <td>-3.585870</td>\n",
       "      <td>-0.006133</td>\n",
       "      <td>-0.032499</td>\n",
       "      <td>-0.775543</td>\n",
       "      <td>-0.929274</td>\n",
       "      <td>-0.775543</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-10.229648</td>\n",
       "      <td>9.311055</td>\n",
       "      <td>19.540703</td>\n",
       "      <td>3.650374</td>\n",
       "      <td>13.325232</td>\n",
       "      <td>0.265415</td>\n",
       "      <td>0.704967</td>\n",
       "      <td>0.265415</td>\n",
       "      <td>-7.931640</td>\n",
       "      <td>28.699168</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.335100</td>\n",
       "      <td>2.298009</td>\n",
       "      <td>19.388113</td>\n",
       "      <td>17.090105</td>\n",
       "      <td>4.252308</td>\n",
       "      <td>49.127159</td>\n",
       "      <td>1.630011</td>\n",
       "      <td>-1.235981</td>\n",
       "      <td>1.630011</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9.442283</td>\n",
       "      <td>9.158465</td>\n",
       "      <td>18.600748</td>\n",
       "      <td>3.021150</td>\n",
       "      <td>9.127346</td>\n",
       "      <td>-0.397009</td>\n",
       "      <td>-0.610361</td>\n",
       "      <td>-0.397009</td>\n",
       "      <td>-7.757687</td>\n",
       "      <td>9.396506</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.635642</td>\n",
       "      <td>1.684596</td>\n",
       "      <td>0.238041</td>\n",
       "      <td>-1.446555</td>\n",
       "      <td>-0.203625</td>\n",
       "      <td>-1.188900</td>\n",
       "      <td>0.539522</td>\n",
       "      <td>0.625620</td>\n",
       "      <td>0.539522</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>-6.314183</td>\n",
       "      <td>8.001831</td>\n",
       "      <td>14.316014</td>\n",
       "      <td>2.586743</td>\n",
       "      <td>6.691239</td>\n",
       "      <td>-0.442090</td>\n",
       "      <td>-0.549325</td>\n",
       "      <td>-0.442090</td>\n",
       "      <td>-6.576638</td>\n",
       "      <td>8.105592</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167276</td>\n",
       "      <td>-0.262455</td>\n",
       "      <td>0.103761</td>\n",
       "      <td>0.366217</td>\n",
       "      <td>0.144603</td>\n",
       "      <td>0.769014</td>\n",
       "      <td>0.773993</td>\n",
       "      <td>0.584421</td>\n",
       "      <td>0.773993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>-7.718013</td>\n",
       "      <td>7.574578</td>\n",
       "      <td>15.292592</td>\n",
       "      <td>2.828992</td>\n",
       "      <td>8.003196</td>\n",
       "      <td>0.052137</td>\n",
       "      <td>-0.094606</td>\n",
       "      <td>0.052137</td>\n",
       "      <td>-9.341573</td>\n",
       "      <td>8.343633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.369085</td>\n",
       "      <td>-1.623560</td>\n",
       "      <td>0.769055</td>\n",
       "      <td>2.392615</td>\n",
       "      <td>0.072008</td>\n",
       "      <td>0.412606</td>\n",
       "      <td>-0.400311</td>\n",
       "      <td>-0.071717</td>\n",
       "      <td>-0.400311</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>-8.941787</td>\n",
       "      <td>9.250019</td>\n",
       "      <td>18.191806</td>\n",
       "      <td>3.069040</td>\n",
       "      <td>9.419007</td>\n",
       "      <td>-0.741326</td>\n",
       "      <td>-0.822461</td>\n",
       "      <td>-0.741326</td>\n",
       "      <td>-6.411841</td>\n",
       "      <td>5.627527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278459</td>\n",
       "      <td>2.529946</td>\n",
       "      <td>-3.622492</td>\n",
       "      <td>-6.152438</td>\n",
       "      <td>-0.642260</td>\n",
       "      <td>-3.529744</td>\n",
       "      <td>1.049613</td>\n",
       "      <td>1.170367</td>\n",
       "      <td>1.049613</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>-10.495155</td>\n",
       "      <td>6.909285</td>\n",
       "      <td>17.404440</td>\n",
       "      <td>2.887034</td>\n",
       "      <td>8.334968</td>\n",
       "      <td>-3.062040</td>\n",
       "      <td>-3.227283</td>\n",
       "      <td>-3.062040</td>\n",
       "      <td>-10.437171</td>\n",
       "      <td>4.263371</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.401685</td>\n",
       "      <td>0.057984</td>\n",
       "      <td>-2.645914</td>\n",
       "      <td>-2.703899</td>\n",
       "      <td>-0.172511</td>\n",
       "      <td>-0.966329</td>\n",
       "      <td>1.102281</td>\n",
       "      <td>1.538109</td>\n",
       "      <td>1.102281</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>-7.617304</td>\n",
       "      <td>11.026169</td>\n",
       "      <td>18.643473</td>\n",
       "      <td>3.574498</td>\n",
       "      <td>12.777037</td>\n",
       "      <td>2.176931</td>\n",
       "      <td>2.610819</td>\n",
       "      <td>2.176931</td>\n",
       "      <td>-10.840009</td>\n",
       "      <td>5.923552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131618</td>\n",
       "      <td>-3.222705</td>\n",
       "      <td>-5.102617</td>\n",
       "      <td>-1.879911</td>\n",
       "      <td>-0.111093</td>\n",
       "      <td>-0.781860</td>\n",
       "      <td>-3.766940</td>\n",
       "      <td>-4.007019</td>\n",
       "      <td>-3.766940</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2160 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         C3_min     C3_max     C3_ptp    C3_std     C3_var   C3_mean  \\\n",
       "0     -6.945907   7.385367  14.331273  2.601630   6.768480  0.146694   \n",
       "1     -6.897078   7.125963  14.023041  2.893735   8.373701  0.302238   \n",
       "2     -6.289769  10.486000  16.775769  2.652434   7.035407  0.746673   \n",
       "3    -10.229648   9.311055  19.540703  3.650374  13.325232  0.265415   \n",
       "4     -9.442283   9.158465  18.600748  3.021150   9.127346 -0.397009   \n",
       "...         ...        ...        ...       ...        ...       ...   \n",
       "2155  -6.314183   8.001831  14.316014  2.586743   6.691239 -0.442090   \n",
       "2156  -7.718013   7.574578  15.292592  2.828992   8.003196  0.052137   \n",
       "2157  -8.941787   9.250019  18.191806  3.069040   9.419007 -0.741326   \n",
       "2158 -10.495155   6.909285  17.404440  2.887034   8.334968 -3.062040   \n",
       "2159  -7.617304  11.026169  18.643473  3.574498  12.777037  2.176931   \n",
       "\n",
       "      C3_median  C3_average     C4_min     C4_max  ...  Cz_average  min_diff  \\\n",
       "0      0.204471    0.146694  -6.704814  10.965133  ...   -0.165359  0.241093   \n",
       "1      0.399786    0.302238  -6.149386   8.517586  ...    0.922939  0.747692   \n",
       "2      0.735485    0.746673  -6.018158   7.171740  ...   -0.322930  0.271611   \n",
       "3      0.704967    0.265415  -7.931640  28.699168  ...   -0.335100  2.298009   \n",
       "4     -0.610361   -0.397009  -7.757687   9.396506  ...   -0.635642  1.684596   \n",
       "...         ...         ...        ...        ...  ...         ...       ...   \n",
       "2155  -0.549325   -0.442090  -6.576638   8.105592  ...   -0.167276 -0.262455   \n",
       "2156  -0.094606    0.052137  -9.341573   8.343633  ...    0.369085 -1.623560   \n",
       "2157  -0.822461   -0.741326  -6.411841   5.627527  ...    0.278459  2.529946   \n",
       "2158  -3.227283   -3.062040 -10.437171   4.263371  ...   -0.401685  0.057984   \n",
       "2159   2.610819    2.176931 -10.840009   5.923552  ...   -0.131618 -3.222705   \n",
       "\n",
       "       max_diff   ptp_diff  std_diff   var_diff  mean_diff  median_diff  \\\n",
       "0      3.579767   3.338674  0.150621   0.806409  -0.371752    -0.645457   \n",
       "1      1.391623   0.643931 -0.327252  -1.786865  -0.157034    -0.473030   \n",
       "2     -3.314260  -3.585870 -0.006133  -0.032499  -0.775543    -0.929274   \n",
       "3     19.388113  17.090105  4.252308  49.127159   1.630011    -1.235981   \n",
       "4      0.238041  -1.446555 -0.203625  -1.188900   0.539522     0.625620   \n",
       "...         ...        ...       ...        ...        ...          ...   \n",
       "2155   0.103761   0.366217  0.144603   0.769014   0.773993     0.584421   \n",
       "2156   0.769055   2.392615  0.072008   0.412606  -0.400311    -0.071717   \n",
       "2157  -3.622492  -6.152438 -0.642260  -3.529744   1.049613     1.170367   \n",
       "2158  -2.645914  -2.703899 -0.172511  -0.966329   1.102281     1.538109   \n",
       "2159  -5.102617  -1.879911 -0.111093  -0.781860  -3.766940    -4.007019   \n",
       "\n",
       "      average_diff  EventType  \n",
       "0        -0.371752          0  \n",
       "1        -0.157034          0  \n",
       "2        -0.775543          0  \n",
       "3         1.630011          0  \n",
       "4         0.539522          0  \n",
       "...            ...        ...  \n",
       "2155      0.773993          1  \n",
       "2156     -0.400311          1  \n",
       "2157      1.049613          1  \n",
       "2158      1.102281          1  \n",
       "2159     -3.766940          1  \n",
       "\n",
       "[2160 rows x 33 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def catch(data : Board, col : str, channels : Clause, event : int,\n",
    "            norm : bool = False) -> Board :\n",
    "    func = [np.min, np.max, np.ptp, np.std, np.var, np.mean, np.median, np.average]\n",
    "    tmp  = data[data[col] == event]\n",
    "    name = [f.__name__ for f in func]\n",
    "    head = np.append([f\"{c}_{f}\" for c in channels for f in name],\n",
    "                     [f\"{f}_diff\" for f in name])\n",
    "\n",
    "    if norm :\n",
    "        tmp = [[normalized(x) for x in tmp[c]] for c in channels]\n",
    "\n",
    "        _c3, _c4, _cz = [[[f(v) for v in s] for f in func] for s in tmp]\n",
    "    else :\n",
    "        _c3, _c4, _cz = [[[f(v) for v in tmp[c]] for f in func] for c in channels]\n",
    "    \n",
    "    _ff = [np.array(v1) - v0 for v0, v1 in zip(_c3, _c4)]\n",
    "    _df = pd.DataFrame(np.array((*_c3, *_c4, *_cz, *_ff)).T, columns = head)\n",
    "    # _df = pd.DataFrame(np.array((*_cz, *_ff, )).T)\n",
    "    \n",
    "    _df[col] = event\n",
    "\n",
    "    return _df\n",
    "\n",
    "norm : bool = False\n",
    "\n",
    "df_left  = catch(trains, 'EventType', eeg_Chans, 0, norm = norm)\n",
    "df_right = catch(trains, 'EventType', eeg_Chans, 1, norm = norm)\n",
    "\n",
    "X = pd.concat((df_left, df_right), ignore_index = True)\n",
    "y = X['EventType']\n",
    "\n",
    "display(X)\n",
    "\n",
    "X.drop(columns = ['EventType'], inplace = True)\n",
    "\n",
    "# tmp = [[normalized(x) for x in trains[c]] for c in eeg_Chans]\n",
    "\n",
    "# display(np.shape(tmp))\n",
    "\n",
    "# tmp = pd.DataFrame((tmp), index = eeg_Chans).T\n",
    "\n",
    "# func    = [np.min, np.max, np.ptp, np.std, np.var, np.mean, np.median, np.average], columns = eeg_Chans\n",
    "\n",
    "# tmp     = trains[trains['EventType'] == 0]\n",
    "# left_c3 = [[f(v) for v in tmp['C3']] for f in func]\n",
    "# left_c4 = [[f(v) for v in tmp['C4']] for f in func]\n",
    "# left_cz = [[f(v) for v in tmp['Cz']] for f in func]\n",
    "# left_ff = [np.array(v1) - v0 for v0, v1 in zip(left_c3, left_c4)]\n",
    "# left_cr = np.correlate(tmp['C3'].values, tmp['C4'].values)\n",
    "\n",
    "# tmp      = trains[trains['EventType'] == 1]\n",
    "# right_c3 = [[f(v) for v in tmp['C3']] for f in func]\n",
    "# right_c4 = [[f(v) for v in tmp['C4']] for f in func]\n",
    "# right_cz = [[f(v) for v in tmp['Cz']] for f in func]\n",
    "# right_ff = [np.array(v1) - v0 for v0, v1 in zip(left_c3, left_c4)]\n",
    "# right_cr = np.correlate(tmp['C3'].values, tmp['C4'].values)\n",
    "\n",
    "# df_left  = pd.DataFrame(np.array((*left_c3, *left_c4, *left_ff, *left_cz)).T)\n",
    "# df_right = pd.DataFrame(np.array((*right_c3, *right_c4, *right_ff, *right_cz)).T)\n",
    "\n",
    "# df_left['EventType']  = 0\n",
    "# df_right['EventType'] = 1\n",
    "\n",
    "# len(left_cr[0]), np.shape(left_c3), np.shape(tmp['C3'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation des spectrogrammes / Test\n",
    "\n",
    "def logMelSpectrogram(data : Vector, rate : int, dt : float = 1e-2) -> Vector :\n",
    "    tps = 1 << int(np.floor(np.log2(rate * dt)))\n",
    "    # print(tps)\n",
    "    # Spectrogramme\n",
    "    stfts = np.abs(librosa.stft(y = data, n_fft = tps, hop_length = 1 << 2, center = True)).T\n",
    "    # Filtre de MEL\n",
    "    liny  = librosa.filters.mel(sr = rate, n_fft = tps + 1, n_mels = stfts.shape[-1]).T\n",
    "    # Application du filtre au spectrogramme\n",
    "    mel_  = np.tensordot(stfts, liny, 1)\n",
    "\n",
    "    return np.log(mel_ + 1e-6)\n",
    "    \n",
    "def structure(data : Board | Vector, rate : int, whr : Clause) -> Vector :\n",
    "    # return np.array([logMelSpectrogram(X, rate, 2) for X in data[whr]])\n",
    "    # return np.stack([[signal.welch(X, rate)[1] for X in data[c]] for c in whr], axis = 2)\n",
    "    return np.stack(trains['C4'] - trains['C3'], axis = 1)\n",
    "    # return np.stack([[X for X in data[c]] for c in whr], axis = 2)\n",
    "\n",
    "def img_spectrogram(raw : Vector, rate : int, nfft : int = 1 << 10) -> Vector :\n",
    "    return librosa.feature.melspectrogram(y = raw, sr = rate, hop_length = 1, \n",
    "                            n_fft = nfft, n_mels = 32, fmin = 0, fmax = 20, win_length = 32)\n",
    "\n",
    "def spectrogram_dep(data : Board, rate : int, channels : Clause, n_row : int = 5, n_col : int = 12) :\n",
    "    sample = np.random.default_rng().integers(data.shape[0], size = n_row)\n",
    "\n",
    "    sample.sort()\n",
    "\n",
    "    plt.figure(figsize = (18, 2 * .48 * n_row))\n",
    "\n",
    "    pos = 0\n",
    "\n",
    "    for k in sample :\n",
    "        for c in channels :\n",
    "            # x   = normalized(data[c][k])\n",
    "            raw = img_spectrogram(raw = data[c][k], rate = rate)\n",
    "            pos += 1\n",
    "            \n",
    "            plt.subplot(n_row, n_col, pos)\n",
    "            plt.title(f\"{((pos - 1) // 3) + 1} . {k} - {c}\", fontsize = 8)\n",
    "            librosa.display.specshow(data = 1 - raw, sr = rate, hop_length = 1)\n",
    "            \n",
    "            # pos = n_col * (i >> 1) + j\n",
    "            # f, t, Sxx = signal.spectrogram(x, rate)\n",
    "            # plt.subplot(n_row, n_col, pos + 4)\n",
    "            # plt.pcolormesh(t, f, 1 - Sxx, shading = 'gouraud')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show();\n",
    "\n",
    "def spectrogram(data : Board, rate : int, channels : Clause, n_row : int = 5, n_col : int = 12) :\n",
    "    sample = np.random.default_rng().integers(data.shape[0], size = n_row)\n",
    "\n",
    "    sample.sort()\n",
    "\n",
    "    plt.figure(figsize = (18, 2 * .48 * n_row))\n",
    "\n",
    "    freq = np.arange(1, rate >> 1)\n",
    "    pos  = 0\n",
    "    extd = np.append([0, 1, 1], freq[-1])\n",
    "    \n",
    "    for k in sample :\n",
    "        for c in channels :\n",
    "            pos += 1\n",
    "            x   = normalized(data[c][k])\n",
    "            coefficients, frequencies = pywt.cwt(x, scales = freq, wavelet = 'cmor')\n",
    "\n",
    "            plt.subplot(n_row, n_col, pos)\n",
    "            plt.imshow(np.abs(coefficients), aspect = 'auto', cmap = 'jet') #, extent = extd\n",
    "            # plt.colorbar(label=\"Magnitude\")\n",
    "            # plt.ylabel(\"Scale\")\n",
    "            # plt.xlabel(\"Time\")\n",
    "            # plt.title(\"CWT of a Chirp Signal\")\n",
    "            plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_logMelSpectrogram(data, rate) :\n",
    "    sns.heatmap(np.rot90(logMelSpectrogram(data, rate)), cmap = 'inferno')\n",
    "    \n",
    "    # loc, _ = plt.xticks()\n",
    "    # l      = np.round((loc - loc.min()) * len(data) / fe / loc.max(), 2), vmin = -6\n",
    "\n",
    "    # plt.xticks(loc, l)\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Frequency (Mel)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients, frequencies = pywt.cwt(train_eras[0]['C3'][0], scales = np.arange(1, SAMPLE_RATE >> 1), wavelet = 'cmor')\n",
    "\n",
    "1 / frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = [list(harmonic(x, bands_coeff).values()) for x in [trains[c] for c in eeg_Chans]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(H), np.shape(np.stack(np.stack(H, axis = 1), axis = 2)), np.shape(H[0][1][0])\n",
    "# harmonic(trains['C3'][256], bands_coeff).values())), H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(trains.drop(columns = ['EventType']),\n",
    "                                                    trains['EventType'], test_size = .2, random_state = 42)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split([v.tolist() for v in np.array(trains[eeg_Channels])],\n",
    "#                                                     trains['EventType'], test_size = .2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = np.array(X_train)\n",
    "# test_dataset  = X_test\n",
    "\n",
    "# train_dataset = structure(X_train, SAMPLE_RATE, eeg_Chans[:2])\n",
    "# test_dataset  = structure(X_test, SAMPLE_RATE, eeg_Chans[:2])\n",
    "\n",
    "train_dataset = structure(X_train, SAMPLE_RATE, eeg_Chans[:2])\n",
    "test_dataset  = structure(X_test, SAMPLE_RATE, eeg_Chans[:2])\n",
    "\n",
    "# print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### • Test prédiction\n",
    "\n",
    "# UNITS     : int   = 100\n",
    "BATCHSIZE : int   = 30\n",
    "EPOCH     : int   = 1000\n",
    "ZERO      : int   = 64\n",
    "DROPOUT   : float = .2\n",
    "# kl_divergence mean_squared_logarithmic_error mean_absolute_error\n",
    "LOSS      : None  = 'sparse_categorical_crossentropy'\n",
    "ACTIV     : None  = LeakyReLU   # PReLU, \n",
    "OPTIMIZER : None  = 'AdamW'     # adamax, , adafactor, adam, nadam\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# - 1 -\n",
    "model.add(Conv1D(filters = ZERO, kernel_size = (5), dilation_rate = 2,\n",
    "                 input_shape = train_dataset.shape[1: ]))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(ACTIV())\n",
    "\n",
    "# - 2 -\n",
    "model.add(Conv1D(filters = ZERO << 1, kernel_size = (5), dilation_rate = 2))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(ACTIV())\n",
    "\n",
    "# - 3 -\n",
    "model.add(Conv1D(filters = ZERO << 2, kernel_size = (5), dilation_rate = 2))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(ACTIV())\n",
    "model.add(GlobalAveragePooling1D()) # \n",
    "\n",
    "# Classification\n",
    "model.add(Dense(ZERO << 2))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(ACTIV())\n",
    "model.add(Dense(len(hands_event), activation = 'softmax')) # sigmoid  \n",
    "\n",
    "'''\n",
    "# Ajout de la premiere couche lstm\n",
    "model.add(LSTM(ZERO, input_shape = train_dataset.shape[1:], activation = ACTIV(), return_sequences = True)) #\n",
    "model.add(LSTM(ZERO, dropout = DROPOUT, return_sequences = False))\n",
    "\n",
    "# Ajout de la couche de sortie\n",
    "model.add(Dense(len(hands_event), activation = 'softmax'))\n",
    "'''\n",
    "\n",
    "model.compile(optimizer = OPTIMIZER, loss = LOSS, metrics = ['accuracy'])\n",
    "# model.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop    = EarlyStopping(monitor = 'val_accuracy', mode = 'max', verbose = 1, patience = 50)\n",
    "history = model.fit(train_dataset, y_train, validation_data = (test_dataset, y_test), verbose = 1,\n",
    "                    batch_size = BATCHSIZE, epochs = EPOCH, callbacks = [stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_dataset)\n",
    "\n",
    "sum([np.where(x > .5)[0][0] for x in pred] == y_test) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values  = history_dict['loss']\n",
    "acc_values   = history_dict['accuracy']\n",
    "absc         = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.figure(figsize = (12, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(absc, loss_values, label = 'Loss')\n",
    "plt.plot(absc, acc_values, label = 'Accuracy')\n",
    "plt.title('Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(absc, history_dict['val_loss'], label = 'Loss')\n",
    "plt.plot(absc, history_dict['val_accuracy'], label = 'Accuracy')\n",
    "plt.title('Testing')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation densité spectrale du Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Densité Spectral du Signal\n",
    "\n",
    "plot_psd(train_csv, train_eras, rate = SAMPLE_RATE, Channels = eeg_Chans, titled = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Densité spectrale / échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Densité spectral / échantillon\n",
    "\n",
    "n   = 40\n",
    "scp = samples(train_samples, n)\n",
    "pos = -2\n",
    "\n",
    "plt.figure(figsize = (15, n * 1.5))\n",
    "\n",
    "for i in scp :\n",
    "    pos += 2\n",
    "\n",
    "    for c in eeg_Chans :\n",
    "        x = train_eras[0][c][i]\n",
    "        f, Pxx_den = signal.welch(x, SAMPLE_RATE)   # , scaling = 'spectrum'\n",
    "        \n",
    "        plt.subplot(n, 4, pos + 1)\n",
    "        plt.semilogy(f, Pxx_den, label = c)\n",
    "        plt.title(f\"welch - {i + 1}\", fontsize = 11)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(n, 4, pos + 2)\n",
    "        r, _ = plt.psd(x, Fs = SAMPLE_RATE, label = c) # , NFFT = NFFT\n",
    "        plt.title(f\"psd - {i + 1}\", fontsize = 11)\n",
    "        plt.xlabel('')\n",
    "        plt.ylabel('')\n",
    "        # plt.legend()\n",
    "\n",
    "plt.xlabel('frequency [Hz]')\n",
    "# plt.ylabel('PSD [V**2/Hz]')\n",
    "plt.tight_layout()\n",
    "plt.show();\n",
    "\n",
    "# f, Pxx_den = signal.welch(train_eras[0]['C3'][752], SAMPLE_RATE)\n",
    "\n",
    "# print(len(Pxx_den))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation Epoques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Epoques\n",
    "\n",
    "for i in range(len(fics))[:: 3] :\n",
    "    plot_signal(train_csv[i], train_parts[i], train_spots[0][i], train_spots[1][i], channels = eeg_Chans, # \n",
    "                period = SCOPE, lag = LAG, title = headers[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation décomposition des signaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Décomposition des signaux\n",
    "\n",
    "# Test de décomposition des signaux en bandes de fréquences spécifiques compatibles avec les répartitions usuelles\n",
    "# dans le domaine des EEG ['Delta', 'Theta', 'Alpha', 'Beta', 'Gamma']\n",
    "\n",
    "for df, token in zip(train_eras, ['Gauche', 'Droite']) :\n",
    "    print(f\"Exemples - Évènement Discriminé Main {token}\")\n",
    "    plot_wavelets(df, bands_coeff, eeg_Chans, scope = 30, headers = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation des spectrogrammes (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in zip(num_events, ['Gauche', 'Droite']) :\n",
    "    print(f\"Exemples - Évènement Discriminé Main {t}\")\n",
    "    spectrogram(train_eras[i], SAMPLE_RATE, eeg_Chans, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test PCA - (Non cloncluant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Test PCA - (Non cloncluant)\n",
    "\n",
    "nca = SCOPE >> 2\n",
    "pca = PCA(nca)\n",
    "\n",
    "_, ax = plt.subplots(nrows = 2, ncols = 3, figsize = (15, 5))\n",
    "\n",
    "for i, d in enumerate(train_eras) :\n",
    "    for j, c in enumerate(eeg_Chans) :\n",
    "        Z = sc.fit_transform(list(d[c].to_list())) # \n",
    "        principal_components = pca.fit_transform(Z)\n",
    "        \n",
    "        ax[i, j].plot(range(nca), np.cumsum(pca.explained_variance_ratio_))\n",
    "        ax[i, j].set_title(f'{c} . {i}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • MNE époque (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • MNE époque (test)\n",
    "\n",
    "raw_csv = train_csv[0][eeg_Chans]\n",
    "info    = mne.create_info(ch_names = eeg_Chans, sfreq = SAMPLE_RATE, ch_types = 'eeg')\n",
    "raw_mne = mne.io.RawArray(raw_csv.T * 1e-6, info)\n",
    "loc     = np.where(train_csv[0]['EventStart'] == 1)[0]\n",
    "\n",
    "# display(compare(np.sort(np.concatenate((train_spots[0][0], train_spots[1][0]))), loc))\n",
    "\n",
    "tmin, tmax = -0., 1\n",
    "\n",
    "# loc = mne.find_events(raw_mne, stim_channel = 'C3')\n",
    "# event_id = dict(C3 = 1, aud_r = 2, vis_l = 3, vis_r = 4)\n",
    "# raw = mne.io.Raw(raw_mne, preload = True)\n",
    "# raw.filter(2, None, method = 'iir')           # replace baselining with high-pass\n",
    "# events = mne.read_events(event_fname)\n",
    "\n",
    "# raw.info['bads'] = ['MEG 2443']  # set bad channels\n",
    "# picks = mne.pick_types(info, meg = 'grad', eeg = True, eog = False, exclude = 'bads')\n",
    "# Read epochs\n",
    "absc = mne.Epochs(raw_mne, np.array([loc, loc, loc]).T, None, tmin, tmax, proj = False,\n",
    "                    picks = None, baseline = None, preload = True, verbose = False) # event_id picks\n",
    "\n",
    "# labels = epochs.events[::5, -1]\n",
    "\n",
    "# events\n",
    "\n",
    "# raw_mne.plot();\n",
    "\n",
    "# raw_mne['C3'][0][0], len(df_train_csv[2]['Cz'])\n",
    "\n",
    "display(absc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test de classification - Proposition inputs 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### • Test de classification - Proposition inputs 01\n",
    "\n",
    "# X1 = np.where(trains['EventType'] == 0, trains['C3'], trains['Cz']) \n",
    "# X2 = np.where(trains['EventType'] == 0, trains['Cz'], trains['C4']) \n",
    "# X = [signal.welch(list(v), SAMPLE_RATE)[1] for v in X1 + X2]\n",
    "\n",
    "# X = [np.append([], list(harmonic(v, bands_coeff).values())) for v in X]\n",
    "# X = np.where(trains['EventType'] == 0, trains['C3'], 2 * trains['Cz'])\n",
    "# X = [signal.welch(list(v), SAMPLE_RATE)[1] for v in X], trains['C3'] + trains['Cz'], trains['C4'] + trains['Cz']\n",
    "\n",
    "# diff = np.array([np.cos(x) for x in (trains['C3'] ** 2 + trains['C4'] ** 2)])  + 2 * trains['Cz']\n",
    "\n",
    "# dist =np.array([list(np.sqrt(x)) for x in (trains['C3'] ** 2 + trains['C4'] ** 2)])\n",
    "# display(np.shape(np.array()))\n",
    "\n",
    "# diff = np.array([d - v for v, d in zip(, dist)]) - trains['Cz'] * [x.mean() for x in trains['Cz']]\n",
    "\n",
    "# cz_min = [v.min() for v in trains['Cz']]\n",
    "# cz_max = [v.max() for v in trains['Cz']]\n",
    "\n",
    "# display(compare(cz_min, cz_max), )\n",
    "\n",
    "# trio = zip(trains['C4'], trains['C3'], trains['Cz'])\n",
    "\n",
    "# prd = [(c3 - c4) * cz for c3, c4, cz in trio] \n",
    "\n",
    "# 'Delta', 'Theta', 'Alpha', 'Beta', 'Gamma'\n",
    "\n",
    "# b, a = bands_coeff['Delta']\n",
    "\n",
    "# c3 = np.array([bandpass_filter(bw, b, a) for bw in trains['C3']])\n",
    "# c4 = np.array([bandpass_filter(bw, b, a) for bw in trains['C4']])\n",
    "\n",
    "# diff = (c4 - c3) / trains['Cz'].max() #/ cz.max() # [for v in cz]\n",
    "# X = [np.sign(s) for s in trains['C4'] - trains['C3']]\n",
    "# X = np.array([normalized(bandpass_filter(bw, b, a)) for bw in X])\n",
    "\n",
    "# X = [sk_p.minmax_scale(pywt.dwt(c4, wavelet = 'db4')[0] - pywt.dwt(c3, wavelet = 'db4')[0]) for c3, c4 in zip(trains['C3'], trains['C4'])]\n",
    "# X = [v) for v in X]\n",
    "\n",
    "# X = [pywt.dwt(v, wavelet = 'db4')[0] for v in X]\n",
    "# X = [normalized(pywt.dwt(v, wavelet = 'db4')[0]) for v in X]\n",
    "# X = [normalized(pywt.dwt(v, wavelet = 'db4')[0]) for v in X]\n",
    "# X = [normalized(v) for v in X]\n",
    "\n",
    "# X = [np.append([], v.tolist()) for v in X]\n",
    "\n",
    "# ret = pywt.dwt(data, wavelet = 'db1') #, level = 4, mode = 'antisymmetric'\n",
    "\n",
    "# for o in ret :\n",
    "#     print(np.shape(o))\n",
    "\n",
    "# # np.shape(ret[3])[0] / np.shape(train_csv[0]['Cz'])[0]\n",
    "# # (ret)\n",
    "\n",
    "# plt.figure(figsize = (20, 4))\n",
    "\n",
    "# # plt.subplot(1, 3, 1)\n",
    "# plt.plot(data)\n",
    "\n",
    "# for o in ret :\n",
    "#     plt.plot(o)\n",
    "\n",
    "\n",
    "# # plt.title(\"Original Signal\")\n",
    "# # plt.subplot(1, 3, 2)\n",
    "# # plt.title(\"Approximation Coefficients\")\n",
    "# # plt.subplot(1, 3, 3)\n",
    "# # plt.plot(cD)\n",
    "# # plt.title(\"Detail Coefficients\")\n",
    "# # plt.tight_layout()\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test de classification - Proposition inputs 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● RandomForestClassifier(n_jobs=-1) : Accuracy -> 53.9% (±0.014, max : 55.9%)\n",
      "\t-> score / test : 53.0%\n",
      "\n",
      "● KNeighborsClassifier() : Accuracy -> 52.2% (±0.033, max : 57.4%)\n",
      "\t-> score / test : 54.6%\n",
      "\n",
      "● SVC() : Accuracy -> 54.2% (±0.025, max : 58.6%)\n",
      "\t-> score / test : 55.3%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### • Test de classification\n",
    "\n",
    "# pca = PCA()\n",
    "# sc = StandardScaler()\n",
    "\n",
    "# X = pca.fit_transform(X)\n",
    "\n",
    "# display(np.shape(X))\n",
    "\n",
    "# X = (trains['C4'] - trains['C3']) # / [v.max() for v in trains['Cz']]\n",
    "# X = [sk_p.minmax_scale(pywt.dwt(v, wavelet = 'db4')[0]) for v in X]\n",
    "# y = trains['EventType']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 42)\n",
    "\n",
    "X_train_scaled = X_train\n",
    "X_test_scaled  = X_test\n",
    "\n",
    "# X_train_scaled = sc.fit_transform(X_train)\n",
    "# X_test_scaled  = sc.transform(X_test)\n",
    "\n",
    "# csp = CSP(nfilter = 2)\n",
    "# K-plus proches voisins\n",
    "knc = neighbors.KNeighborsClassifier()\n",
    "# SVM (support vector machine)[, 'auto', kernel = 'rbf']\n",
    "clf = svm.SVC(gamma = 'scale')\n",
    "# RandomForest \n",
    "rfc = ensemble.RandomForestClassifier(n_jobs = -1)\n",
    "#\n",
    "lrg = LogisticRegression()\n",
    "#ExtraTreesClassifier \n",
    "# Voting_clf = VotingClassifier(estimators = [('knn', clf1), ('svm', clf2), ('rf', clf3)], voting = 'hard')\n",
    "# cv3        = model_selection.KFold(n_splits = 3, random_state = 42, shuffle = True), clf4\n",
    "\n",
    "# Create a pipeline\n",
    "# pip = Pipeline([('RFC', rfc), ('SVM', clf)])    # ('CSP', csp), \n",
    "\n",
    "# pip.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = pip.predict(X_test)\n",
    "\n",
    "# y_train = np.array(y_train)\n",
    "\n",
    "# print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n",
    "# print(\"Accuracy score:\", accuracy_score(y_test, y_pred)), cv = 5, lrg\n",
    "\n",
    "for reg in [rfc, knc, clf] :\n",
    "    scores : dict = cross_validate(reg, X_train, y_train, scoring = ['accuracy'])\n",
    "    \n",
    "    reg.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    r = scores['test_accuracy']\n",
    "    \n",
    "    print(f\"● {reg} : Accuracy -> {r.mean():.1%} (±{r.std():.2}, max : {r.max():.1%})\")\n",
    "    print(f\"\\t-> score / test : {reg.score(X_test_scaled, y_test):.1%}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, a = bands_coeff['Theta']\n",
    "\n",
    "i = np.random.randint(np.shape(X)[0])\n",
    "# X = [np.sign(s) for s in (trains['C4'] - trains['C3'])]\n",
    "# y = {band: bandpass_filter(X[i], b, a) for band, (b, a) in bands_coeff.items()}\n",
    "y = trains['C4'][i] - trains['C3'][i]\n",
    "\n",
    "plt.figure()\n",
    "# plt.style.use('')\n",
    "\n",
    "plt.plot(normalized(bandpass_filter(y, b, a)), label = 'C4 - C3')\n",
    "plt.plot(normalized(bandpass_filter(np.sign(y), b, a)), label = '[C4 - C3]', c = np.random.rand(1, 3)[0])\n",
    "# plt.plot(pywt.dwt(y, wavelet = 'db4')[0], label = 'C4 - C3')\n",
    "# plt.plot(pywt.dwt(np.sign(y), wavelet = 'db4')[0], label = '[C4 - C3]')\n",
    "# plt.plot(np.zeros(512), ls = '--', c = np.random.rand(1, 3)[0])\n",
    "\n",
    "# print(np.shape(pywt.dwt(y, wavelet = 'db8')))\n",
    "\n",
    "# plt.plot(bandpass_filter(np.sign(trains['C3'][i]), b, a), label = 'C3')\n",
    "# plt.plot(bandpass_filter(np.sign(trains['C4'][i]), b, a), label = 'C4')\n",
    "# plt.plot(bandpass_filter(np.sign(trains['Cz'][i]), b, a), label = 'Cz')\n",
    "\n",
    "# for (band, signal) in reversed(y.items()) :\n",
    "#     plt.plot(pd.Series(signal), label = f'{band}', c = np.random.rand(1, 3)[0])\n",
    "\n",
    "plt.title(f\"{i}\")\n",
    "plt.legend(loc = 'upper right')\n",
    "\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    ## K-plus proches voisins\n",
    "    'knn__n_neighbors' : range(2),\n",
    "    ## SVM\n",
    "    'svm__C'      : [0.1, 1, 5],\n",
    "    'svm__kernel' : ['linear', 'softmax', 'sigmoid', 'rbf'],\n",
    "    ## RandomForest\n",
    "    # 'rf__max_features'      : ['sqrt', 'log2', None],\n",
    "    # 'rf__min_samples_split' : range(2, 32, 2),\n",
    "    # , ('rf', clf3), ('rf', clf3)\n",
    "    'estimators': [[('knn', knc), ('svm', svm)], [('knn', knc), ('svm', svm)]] \n",
    "    }\n",
    "\n",
    "grid = model_selection.GridSearchCV(estimator = Voting_clf, param_grid = params, cv = 5) \\\n",
    "    .fit(X_train_scaled, y_train)\n",
    "\n",
    "# parametres = {'max_features' : ['log2', 'sqrt', None], 'min_samples_split' : range(2, 32, 2)}\n",
    "\n",
    "# vclf = model_selection.GridSearchCV(estimator = clf3, param_grid = parametres, cv = 3) \\\n",
    "#     .fit(X_train_scaled, y_train)\n",
    "\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n",
    "print('score train :', grid.score(X_train_scaled, y_train))\n",
    "print('score test :', grid.score(X_test_scaled, y_test))\n",
    "\n",
    "# print(vclf.best_estimator_, vclf.best_params_, vclf.best_score_)\n",
    "# print('score train :', grid.score(X_train_scaled, y_train), vclf.score(X_train_scaled, y_train))\n",
    "# print('score test  :', grid.score(X_test_scaled, y_test), vclf.score(X_test_scaled, y_test))\n",
    "\n",
    "df_train_cpy, event_start = fancy_df(train_csv, label_csv['EventType'], hands_event, SCOPE)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize = (24, 5), sharey = True)\n",
    "sig = .05\n",
    "\n",
    "axes.plot(train_csv['C3'])\n",
    "\n",
    "for p in event_start :\n",
    "    axes.axvspan(p[0] - (SCOPE >> 1), p[0] + 1.5 * SCOPE, facecolor = 'orangered', alpha = .5)\n",
    "\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos    = 16\n",
    "start  = event_start[pos][0]\n",
    "entrant  = start + SCOPE\n",
    "df   = df_train_cpy['C3_4'][start : entrant]\n",
    "smooth = df.copy()\n",
    "n      = 5\n",
    "alpha  = 1 / 3\n",
    "dec    = int(n / alpha)\n",
    "\n",
    "plt.figure(figsize = (24, 5))\n",
    "plot_window(train_csv, ['C3', 'C4', 'C3 + C4'], start, SCOPE)\n",
    "\n",
    "# Lissage des hautes fréquences\n",
    "for _ in range(n) :\n",
    "    smooth = simple_exponential_smoothing(smooth, alpha, 0)\n",
    "\n",
    "smooth = pd.Series(index = range(start, entrant + n - dec), data = smooth[dec :])\n",
    "\n",
    "# plt.plot(raw - smooth, label = hands[event_start[pos][1]])\n",
    "plt.plot(smooth, '--', label = hands_event[event_start[pos][1]])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Introduction :  \n",
    "L’électroencéphalogramme (EEG) est une technique d’imagerie cérébrale utilisée pour étudier les activités du cerveau.  \n",
    "En plaçant des capteurs sur le cuir chevelu, l’activité électrique du cerveau est enregistrée,  \n",
    "   ce qui permet de comprendre les fonctionnements cérébraux et d’identifier certains schémas que l’on peut ensuite attribuer à des comportements précis.  \n",
    "Un des schémas d'EEG qui a été beaucoup étudié est l’imagerie motrice (IM), ou le mouvement imaginaire de la main.  \n",
    "Les IM créent des schémas bien définis qui peuvent être détectés.  \n",
    "Le but de ce projet est de créer et d’entraîner un programme permettant de prédire si l’IM d’une personne correspond à un mouvement de la main droite ou de la main gauche.  \n",
    "# **2. Étapes du projet**\n",
    "- Prétraitement des Données :  \n",
    "Les données EEG sont sujettes à des artefacts ou des erreurs de collecte dues à des mouvements parasites ou des interférences.  \n",
    "Il est donc nécessaire d'appliquer un système de prétraitement des données pour réduire le bruit et extraire les bandes de fréquences pertinentes.\n",
    "\n",
    "- Segmentation des données et extraction des caractéristiques :  \n",
    "Les données EEG sont présentées comme un flux continu. Il est donc important, pour une meilleure analyse, de diviser les données en segments temporels correspondant à l’IM.  \n",
    "Ensuite, identifier et extraire les caractéristiques pertinentes des signaux EEG associées aux IM est essentiel.  \n",
    "Cela comprend la puissance et d'autres spécificités de l’activité électrique qui définissent les IM.\n",
    "\n",
    "- Analyse statistique exploratoire :  \n",
    "Utiliser les outils d’analyse exploratoire pour mieux comprendre les données et identifier les tendances ou les patterns significatifs.\n",
    "\n",
    "- Entraînement du modèle :  \n",
    "Entraîner un modèle permettant de distinguer les différences entre les IM des mains droite et gauche.  \n",
    "Optimiser le modèle et évaluer sa performance sur un ensemble de test.\n",
    "\n",
    "- Conclusion :  \n",
    "Ces étapes sont cruciales pour développer un programme efficace de prédiction des mouvements imaginaires de la main basé sur les données EEG.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "parts = []        #\n",
    "temp = [[], []]  # Les époques pour tous les cannaux et tous les évènements.\n",
    "spots = [[], []]  # Apparitions des évènements\n",
    "\n",
    "# Pour la standardisation du nombre d'échantillon max conservé\n",
    "ceil   = min([len(label_csv[i]['EventType']) for i in count])\n",
    "merge  = False\n",
    "\n",
    "# Extraction des données relavitives à l'apparition des évènements.\n",
    "for i in count :\n",
    "    df   = train_csv[i]\n",
    "    kind  = label_csv[i]['EventType'][: ceil]\n",
    "    loc = np.where(df['EventStart'] == 1)[: ceil]\n",
    "\n",
    "    parts.append(zero_removal(df['C3'], 75))\n",
    "\n",
    "    for i in num_events :\n",
    "        spots[i].append(np.array(*loc)[*np.where(kind == i)])\n",
    "\n",
    "        room = event_epochs(spots[i][-1], SCOPE, LAG)\n",
    "\n",
    "        temp[i].append([full_event(df[c], room, merge)for c in eeg_Channels])\n",
    "\n",
    "# Regroupement des données en fonction du type de l'évènement et du cannal d'observation\n",
    "if merge :\n",
    "    temp = [[[np.append([], T[j :: 3]) for T in temp[i]] for j in range(3)] for i in num_events]\n",
    "else :\n",
    "    store = [[[], [], []], [[], [], []]]\n",
    "    \n",
    "    [[[[store[i][j].append(G) for G in X] for j, X in enumerate(T)] for T in temp[i]] for i in num_events]\n",
    "\n",
    "    temp = store\n",
    "    # res = [[np.concatenate(np.stack(tries[i], axis = 1)[j], axis = 0) for j in range(3)] for i in n_type]\n",
    "\n",
    "eras    = [pd.DataFrame(dict(zip(eeg_Channels, [pd.Series(X) for X in temp[i]]))) for i in num_events]\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# display(df_left)\n",
    "# display(df_right)\n",
    "\n",
    "# left = np.array([[f(v) for v in tmp[c]] for c in eeg_Chans for f in foo])\n",
    "# display(pd.DataFrame(left.T, columns = [f\"L{c}_{x.__name__}\" for x in foo for c in eeg_Chans])), columns = [f\"L{c}_{x.__name__}\" for x in foo for c in eeg_Chans]\n",
    "\n",
    "# print(np.shape(left_df))\n",
    "\n",
    "# right = np.array([[f(v) for v in tmp[c]] for c in eeg_Chans for f in foo])\n",
    "# display(pd.DataFrame(right.T, columns = [f\"R{c}_{x.__name__}\" for x in foo for c in eeg_Chans]))\n",
    "\n",
    "# left[1] - left[0], left[4] - left[3], left[7] - left[6], left[10] - left[9], left[13] - left[12]\n",
    "# left[0], left[1]\n",
    "\n",
    "# left_min = [apply_func(tmp[c], np.min) for c in eeg_Chans]apply_func(tmp[c], f)\n",
    "# left_max = [apply_func(tmp[c], np.max) for c in eeg_Chans]\n",
    "# left_std = [apply_func(tmp[c], np.std) for c in eeg_Chans]\n",
    "# left_ptp = [apply_func(tmp[c], np.ptp) for c in eeg_Chans]\n",
    "# left_men = [apply_func(tmp[c], np.mean) for c in eeg_Chans]\n",
    "# left_med = [apply_func(tmp[c], np.median) for c in eeg_Chans]\n",
    "\n",
    "# right_min = [apply_func(tmp[c], np.min) for c in eeg_Chans]\n",
    "# right_max = [apply_func(tmp[c], np.max) for c in eeg_Chans]\n",
    "# right_std = [apply_func(tmp[c], np.std) for c in eeg_Chans]\n",
    "# right_ptp = [apply_func(tmp[c], np.ptp) for c in eeg_Chans]\n",
    "# right_men = [apply_func(tmp[c], np.mean) for c in eeg_Chans]\n",
    "# right_med = [apply_func(tmp[c], np.median) for c in eeg_Chans]\n",
    "\n",
    "# display(pd.DataFrame(np.array((*left_min, *left_max, *left_std, *left_ptp, *left_men, *left_med)).T))\n",
    "# display(pd.DataFrame(np.array((*right_min, *right_max, *right_std, *right_ptp, *right_men, *right_med)).T))\n",
    "\n",
    "# pd.DataFrame(np.array((lc3_i, rc3_i, lc3_a, rc3_a, lc3_p, rc3_p, lc3_m, rc3_m,\n",
    "#                        lc4_i, rc4_i, lc4_a, rc4_a, lc4_p, rc4_p, lc4_m, rc4_m,\n",
    "#                        lcz_i, rcz_i, lcz_a, rcz_a, lcz_p, rcz_p, lcz_m, rcz_m)).T)\n",
    "\n",
    "# lc3_i = np.array(apply_func(left['C3'], np.min))\n",
    "# lc4_i = np.array(apply_func(left['C4'], np.min))\n",
    "# lcz_i = np.array(apply_func(left['Cz'], np.min))\n",
    "\n",
    "# lc3_a = np.array(apply_func(left['C3'], np.max))\n",
    "# lc4_a = np.array(apply_func(left['C4'], np.max))\n",
    "# lcz_a = np.array(apply_func(left['Cz'], np.max))\n",
    "\n",
    "# lc3_m = np.array(apply_func(left['C3'], np.mean))\n",
    "# lc4_m = np.array(apply_func(left['C4'], np.mean))\n",
    "# lcz_m = np.array(apply_func(left['Cz'], np.mean))\n",
    "\n",
    "# rc3_i = np.array(apply_func(right['C3'], np.min))\n",
    "# rc4_i = np.array(apply_func(right['C4'], np.min))\n",
    "# rcz_i = np.array(apply_func(right['Cz'], np.min))\n",
    "\n",
    "# rc3_a = np.array(apply_func(right['C3'], np.max))\n",
    "# rc4_a = np.array(apply_func(right['C4'], np.max))\n",
    "# rcz_a = np.array(apply_func(right['Cz'], np.max))\n",
    "\n",
    "# rc3_m = np.array(apply_func(right['C3'], np.mean))\n",
    "# rc4_m = np.array(apply_func(right['C4'], np.mean))\n",
    "# rcz_m = np.array(apply_func(right['Cz'], np.mean))\n",
    "\n",
    "# plt.scatter(x = lc3_i, y = lc3_a)\n",
    "# plt.scatter(x = rc3_i, y = rc3_a)\n",
    "# plt.scatter(x = lc4_i, y = lc4_a)\n",
    "# plt.scatter(x = rc4_i, y = rc4_a)\n",
    "\"\"\"\n",
    "\n",
    "# def apply_func(data : Board, foo) -> Vector : return [foo(v) for v in data]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
