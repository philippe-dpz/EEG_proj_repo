{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"text-align:center\" ><b>EEG</b> - Prédiction des Mouvements Imaginaires de la Main</h2>\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Le projet**\n",
    "- Intoduction  \n",
    "https://github.com/DataScientest-Studio/mar24_cds_eeg/blob/eric/references/Description_projet_EEG.pdf  \n",
    "https://www.bbci.de/competition/iv/desc_2b.pdf\n",
    "- Ressources / Données   \n",
    "https://www.kaggle.com/competitions/ucsd-neural-data-challenge/overview  \n",
    "- Bibliographie  \n",
    "https://www.bbci.de/competition/iv/desc_2b.pdf\n",
    "#### **2. Liens utils**\n",
    "- SciPy - *open-source software for mathematics, science, and engineering*  \n",
    "https://docs.scipy.org/doc/scipy/index.html  \n",
    "https://docs.scipy.org/doc/scipy/reference/signal.html  \n",
    "- MNE - *MEG + EEG Analysis & Visualisation*\n",
    "   - Accueil  \n",
    "   https://mne.tools/stable/index.html\n",
    "\n",
    "   - MNE - Data structures from arbitrary data  \n",
    "   https://mne.tools/stable/auto_tutorials/io/10_reading_meg_data.html#creating-mne-data-structures-from-arbitrary-data-from-memory\n",
    "   \n",
    "   - MNE - EEG Preprocessing  \n",
    "   https://mne.tools/dev/auto_tutorials/preprocessing/index.html  \n",
    "\n",
    "- pyRiemann - *Biosignals classification with Riemannian geometry*  \n",
    "https://pyriemann.readthedocs.io/en/latest/  \n",
    "- neurodsp - *Neuro Digital Signal Processing Toolbox*  \n",
    "https://neurodsp-tools.github.io/neurodsp/index.html#\n",
    "- Rythme Mu  \n",
    "https://fr.wikipedia.org/wiki/Rythme_Mu\n",
    "- Spectrogram from EEG  \n",
    "https://www.kaggle.com/code/cdeotte/how-to-make-spectrogram-from-eeg\n",
    "- Divers  \n",
    "https://signalprocessingsociety.org/  \n",
    "https://fr.wikipedia.org/wiki/Filtre_de_Butterworth  \n",
    "https://fr.wikipedia.org/wiki/Moyenne_mobile  \n",
    "https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html  \n",
    "https://perso.etis-lab.fr/ghaffari/2014_CCMB_Floride_USA.pdf  \n",
    "https://www.youtube.com/watch?v=wB417SAbdak&list=PLXc9qfVbMMN2TAoLHVW5NvNmJtwiHurzw  \n",
    "https://fastercapital.com/fr/sujet/identification-des-artefacts-de-traitement-du-signal-dans-des-sc%C3%A9narios-r%C3%A9els.html#:~:text=L'inspection%20visuelle%20est%20la,des%20pertes%20et%20du%20bruit.  \n",
    "   - Z-Score Normalisation  \n",
    "   https://fr.wikipedia.org/wiki/Cote_Z_(statistiques)  \n",
    "   https://typeset.io/questions/why-is-z-score-normalisation-necessary-in-pre-processing-eeg-1xv5jepyq5  \n",
    "\n",
    "   - Traitement numérique du signal  \n",
    "   https://fr.wikipedia.org/wiki/Traitement_num%C3%A9rique_du_signal  \n",
    "   - Ondelette  \n",
    "      - Wiki  \n",
    "      https://fr.wikipedia.org/wiki/Ondelette  \n",
    "\n",
    "      - L’analyse par ondelettes dans la vie de tous les jours  \n",
    "      https://interstices.info/lanalyse-par-ondelettes-dans-la-vie-de-tous-les-jours/  \n",
    "\n",
    "      - A guide for using the Wavelet Transform in Machine Learning  \n",
    "      https://ataspinar.com/2018/12/21/a-guide-for-using-the-wavelet-transform-in-machine-learning/\n",
    "      \n",
    "      - pyWavelets - *open source wavelet transform*  \n",
    "      https://pywavelets.readthedocs.io/en/latest/\n",
    "\n",
    "      - Ondelettes et applications  \n",
    "      https://www.i2m.univ-amu.fr/~caroline.chaux/GEOMDATA/TI-te5215.pdf\n",
    "\n",
    "   - Maximum de vraisemblance  \n",
    "   https://pmarchand1.github.io/ECL8202/notes_cours/03-Maximum_vraisemblance.html  \n",
    "   https://fr.wikipedia.org/wiki/Maximum_de_vraisemblance#:~:text=En%20statistique%2C%20l'estimateur%20du,maximisant%20la%20fonction%20de%20vraisemblance  \n",
    "\n",
    "   - Transformation de Fourier discrète  \n",
    "   https://fr.wikipedia.org/wiki/Transformation_de_Fourier_discr%C3%A8te  \n",
    "      - La Transformation de Fourier n’est pas adaptée à l’analyse des signaux non stationnaires.\n",
    "   - Neural Data Science in Python  \n",
    "   https://neuraldatascience.io/intro.html\n",
    "\n",
    "   - Preprocessing of EEG  \n",
    "   https://www.frontiersin.org/articles/10.3389/fninf.2015.00016/full#:~:text=The%20depositable%20preprocessing%20pipeline%20consists,with%20a%20low%20recording%20SNR  \n",
    "   https://typeset.io/papers/preprocessing-of-eeg-4go8vhcbty  \n",
    "   https://learn.neurotechedu.com/preprocessing  \n",
    "   https://g0rella.github.io/gorella_mwn/preprocessing_eeg.html  \n",
    "   \n",
    "   - Biblio :  \n",
    "   https://perso.telecom-paristech.fr/bloch/P6Image/ondelettestrsp.pdf  \n",
    "   https://www.math.u-bordeaux.fr/~jbigot/Site/Enseignement_files/ondelettesIMAT.pdf  \n",
    "   http://w3.cran.univ-lorraine.fr/perso/radu.ranta/pdf/cours_deb_ond%28fr%29.pdf\n",
    "   \n",
    "   - Digital Filtering  \n",
    "   http://notebooks.pluxbiosignals.com/notebooks/Categories/Pre-Process/digital_filtering_eeg_rev.html\n",
    "\n",
    "   - Processus stationnaire  \n",
    "   https://fr.wikipedia.org/wiki/Processus_stationnaire\n",
    "\n",
    "   - Analyse en composantes principales  \n",
    "   https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales#:~:text=L'ACP%2C%20d%C3%A9sign%C3%A9e%20en%20g%C3%A9n%C3%A9ral,une%20grandeur%20physique%2C%20comme%20les"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Chargement des différentes librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Chargement des différentes librairies\n",
    "\n",
    "import sys, os, time, math\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "from src.thot.mathesis import *\n",
    "\n",
    "from sklearn import model_selection, preprocessing as sk_p\n",
    "from sklearn import ensemble, svm, neighbors\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from keras.models import Sequential                             # type: ignore\n",
    "from keras.callbacks import EarlyStopping                       # type: ignore\n",
    "from keras.layers import GlobalAveragePooling1D                 # type: ignore\n",
    "from keras.layers import Dense, Dropout, Conv1D, LSTM           # type: ignore\n",
    "from keras.layers import LeakyReLU, ReLU, PReLU, ConvLSTM1D     # type: ignore\n",
    "# from keras.layers import Bidirectional, TimeDistributed, RepeatVector, Flatten\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.optimizers import AdamW, Adam            # type: ignore\n",
    "\n",
    "# from pyriemann.spatialfilters import CSP\n",
    "\n",
    "import pywt, librosa\n",
    "import seaborn as sns\n",
    "\n",
    "# from scipy.fft import fft\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Déclaration de constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-62, 500)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### • Déclaration de constantes\n",
    "\n",
    "# Fréquence d'échantillonnage - Hz (Nombre de valeur / sec)\n",
    "SAMPLE_RATE  = 250\n",
    "# Temps additionel pour étendre le domaines d'étude.\n",
    "LAG : int    = -62     # Décalage du signal dû signal ~250ms\n",
    "#\n",
    "PW2 : int    = int(np.floor(np.log2(SAMPLE_RATE))) # 2 << SAMPLE_RATE // 32\n",
    "#\n",
    "NFFT : int   = 1 << PW2\n",
    "# Epoque en sec donnée en nombre d'échantillon consectutif # 4\" de données (multiple de 2)\n",
    "SCOPE : int  = SAMPLE_RATE << 1 # (1 << PW2) * 4\n",
    "# Deux enregistrements bipolaires + neutre\n",
    "eeg_Chans    = ['C3', 'C4', 'Cz']\n",
    "# Liste des cannaux eeg associés aux évènement 0 et 1\n",
    "eeg_left     = [f'{c}_0' for c in eeg_Chans]\n",
    "eeg_right    = [f'{c}_1' for c in eeg_Chans]\n",
    "full_eeg     = eeg_left + eeg_right\n",
    "# Trois enregistrements musculaires\n",
    "ecg_Chans    = ['EOG:ch01', 'EOG:ch02', 'EOG:ch03']\n",
    "# Liste de tous les cannaux des dataframes\n",
    "all_chans    = eeg_Chans + ecg_Chans\n",
    "# Correspondance pour la classification\n",
    "hands_event  = {0: 'Left', 1: 'Right'}\n",
    "# Les bandes de fréquences d'intérêt\n",
    "eeg_bands    = {'Delta' : (0.1, 4.0),\n",
    "                'Theta' : (4.1, 8.0),\n",
    "                'Alpha' : (8.1, 14.0),\n",
    "                'Beta'  : (14.1, 30.0),\n",
    "                'Gamma' : (30.1, (SAMPLE_RATE >> 1) - 1),}\n",
    "# Coefficients pour filtres Butterworth numérique d'ordre N pour le filtrage passe-bande\n",
    "bands_coeff  = {band : butter_bandpass(low, high, SAMPLE_RATE) for band, (low, high) in eeg_bands.items()}\n",
    "# Nombre dévènement à prédir\n",
    "num_events   = range(len(hands_event))\n",
    "\n",
    "LAG, SCOPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Acquisition des données d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Acquisition des données d'entrainement\n",
    "\n",
    "target  = \"../data/data.zip\"\n",
    "count   = len('train/')\n",
    "fics    = [x[count :] for x in files_in_zip(target, directory = 'train')]\n",
    "\n",
    "[fics.remove(x) for x in fics[:: -3]]\n",
    "\n",
    "# Acquisition des fichiers du répertoir dans le fichier zip\n",
    "train_csv = csv_in_zip(target, directory = 'train', files = fics)\n",
    "label_csv = csv_in_zip(target, directory = 'y_train_only', files = fics)\n",
    "\n",
    "notes   = filename(fics)\n",
    "headers = [f\"{t} . {i + 1}\" for i, t in enumerate(notes)]\n",
    "count   = range(len(train_csv))\n",
    "\n",
    "# fics    = [f'B0{i}0{j}T.csv' for i in range(1, 9) for j in range(1, 4)]\n",
    "# df_train_pkl = pkl_in_zip(path, fichier_specifique = 'epoched_train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Acquisition des données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Acquisition des données de test\n",
    "\n",
    "test_csv = csv_in_zip(target, directory = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Catch22 émulateur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://r-packages.io/packages/Rcatch22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splinefit(y : Vector, nBreaks : int = 3, deg : int = 3) -> Vector :\n",
    "    size      : int = len(y)\n",
    "    piecesExt : int = 4\n",
    "    nSpline   : int = 4\n",
    "    pieces    : int = 2\n",
    "    nCoeff    : int = nSpline * piecesExt\n",
    "\n",
    "    breaks = [0, int(size >> 1) - 1, size - 1]\n",
    "    \n",
    "    # -- splinebase\n",
    "    \n",
    "    # repeat spacing\n",
    "    hCopy = list(adjacent(breaks)) * 2\n",
    "    # add breaks\n",
    "    hExt  = adjacent(list(np.flip(breaks[0] - np.cumsum(hCopy[1 :]))) +\n",
    "                     list(breaks) +\n",
    "                     list(breaks[2] + np.cumsum(hCopy[: deg])))\n",
    "    # expand h using the index matrix ii\n",
    "    I2 = [[min(j + i, piecesExt - 1) for j in range(piecesExt)]\n",
    "          for i in range(deg + 1)]\n",
    "    # expanded h\n",
    "    H  = [hExt[I2[i % nSpline][i // nSpline]] for i in range(nCoeff)]\n",
    "    # recursive generation of B-splines\n",
    "    Q  = np.zeros((nSpline, piecesExt))\n",
    "    # initialise polynomial coefficients + 1\n",
    "    Kf = np.zeros((nCoeff, nSpline))\n",
    "    # Q2 = np.zeros((nCoeff, nSpline)) \n",
    "\n",
    "    for i in range(0, nCoeff, nSpline) : Kf[i][0] = 1\n",
    "\n",
    "    for k in range(1, nSpline) :\n",
    "        # antiderivatives of splines\n",
    "        for j in range(k) :\n",
    "            for l in range(nCoeff) : Kf[l][j] *= H[l] / (k - j)\n",
    "        \n",
    "        for l in range(nCoeff) :\n",
    "            for m in range(nSpline) : Q[l % nSpline][l // nSpline] += Kf[l][m]\n",
    "\n",
    "            # Q2[l % nSpline][l // nSpline] += sum(K[l])\n",
    "\n",
    "            # tmp = Q[l % nSpline][l // nSpline] - Q2[l % nSpline][l // nSpline]\n",
    "\n",
    "            # if (tmp != 0) :\n",
    "            #     inc += 1\n",
    "            #     print(inc, k, l, tmp)\n",
    "\n",
    "                # for m in range(nSpline) : print(K[l][m])\n",
    "        \n",
    "        # cumsum\n",
    "        for l in range(piecesExt) :\n",
    "            for m in range(1, nSpline) : Q[m][l] += Q[m - 1][l]\n",
    "        \n",
    "        for l in range(nCoeff) :\n",
    "            md = l % nSpline \n",
    "            \n",
    "            Kf[l][k] = 0 if (md == 0) else Q[md - 1][l // nSpline]  # questionable\n",
    "        \n",
    "        # normalise antiderivatives by max value\n",
    "        fmax = [Q[nSpline - 1][i] for _ in range(nSpline) for i in range(piecesExt)]\n",
    "        \n",
    "        for j in range(k + 1) :\n",
    "            for l in range(nCoeff) : Kf[l][j] /= fmax[l]\n",
    "\n",
    "        # diff to adjacent antiderivatives\n",
    "        for i in range(nCoeff - deg) :\n",
    "            for j in range(k + 1) : Kf[i][j] -= Kf[deg + i][j]\n",
    "\n",
    "        for i in range(1, nCoeff, nSpline) : Kf[i][k] = 0\n",
    "    \n",
    "    # scale coefficients\n",
    "    scale = np.ones(nCoeff)\n",
    "    \n",
    "    for k in range(nSpline - 1) :\n",
    "        scale = np.divide(scale, H)\n",
    "\n",
    "        for i in range(nCoeff) :\n",
    "            Kf[i][(nSpline - 1) - (k + 1)] *= scale[i]\n",
    "    \n",
    "    # reduce pieces and sort coefficients by interval number\n",
    "    jj = [[nSpline * (j + 1) if (i == 0) else deg for j in range(pieces)]\n",
    "          for i in range(nSpline)]\n",
    "    \n",
    "    for i in range(1, nSpline) :\n",
    "        for j in range(pieces) : jj[i][j] += jj[i - 1][j]\n",
    "    \n",
    "    print([jj[i % nSpline][i // nSpline] - 2 for i in range(nSpline * pieces)])\n",
    "\n",
    "    coefs_out = [[Kf[jj[i % nSpline][i // nSpline] - 2][j] for j in range(nSpline)]\n",
    "                 for i in range(nSpline * pieces)]\n",
    "    \n",
    "    # -- create first B-splines to feed into optimization\n",
    "    \n",
    "    score : int = size * nSpline\n",
    "\n",
    "    # x-values for B-splines\n",
    "    xsB    = np.zeros(score)\n",
    "    indexB = np.zeros(score)\n",
    "    \n",
    "    stop : int = 1\n",
    "\n",
    "    for i in range(size) :\n",
    "        if((i >= breaks[stop]) & (stop < nBreaks - 1)) : stop += 1\n",
    "        \n",
    "        m = stop - 1\n",
    "\n",
    "        for j in range(nSpline) :\n",
    "            p = i * nSpline + j\n",
    "\n",
    "            xsB[p]    = i - breaks[m]\n",
    "            indexB[p] = m * nSpline + j\n",
    "\n",
    "    vB = [coefs_out[indexB[i]][0] for i in range(score)]\n",
    "    \n",
    "    for i in range(1, nSpline) :\n",
    "        for j in range(score) :\n",
    "            vB[j] = vB[j] * xsB[j] + coefs_out[indexB[j]][i]\n",
    "    \n",
    "    A = np.zeros((nSpline + 1) * size)\n",
    "\n",
    "    stop = 0\n",
    "\n",
    "    for i in range(score) :\n",
    "        if (i / nSpline >= breaks[1]) : stop = 1\n",
    "\n",
    "        A[(i % nSpline) + stop + (i // nSpline) * (nSpline + 1)] = vB[i]\n",
    "    \n",
    "    # coeffs of B-splines to combine by optimised weighting in x\n",
    "    C = np.zeros((pieces + nSpline - 1, nSpline * pieces))\n",
    "\n",
    "    n2 : int = nSpline << 1\n",
    "    \n",
    "    for i in range((nSpline ** 2) * pieces) :\n",
    "        j = i // nSpline\n",
    "    \n",
    "        C[(i % nSpline) + (j % 2)][j] = coefs_out[i % n2][i // n2]\n",
    "    \n",
    "    x = np.linalg.solve(A, y) # lsqsolve_sub(nSpline + 1, A, y)\n",
    "    \n",
    "    # final coefficients\n",
    "    coefsSpline = np.zeros((pieces, nSpline))\n",
    "    \n",
    "    # multiply with x\n",
    "    for j in range(nSpline * pieces) :\n",
    "        for i in range(nSpline + 1) :\n",
    "            coefsSpline[j % pieces][j // pieces] += C[i][j] * x[i]\n",
    "    \n",
    "    # compute piecewise polynomial\n",
    "    \n",
    "    yOut = [coefsSpline[boolInt(i < breaks[1])][0] for i in range(size)]\n",
    "\n",
    "    for i in range(1, nSpline) :\n",
    "        for j in range(size) :\n",
    "            p = boolInt(j < breaks[1])\n",
    "            yOut[j] *= (j - breaks[1] * p) + coefsSpline[p][i]\n",
    "    \n",
    "    return yOut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CO_Embed2_Dist_tau_d_expfit_meandiff(y : Vector) -> float :\n",
    "    size : int = len(y)\n",
    "    tau  : int = co_firstzero(y, size)\n",
    "    \n",
    "    if (tau > size / 10) : tau = int(size / 10)\n",
    "\n",
    "    n : int = size - tau\n",
    "    \n",
    "    d = np.zeros(n)\n",
    "\n",
    "    for i in range(n - 1) :\n",
    "        d[i] = math.sqrt((y[i + 1] - y[i]) ** 2 + (y[i + tau] - y[i + tau + 1]) ** 2)\n",
    "        \n",
    "        if (math.isnan(d[i])) : return math.nan\n",
    "\n",
    "    n_bins : int = num_bins_auto(d)\n",
    "\n",
    "    if (n_bins == 0) : return 0\n",
    "    \n",
    "    hist_counts, bin_edges = histcounts_preallocated(d, n_bins)\n",
    "    \n",
    "    # normalise to probability\n",
    "    histCountsNorm = hist_counts / (n - 1)\n",
    "    \n",
    "    # mean for exponential fit\n",
    "    mn = np.mean(d)\n",
    "     \n",
    "    d_exp_fit_diff = np.zeros(n_bins)\n",
    "\n",
    "    for i in range(n_bins) :\n",
    "        expf = math.exp((bin_edges[i] + bin_edges[i + 1]) * -0.5 / mn) / mn\n",
    "\n",
    "        if (expf < 0) : expf = 0\n",
    "        \n",
    "        d_exp_fit_diff[i] = abs(histCountsNorm[i] - expf)\n",
    "    \n",
    "    return np.mean(d_exp_fit_diff)\n",
    "\n",
    "def CO_f1ecac(y : LComplex) -> float :\n",
    "    # Compute autocorrelations\n",
    "    corrs  = co_autocorrs(y)\n",
    "    # Threshold to cross\n",
    "    thresh = 1.0 / math.e\n",
    "\n",
    "    for i in range(len(y)) :\n",
    "        if (corrs[i + 1] < thresh) :\n",
    "            return i + (thresh - corrs[i]) / (corrs[i + 1] - corrs[i])\n",
    "    \n",
    "    return len(y) - 1\n",
    "\n",
    "def CO_FirstMin_ac(y : LComplex) -> int :\n",
    "    size : int = len(y)\n",
    "    corrs = co_autocorrs(y)\n",
    "\n",
    "    for i in range(1, size - 1) :\n",
    "        if ((corrs[i] < corrs[i - 1]) & (corrs[i] < corrs[i + 1])) :\n",
    "            return i\n",
    "    \n",
    "    return size - 1\n",
    "\n",
    "def CO_HistogramAMI_even_2_5(y : Vector, numBins : int = 5) -> float :\n",
    "    tau : int = 2\n",
    "    # set bin edges\n",
    "    minValue = min(y)\n",
    "    binStep  = (max(y) - minValue + .2) / numBins\n",
    "\n",
    "    \n",
    "    binEdges = minValue + binStep * np.array(range(numBins + 1)) - .1\n",
    "    binsExt : int = len(binEdges)\n",
    "\n",
    "    n_bins = range(numBins)\n",
    "    \n",
    "    #  count histogram bin contents\n",
    "    bins1 = histBinAssign(y[: -tau], binEdges)\n",
    "    bins2 = histBinAssign(y[tau :], binEdges)\n",
    "    # joint\n",
    "    bins12 = (np.array(bins1) - 1) * binsExt + bins2\n",
    "    # fancy solution for joint histogram here\n",
    "    jointHistLinear = np.histogram_bin_edges(bins12, range(1, binsExt ** 2 + 1), binsExt ** 2)\n",
    "    # histcount_edges(bins12, binEdges12);\n",
    "    \n",
    "    # transfer to 2D histogram (no last bin, as in original implementation)\n",
    "    pij = [[jointHistLinear[i * binsExt + j] for j in n_bins] for i in n_bins]\n",
    "    \n",
    "    sumBins = sum([sum(p) for p in pij])\n",
    "    # normalise\n",
    "    pij = [p / sumBins for p in pij]\n",
    "\n",
    "     # marginals\n",
    "    pi = np.zeros(numBins)\n",
    "    pj = np.zeros(numBins)\n",
    "\n",
    "    for i in n_bins :\n",
    "        for j in n_bins :\n",
    "            pi[i] += pij[i][j]\n",
    "            pj[j] += pij[i][j]\n",
    "\n",
    "    # mutual information\n",
    "    ami : float = 0\n",
    "\n",
    "    for i in n_bins :\n",
    "        for j in n_bins :\n",
    "            if(pij[i][j] > 0) :\n",
    "                ami += pij[i][j] * math.log(pij[i][j] / (pj[j] * pi[i]))\n",
    "    \n",
    "    return ami\n",
    "\n",
    "def CO_trev_1_num(y : Vector) -> float :\n",
    "    return np.mean(adjacent(y) ** 3)\n",
    "\n",
    "def DN_HistogramMode(y : Vector, nBins : int = 10) -> float :\n",
    "    numMaxs : int = 1\n",
    "    maxCount = 0\n",
    "    out = 0\n",
    "    \n",
    "    histCounts, binEdges = histcounts_preallocated(y, nBins)\n",
    "    \n",
    "    for i in range(nBins) :\n",
    "        if (histCounts[i] > maxCount) :\n",
    "            maxCount = histCounts[i]\n",
    "            numMaxs  = 1\n",
    "            out      = (binEdges[i] + binEdges[i + 1])\n",
    "        elif (histCounts[i] == maxCount) :\n",
    "            numMaxs += 1\n",
    "            out     += (binEdges[i] + binEdges[i + 1])\n",
    "        \n",
    "    return out / (numMaxs << 1)\n",
    "\n",
    "def DN_HistogramMode_10(y : Vector) -> float :\n",
    "    return DN_HistogramMode(y, 10)\n",
    "\n",
    "def DN_HistogramMode_5(y : Vector) -> float :\n",
    "    return DN_HistogramMode(y, 10)\n",
    "\n",
    "def DN_Mean(a : Vector) -> float :\n",
    "    return a.mean()\n",
    "\n",
    "def DN_OutlierInclude_np_001_mdrmd(y : Vector, sign : int) -> float :\n",
    "    size : int   = len(y)\n",
    "    tot  : int   = 0\n",
    "    inc  : float = 1e-2 # 0.01\n",
    "    yWork = []\n",
    "\n",
    "    constant_flag : bool = True\n",
    "\n",
    "    # apply sign and check constant time series\n",
    "    for x in y :\n",
    "        if (x != y[0]) : constant_flag = False\n",
    "\n",
    "        # apply sign, save in new variable\n",
    "        yWork.append(sign * x)\n",
    "        \n",
    "        # count pos / negs\n",
    "        if (yWork[-1] >= 0) : tot += 1\n",
    "\n",
    "    if (constant_flag) : return 0\n",
    "    \n",
    "    # find maximum (or minimum, depending on sign)\n",
    "    max_val = max(yWork)\n",
    "    \n",
    "    #  maximum value too small ? return 0\n",
    "    if (max_val < inc) : return 0\n",
    "    \n",
    "    # save the indices where y > threshold\n",
    "    r = []\n",
    "    #  save the median over indices with absolute value > threshold\n",
    "    msDti1 = []\n",
    "    msDti3 = []\n",
    "    msDti4 = []\n",
    "    \n",
    "    k = 100 / tot\n",
    "    s = 2 / size\n",
    "    \n",
    "    nThresh = int(max_val / inc) + 1\n",
    "    \n",
    "    for j in range(nThresh) :\n",
    "        r += [i + 1 for i in range(size) if (yWork[i] >= j * inc)]\n",
    "        \n",
    "        #  intervals between high-values\n",
    "        tmp = adjacent(r)\n",
    "        high_size = len(tmp)\n",
    "        \n",
    "        msDti1.append(np.mean(tmp[: high_size]))\n",
    "        msDti3.append(k * high_size)\n",
    "        msDti4.append(np.median(r) * s - 1)\n",
    "    \n",
    "    mj      : int = 0\n",
    "    trimthr : int = 2\n",
    "    fbi     : int = nThresh - 1\n",
    "\n",
    "    for i in range(nThresh) :\n",
    "        if (msDti3[i] > trimthr) : mj = i\n",
    "\n",
    "        k = nThresh - i - 1\n",
    "\n",
    "        if (math.isnan(msDti1[k])) : fbi = k\n",
    "    \n",
    "    return np.median(msDti4[: (mj if mj < fbi else fbi) + 1])\n",
    "\n",
    "def DN_OutlierInclude_p_001_mdrmd(y : Vector) -> float :\n",
    "    return DN_OutlierInclude_np_001_mdrmd(y, 1.0)\n",
    "\n",
    "def DN_OutlierInclude_n_001_mdrmd(y : Vector) -> float :\n",
    "    return DN_OutlierInclude_np_001_mdrmd(y, -1.0)\n",
    "\n",
    "def DN_Spread_Std(a : Vector) -> float :\n",
    "    return np.std(a)\n",
    "\n",
    "def FC_LocalSimple_mean_tauresrat(y : Vector, train_length : int) -> float :\n",
    "    size : int = len(y)\n",
    "    \n",
    "    res       = fc_local_simple_mean(y, train_length)\n",
    "    resAC1stZ = co_firstzero(res, size - train_length)\n",
    "    yAC1stZ   = co_firstzero(y, size)\n",
    "    \n",
    "    return resAC1stZ / yAC1stZ\n",
    "\n",
    "def FC_LocalSimple_mean_stderr(y : Vector, train_length : int) -> float :\n",
    "    return np.std(fc_local_simple_mean(y, train_length))\n",
    "\n",
    "def FC_LocalSimple_mean3_stderr(y : Vector) -> float :\n",
    "    return FC_LocalSimple_mean_stderr(y, 3)\n",
    "\n",
    "def FC_LocalSimple_mean1_tauresrat(y : Vector) -> float :\n",
    "    return FC_LocalSimple_mean_tauresrat(y, 1)\n",
    "\n",
    "def IN_AutoMutualInfoStats_40_gaussian_fmmi(y : Vector, tau : int = 40) -> float :\n",
    "    size : int = len(y)\n",
    "    # don't go above half the signal length\n",
    "    tau = min(tau, math.ceil(size / 2))\n",
    "    # compute autocorrelations and compute automutual information\n",
    "    ami = [math.log(1 - autocorr_lag(y, i + 1) ** 2) * -.5 for i in range(tau)]\n",
    "\n",
    "    # find first minimum of automutual information\n",
    "    for i in range(1, tau - 1) :\n",
    "        if((ami[i] < ami[i - 1]) & (ami[i] < ami[i + 1])) : return i\n",
    "\n",
    "    return tau\n",
    "\n",
    "def MD_hrv_classic_pnn40(y : Vector, pNNx : int = 40) -> float :\n",
    "    return len([0 for x in adjacent(y) if abs(x * 1000) > pNNx]) / len(y)\n",
    "\n",
    "@deprecated(\"Problème dans l'implémentation de 'splinefit'\")\n",
    "def PD_PeriodicityWang_th0_01(y : Vector, th : float = 1e-2) -> int :\n",
    "    size  : int = len(y)\n",
    "    # compute autocorrelations up to 1/3 of the length of the time series\n",
    "    acmax : int = int(math.ceil(size / 3.0))\n",
    "    \n",
    "    # fit a spline with 3 nodes to the data\n",
    "    # subtract spline from data to remove trend\n",
    "    ySub = np.array(y) - splinefit(y)\n",
    "    # correlation/ covariance the same, don't care for scaling\n",
    "    # (cov would be more efficient)\n",
    "    acf = [cov_mean(ySub[: -tau], ySub[tau :]) for tau in range(1, acmax)]\n",
    "\n",
    "    # find troughts and peaks\n",
    "    troughs = []\n",
    "    peaks   = []\n",
    "\n",
    "    for i in range(1, acmax - 1) :\n",
    "        slopeIn  = acf[i] - acf[i - 1]\n",
    "        slopeOut = acf[i + 1] - acf[i]\n",
    "        \n",
    "        if   ((slopeIn < 0) & (slopeOut > 0)) : troughs.append(i)\n",
    "        elif ((slopeOut < 0) & (slopeIn > 0)) : peaks.append(i)\n",
    "    \n",
    "    # search through all peaks for one that meets the conditions:\n",
    "    # (a) a trough before it\n",
    "    # (b) difference between peak and trough is at least 0.01\n",
    "    # (c) peak corresponds to positive correlation\n",
    "    nTroughs : int = len(troughs)\n",
    "    \n",
    "    for i in range(len(peaks)) :\n",
    "        ip : int = peaks[i]\n",
    "        # find trough before this peak\n",
    "        j  : int = -1\n",
    "\n",
    "        while ((troughs[j + 1] < ip) & (j + 1 < nTroughs)) : j += 1\n",
    "\n",
    "        # (a) should be implicit\n",
    "        if (j == -1) : continue\n",
    "        # (b) different between peak and trough it as least 0.01\n",
    "        if (acf[ip] - acf[troughs[j]] < th) : continue\n",
    "        # (c) peak corresponds to positive correlation\n",
    "        if (acf[ip] < 0) : continue\n",
    "        \n",
    "        # use this frequency that first fulfils all conditions.\n",
    "        return ip\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def SB_BinaryStats_diff_longstretch0(y : Vector) -> int :\n",
    "    return sb_binary_stats_diff_longstretch(adjacent(y) < 0)\n",
    "\n",
    "def SB_BinaryStats_mean_longstretch1(y : Vector) -> int :\n",
    "    return sb_binary_stats_diff_longstretch(deviation(y) <= 0)\n",
    "\n",
    "def SB_MotifThree_quantile_hh(y : Vector, alphabet_size = 3) -> float :\n",
    "    size : int = len(y)\n",
    "    inc  : int = size - 1\n",
    "    # transfer to alphabet\n",
    "    yt   = sb_coarsegrain(y, \"quantile\", 3)\n",
    "    abcd = range(alphabet_size) \n",
    "\n",
    "    # using selfresizing array for memory efficiency. Time complexity\n",
    "    # should be comparable due to ammotization.\n",
    "    Rh = [[j for j in range(size) if (yt[j] == i + 1)] for i in abcd]\n",
    "    \n",
    "    # removing last item if it is == max possible idx since later\n",
    "    # we are taking idx + 1 from yt\n",
    "    for i in abcd :\n",
    "        l = len(Rh[i])\n",
    "\n",
    "        if ((l != 0) & (Rh[i][l - 1] == inc)) : Rh[i] = Rh[i][: l - 1]\n",
    "\n",
    "    # allocate separately\n",
    "    out2 = [[len([k for k in Rh[i] if yt[k + 1] == (j + 1)]) / inc for j in abcd]\n",
    "           for i in abcd]\n",
    "\n",
    "    return sum([f_entropy(x) for x in out2])\n",
    "\n",
    "def SB_TransitionMatrix_3ac_sumdiagcov(y : Vector, num_groups : int = 3) -> float :\n",
    "    size  : int = len(y)\n",
    "    tau   : int = co_firstzero(y, size)\n",
    "    nDown : int = int((size - 1) / tau) + 1\n",
    "\n",
    "    grp   = range(num_groups)\n",
    "    yDown =  [y[i * tau] for i in range(nDown)]\n",
    "    # transfer to alphabet\n",
    "    yCG = sb_coarsegrain(yDown, \"quantile\", num_groups)\n",
    "    T   = np.zeros((num_groups, num_groups))\n",
    "    \n",
    "    # more efficient way of doing the below \n",
    "    for j in range(nDown - 1) : T[yCG[j] - 1][yCG[j + 1] - 1] += 1\n",
    "    \n",
    "    cov_tmp  = 0\n",
    "    dev_cols = [deviation([T[i][j] / (nDown - 1) for i in grp]) for j in grp]\n",
    "\n",
    "    for i in grp :\n",
    "        for j in range(i, i + 1) : cov_tmp += np.dot(dev_cols[i], dev_cols[j])\n",
    "\n",
    "    \"\"\"\n",
    "    COV = np.zeros((num_groups, num_groups))\n",
    "    \n",
    "    for i in range(num_groups) :\n",
    "        for j in range(i, num_groups) :\n",
    "            covTemp = np.dot(dev_cols[i], dev_cols[j]) / (num_groups - 1)\n",
    "            \n",
    "            COV[i][j] = covTemp\n",
    "            COV[j][i] = covTemp\n",
    "\n",
    "    print([COV[i][i] for i in range(num_groups)])\n",
    "    \n",
    "    return sum([COV[i][i] for i in range(num_groups)])\n",
    "    \"\"\"\n",
    "\n",
    "    return cov_tmp / (num_groups - 1)\n",
    "\n",
    "def SC_FluctAnal_2_50_1_logi_prop_r1(y : Vector, lag : int, how : str) -> float :\n",
    "    size : int = len(y)\n",
    "    \n",
    "    # generate log spaced tau vector\n",
    "    lin_low  = math.log(5)\n",
    "    lin_high = math.log(size / 2)\n",
    "    \n",
    "    nTauSteps : int = 50\n",
    "    tauStep = (lin_high - lin_low) / (nTauSteps - 1)\n",
    "\n",
    "    tau = [int(round(math.exp(lin_low + i * tauStep))) for i in range(nTauSteps)]\n",
    "    \n",
    "    # check for uniqueness, use ascending order\n",
    "    nTau : int = nTauSteps\n",
    "\n",
    "    for i in range(nTauSteps - 1) :\n",
    "        while ((tau[i] == tau[i + 1]) & (i < nTau - 1)) :\n",
    "            tau[i + 1 : nTauSteps - 1] = tau[i + 2 : nTauSteps]\n",
    "            # for j in range(i + 1, nTauSteps - 1) : tau[j] = tau[j + 1]\n",
    "\n",
    "            # lost one\n",
    "            nTau -= 1\n",
    "    \n",
    "    # fewer than 12 points -> leave.\n",
    "    if(nTau < 12) : return 0\n",
    "    \n",
    "    sizeCS : int = int(size / lag)\n",
    "    yCS = [0 for _ in range(sizeCS)] # malloc(sizeCS * sizeof(double))\n",
    "\n",
    "    # transform input vector to cumsum\n",
    "    yCS[0] = y[0]\n",
    "\n",
    "    for i in range(sizeCS - 1) : yCS[i + 1] = yCS[i] + y[(i + 1) * lag]\n",
    "    \n",
    "    # first generate a support for regression (detrending)\n",
    "    xReg = range(1, tau[nTau - 1] + 1)\n",
    "    \n",
    "    # iterate over taus, cut signal, detrend and save amplitude of remaining signal\n",
    "    F = np.zeros(nTau)\n",
    "\n",
    "    for i in range(nTau) :\n",
    "        nBuffer : int = int(sizeCS / tau[i])\n",
    "\n",
    "        for j in range(nBuffer) :\n",
    "            p = j * tau[i]\n",
    "\n",
    "            _, m, b = linreg(xReg, yCS[p :], tau[i])\n",
    "            \n",
    "            buffer = [yCS[p + k] - (m * (k + 1) + b) for k in range(tau[i])]\n",
    "            \n",
    "            if (how == \"rsrangefit\") :\n",
    "                F[i] = (max(max(buffer), tau[i]) - min(min(buffer), tau[i])) ** 2\n",
    "            elif (how == \"dfa\") :\n",
    "                F[i] = sum(np.array(buffer[: tau[i]]) ** 2)\n",
    "            else :\n",
    "                return 0.0\n",
    "        \n",
    "        if   (how == \"rsrangefit\") :\n",
    "            F[i] = math.sqrt(F[i] / nBuffer)\n",
    "        elif (how == \"dfa\") :\n",
    "            F[i] = math.sqrt(F[i] / (nBuffer * tau[i]))\n",
    "\n",
    "    logtt = np.log(tau[: nTau])\n",
    "    logFF = np.log(F[: nTau])\n",
    "\n",
    "    minPoints : int = 6\n",
    "\n",
    "    sserr = np.zeros(nTau - 2 * minPoints + 1)\n",
    "\n",
    "    for i in range(minPoints, nTau - minPoints + 1) :\n",
    "        # this could be done with less variables of course    \n",
    "        it = i - 1\n",
    "        \n",
    "        _ , m1, b1 = linreg(logtt, logFF, i)\n",
    "        _ , m2, b2 = linreg(logtt[it :], logFF[it :], nTau - it)\n",
    "        \n",
    "        # buffer = [logtt[j] * m1 + b1 - logFF[j] for j in range(i)]\n",
    "        buffer = logtt[: i] * m1 + b1 - logFF[: i]\n",
    "        \n",
    "        sserr[i - minPoints] += np.linalg.norm(buffer[: i])\n",
    "\n",
    "        buffer = [logtt[j + it] * m2 + b2 - logFF[j + it] for j in range(nTau - it)]\n",
    "        \n",
    "        sserr[i - minPoints] += np.linalg.norm(buffer[: nTau - it])\n",
    "    \n",
    "    minimum = np.min(sserr)\n",
    "\n",
    "    return ((np.where(sserr == minimum)[0] + 1) / nTau)[0]\n",
    "\n",
    "def SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1(y : Vector) -> int :\n",
    "    return SC_FluctAnal_2_50_1_logi_prop_r1(y, 2, \"dfa\")\n",
    "\n",
    "def SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1(y : Vector) -> int :\n",
    "    return SC_FluctAnal_2_50_1_logi_prop_r1(y, 1, \"rsrangefit\")\n",
    "\n",
    "def SP_Summaries_welch_rect(y : Vector, what : str) -> float :\n",
    "    size : int = len(y)\n",
    "    nfft : int = pow2(size)\n",
    "    Fs = 1.0    # sampling frequency\n",
    "    \n",
    "    # rectangular window for Welch-spectrum / compute Welch-power\n",
    "    nWelch, S, f = welch(y, nfft, Fs, np.ones(size))\n",
    "    \n",
    "    # angualr frequency and spectrum on that\n",
    "    # w  = f * 2 * math.pi\n",
    "    Sw = S / (2 * math.pi)\n",
    "    \n",
    "    for s in Sw :\n",
    "        if (math.isinf(s)) : return 0\n",
    "    \n",
    "    csS = np.cumsum(Sw)\n",
    "    \n",
    "    if (what == \"centroid\") :        \n",
    "        csSThres = csS[nWelch - 1] * 0.5\n",
    "\n",
    "        for i in range(nWelch) :\n",
    "            if(csS[i] > csSThres) : return f[i] * 2 * math.pi\n",
    "    elif (what == \"area_5_1\") :\n",
    "        return (sum(Sw[: int(nWelch // 5)]) * (f[1] - f[0]) * 2 * math.pi).real\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def SP_Summaries_welch_rect_area_5_1(y : Vector) -> float :\n",
    "    return SP_Summaries_welch_rect(y, \"area_5_1\")\n",
    "    \n",
    "def SP_Summaries_welch_rect_centroid(y : Vector) -> float :\n",
    "    return SP_Summaries_welch_rect(y, \"centroid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch22 = [CO_Embed2_Dist_tau_d_expfit_meandiff,\n",
    "          CO_f1ecac,\n",
    "          CO_FirstMin_ac,\n",
    "          CO_HistogramAMI_even_2_5,\n",
    "          CO_trev_1_num,\n",
    "          DN_HistogramMode_10,\n",
    "          DN_HistogramMode_5,\n",
    "          DN_Mean,\n",
    "          DN_OutlierInclude_p_001_mdrmd,\n",
    "          DN_OutlierInclude_n_001_mdrmd,\n",
    "          DN_Spread_Std,\n",
    "          FC_LocalSimple_mean3_stderr,\n",
    "          FC_LocalSimple_mean1_tauresrat,\n",
    "          IN_AutoMutualInfoStats_40_gaussian_fmmi,\n",
    "          MD_hrv_classic_pnn40,\n",
    "        #   PD_PeriodicityWang_th0_01,\n",
    "          SB_BinaryStats_diff_longstretch0,\n",
    "          SB_BinaryStats_mean_longstretch1,\n",
    "          SB_MotifThree_quantile_hh,\n",
    "          SB_TransitionMatrix_3ac_sumdiagcov,\n",
    "          SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1,\n",
    "          SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1,\n",
    "          SP_Summaries_welch_rect_area_5_1,\n",
    "          SP_Summaries_welch_rect_centroid,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\b.luron\\AppData\\Local\\Temp\\ipykernel_34856\\1059410348.py:467: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  if (math.isinf(s)) : return 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06835508033172982,\n",
       " 0.5605481639257237,\n",
       " 1,\n",
       " 0.0038449298910453204,\n",
       " -0.07952272775049883,\n",
       " -0.9999237048905165,\n",
       " -0.9999237048905165,\n",
       " -1.1932555123216602,\n",
       " 0.6120000000000001,\n",
       " -0.17599999999999993,\n",
       " 3.476309819282354,\n",
       " 2.4230646499380937,\n",
       " 1.0,\n",
       " 5,\n",
       " 0.982,\n",
       " 9,\n",
       " 43,\n",
       " 1.7882225782083592,\n",
       " 0.0414175043473721,\n",
       " 0.717391304347826,\n",
       " 0.043478260869565216,\n",
       " 0.009029671206882637,\n",
       " 1.5830681730979816]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(np.histogram_bin_edges(trains['C3'][0], 5) - .1, min(trains['C3'][0]) - .1, max(trains['C3'][0]) + .1)\n",
    " #, trains['C3'][0] #, \n",
    "\n",
    "# MD_hrv_classic_pnn40(trains['C3'][0], 1525)\n",
    "\n",
    "# SB_MotifThree_quantile_hh(trains['C3'][0])\n",
    "\n",
    "# DN_OutlierInclude_np_001_mdrmd(trains['C4'][0], 1), rsrangefit\n",
    "# SC_FluctAnal_2_50_1_logi_prop_r1(trains['C4'][5], 2, \"dfa\")\n",
    "# SP_Summaries_welch_rect(trains['C4'][5], \"area_5_1\")\n",
    "# stddev(trains['C3'][0], trains['C4'][3])\n",
    "\n",
    "# CO_Embed2_Dist_tau_d_expfit_meandiff(trains['C3'][57])\n",
    "\n",
    "# np.complex128(deviation(trains['C3'][107]))\n",
    "\n",
    "[foo(trains['C4'][107]) for foo in catch22]\n",
    "\n",
    "# co_autocorrs(trains['C3'][107])\n",
    "\n",
    "# splinefit(trains['C3'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Pré-traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Pré-traitement des données\n",
    "\n",
    "def data_spliting(datas : list[Board], Channels : Clause, number_event : int | Index,\n",
    "                  labels : list[Board] | None = None, merge : bool = False, level : bool = True) :\n",
    "    count = range(len(datas))\n",
    "    parts = []        #\n",
    "    temp  = [[], []]  # Les époques pour tous les cannaux et tous les évènements.\n",
    "    spots = [[], []]  # Apparitions des évènements\n",
    "    # Pour la standardisation du nombre d'échantillon max conservé\n",
    "    ceil  = min([len(labels[i]['EventType']) for i in count])\n",
    "\n",
    "    if type(number_event) == int : number_event = range(number_event)\n",
    "\n",
    "    # Extraction des données relavitives à l'apparition des évènements.\n",
    "    if level :\n",
    "        for i in count :\n",
    "            df   = datas[i]\n",
    "            kind = labels[i]['EventType'][: ceil]\n",
    "            loc  = np.where(df['EventStart'] == 1)[: ceil]\n",
    "\n",
    "            parts.append(zero_removal(df['C3'], 75))\n",
    "\n",
    "            for j in number_event :\n",
    "                spots[j].append(np.array(*loc)[*np.where(kind == j)])\n",
    "\n",
    "                # [tf.convert_to_tensor(X) for X in event_epochs(spots[j][-1], SCOPE, LAG)]\n",
    "                room = event_epochs(spots[j][-1], SCOPE, LAG)\n",
    "\n",
    "                temp[j].append([full_event(df[c], room, merge) for c in Channels])\n",
    "    else :\n",
    "        for i in count :\n",
    "            df   = datas[i]\n",
    "            kind = label_csv[i]['EventType']\n",
    "            loc  = np.where(df['EventStart'] == 1)\n",
    "\n",
    "            parts.append(zero_removal(df['C3'], 75))\n",
    "\n",
    "            for j in number_event :\n",
    "                spots[j].append(np.array(*loc)[*np.where(kind == j)])\n",
    "\n",
    "                room = event_epochs(spots[j][-1], SCOPE, LAG)\n",
    "\n",
    "                temp[j].append([full_event(df[c], room, merge) for c in Channels])\n",
    "\n",
    "    # Regroupement des données en fonction du type de l'évènement et du cannal d'observation\n",
    "    if merge :\n",
    "        temp = [[[np.append([], T[j :: 3]) for T in temp[i]] for j in range(3)] for i in number_event]\n",
    "    else :\n",
    "        store = [[[], [], []], [[], [], []]]\n",
    "        \n",
    "        [[[[store[i][j].append(G) for G in X] for j, X in enumerate(T)] for T in temp[i]] for i in number_event]\n",
    "\n",
    "        temp = store\n",
    "\n",
    "    eras = [pd.DataFrame({**dict(zip(Channels, [pd.Series(X) for X in temp[i]])), 'EventType': i}) for i in number_event]\n",
    "\n",
    "    return eras, spots, parts\n",
    "\n",
    "# %time\n",
    "train_eras, train_spots, train_parts = data_spliting(train_csv, eeg_Chans, num_events, label_csv)\n",
    "\n",
    "# train_samples = samples(len(train_eras[0]))\n",
    "trains        = pd.concat(train_eras, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SB_BinaryStats_mean_longstretch1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# DN_OutlierInclude_np_001_mdrmd(trains['C3'][0], 1)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mSB_BinaryStats_mean_longstretch1\u001b[49m(trains[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC3\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# np.array([1, 2, 3, 4]) + [5, 6, 7, 8]\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SB_BinaryStats_mean_longstretch1' is not defined"
     ]
    }
   ],
   "source": [
    "# DN_OutlierInclude_np_001_mdrmd(trains['C3'][0], 1)\n",
    "SB_BinaryStats_mean_longstretch1(trains['C3'][1])\n",
    "# np.array([1, 2, 3, 4]) + [5, 6, 7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catch(data : Board, col : str, channels : Clause, event : int,\n",
    "            norm : bool = False) -> Board :\n",
    "    func = [np.min, np.max, np.ptp, np.std, np.var, np.mean, np.median, np.average]\n",
    "    tmp  = data[data[col] == event]\n",
    "    name = [f.__name__ for f in func]\n",
    "    head = np.append([f\"{c}_{f}\" for c in channels for f in name],\n",
    "                     [f\"{f}_diff\" for f in name])\n",
    "\n",
    "    if norm :\n",
    "        tmp = [[normalized(x) for x in tmp[c]] for c in channels]\n",
    "\n",
    "        _c3, _c4, _cz = [[[f(v) for v in s] for f in func] for s in tmp]\n",
    "    else :\n",
    "        _c3, _c4, _cz = [[[f(v) for v in tmp[c]] for f in func] for c in channels]\n",
    "    \n",
    "    _ff = [np.array(v1) - v0 for v0, v1 in zip(_c3, _c4)]\n",
    "    _df = pd.DataFrame(np.array((*_c3, *_c4, *_cz, *_ff)).T, columns = head)\n",
    "    # _df = pd.DataFrame(np.array((*_cz, *_ff, )).T)\n",
    "    \n",
    "    _df[col] = event\n",
    "\n",
    "    return _df\n",
    "\n",
    "norm : bool = False\n",
    "\n",
    "df_left  = catch(trains, 'EventType', eeg_Chans, 0, norm = norm)\n",
    "df_right = catch(trains, 'EventType', eeg_Chans, 1, norm = norm)\n",
    "\n",
    "X = pd.concat((df_left, df_right), ignore_index = True)\n",
    "y = X['EventType']\n",
    "\n",
    "display(X)\n",
    "\n",
    "X.drop(columns = ['EventType'], inplace = True)\n",
    "\n",
    "# tmp = [[normalized(x) for x in trains[c]] for c in eeg_Chans]\n",
    "\n",
    "# display(np.shape(tmp))\n",
    "\n",
    "# tmp = pd.DataFrame((tmp), index = eeg_Chans).T\n",
    "\n",
    "# func    = [np.min, np.max, np.ptp, np.std, np.var, np.mean, np.median, np.average], columns = eeg_Chans\n",
    "\n",
    "# tmp     = trains[trains['EventType'] == 0]\n",
    "# left_c3 = [[f(v) for v in tmp['C3']] for f in func]\n",
    "# left_c4 = [[f(v) for v in tmp['C4']] for f in func]\n",
    "# left_cz = [[f(v) for v in tmp['Cz']] for f in func]\n",
    "# left_ff = [np.array(v1) - v0 for v0, v1 in zip(left_c3, left_c4)]\n",
    "# left_cr = np.correlate(tmp['C3'].values, tmp['C4'].values)\n",
    "\n",
    "# tmp      = trains[trains['EventType'] == 1]\n",
    "# right_c3 = [[f(v) for v in tmp['C3']] for f in func]\n",
    "# right_c4 = [[f(v) for v in tmp['C4']] for f in func]\n",
    "# right_cz = [[f(v) for v in tmp['Cz']] for f in func]\n",
    "# right_ff = [np.array(v1) - v0 for v0, v1 in zip(left_c3, left_c4)]\n",
    "# right_cr = np.correlate(tmp['C3'].values, tmp['C4'].values)\n",
    "\n",
    "# df_left  = pd.DataFrame(np.array((*left_c3, *left_c4, *left_ff, *left_cz)).T)\n",
    "# df_right = pd.DataFrame(np.array((*right_c3, *right_c4, *right_ff, *right_cz)).T)\n",
    "\n",
    "# df_left['EventType']  = 0\n",
    "# df_right['EventType'] = 1\n",
    "\n",
    "# len(left_cr[0]), np.shape(left_c3), np.shape(tmp['C3'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation des spectrogrammes / Test\n",
    "\n",
    "def logMelSpectrogram(data : Vector, rate : int, dt : float = 1e-2) -> Vector :\n",
    "    tps = 1 << int(np.floor(np.log2(rate * dt)))\n",
    "    # print(tps)\n",
    "    # Spectrogramme\n",
    "    stfts = np.abs(librosa.stft(y = data, n_fft = tps, hop_length = 1 << 2, center = True)).T\n",
    "    # Filtre de MEL\n",
    "    liny  = librosa.filters.mel(sr = rate, n_fft = tps + 1, n_mels = stfts.shape[-1]).T\n",
    "    # Application du filtre au spectrogramme\n",
    "    mel_  = np.tensordot(stfts, liny, 1)\n",
    "\n",
    "    return np.log(mel_ + 1e-6)\n",
    "    \n",
    "def structure(data : Board | Vector, rate : int, whr : Clause) -> Vector :\n",
    "    # return np.array([logMelSpectrogram(X, rate, 2) for X in data[whr]])\n",
    "    # return np.stack([[signal.welch(X, rate)[1] for X in data[c]] for c in whr], axis = 2)\n",
    "    return np.stack(trains['C4'] - trains['C3'], axis = 1)\n",
    "    # return np.stack([[X for X in data[c]] for c in whr], axis = 2)\n",
    "\n",
    "def img_spectrogram(raw : Vector, rate : int, nfft : int = 1 << 10) -> Vector :\n",
    "    return librosa.feature.melspectrogram(y = raw, sr = rate, hop_length = 1, \n",
    "                            n_fft = nfft, n_mels = 32, fmin = 0, fmax = 20, win_length = 32)\n",
    "\n",
    "def spectrogram_dep(data : Board, rate : int, channels : Clause, n_row : int = 5, n_col : int = 12) :\n",
    "    sample = np.random.default_rng().integers(data.shape[0], size = n_row)\n",
    "\n",
    "    sample.sort()\n",
    "\n",
    "    plt.figure(figsize = (18, 2 * .48 * n_row))\n",
    "\n",
    "    pos = 0\n",
    "\n",
    "    for k in sample :\n",
    "        for c in channels :\n",
    "            # x   = normalized(data[c][k])\n",
    "            raw = img_spectrogram(raw = data[c][k], rate = rate)\n",
    "            pos += 1\n",
    "            \n",
    "            plt.subplot(n_row, n_col, pos)\n",
    "            plt.title(f\"{((pos - 1) // 3) + 1} . {k} - {c}\", fontsize = 8)\n",
    "            librosa.display.specshow(data = 1 - raw, sr = rate, hop_length = 1)\n",
    "            \n",
    "            # pos = n_col * (i >> 1) + j\n",
    "            # f, t, Sxx = signal.spectrogram(x, rate)\n",
    "            # plt.subplot(n_row, n_col, pos + 4)\n",
    "            # plt.pcolormesh(t, f, 1 - Sxx, shading = 'gouraud')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show();\n",
    "\n",
    "def spectrogram(data : Board, rate : int, channels : Clause, n_row : int = 5, n_col : int = 12) :\n",
    "    sample = np.random.default_rng().integers(data.shape[0], size = n_row)\n",
    "\n",
    "    sample.sort()\n",
    "\n",
    "    plt.figure(figsize = (18, 2 * .48 * n_row))\n",
    "\n",
    "    freq = np.arange(1, rate >> 1)\n",
    "    pos  = 0\n",
    "    extd = np.append([0, 1, 1], freq[-1])\n",
    "    \n",
    "    for k in sample :\n",
    "        for c in channels :\n",
    "            pos += 1\n",
    "            x   = normalized(data[c][k])\n",
    "            coefficients, frequencies = pywt.cwt(x, scales = freq, wavelet = 'cmor')\n",
    "\n",
    "            plt.subplot(n_row, n_col, pos)\n",
    "            plt.imshow(np.abs(coefficients), aspect = 'auto', cmap = 'jet') #, extent = extd\n",
    "            # plt.colorbar(label=\"Magnitude\")\n",
    "            # plt.ylabel(\"Scale\")\n",
    "            # plt.xlabel(\"Time\")\n",
    "            # plt.title(\"CWT of a Chirp Signal\")\n",
    "            plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_logMelSpectrogram(data, rate) :\n",
    "    sns.heatmap(np.rot90(logMelSpectrogram(data, rate)), cmap = 'inferno')\n",
    "    \n",
    "    # loc, _ = plt.xticks()\n",
    "    # l      = np.round((loc - loc.min()) * len(data) / fe / loc.max(), 2), vmin = -6\n",
    "\n",
    "    # plt.xticks(loc, l)\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Frequency (Mel)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients, frequencies = pywt.cwt(train_eras[0]['C3'][0], scales = np.arange(1, SAMPLE_RATE >> 1), wavelet = 'cmor')\n",
    "\n",
    "1 / frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = [list(harmonic(x, bands_coeff).values()) for x in [trains[c] for c in eeg_Chans]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(H), np.shape(np.stack(np.stack(H, axis = 1), axis = 2)), np.shape(H[0][1][0])\n",
    "# harmonic(trains['C3'][256], bands_coeff).values())), H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(trains.drop(columns = ['EventType']),\n",
    "                                                    trains['EventType'], test_size = .2, random_state = 42)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split([v.tolist() for v in np.array(trains[eeg_Channels])],\n",
    "#                                                     trains['EventType'], test_size = .2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = np.array(X_train)\n",
    "# test_dataset  = X_test\n",
    "\n",
    "# train_dataset = structure(X_train, SAMPLE_RATE, eeg_Chans[:2])\n",
    "# test_dataset  = structure(X_test, SAMPLE_RATE, eeg_Chans[:2])\n",
    "\n",
    "train_dataset = structure(X_train, SAMPLE_RATE, eeg_Chans[:2])\n",
    "test_dataset  = structure(X_test, SAMPLE_RATE, eeg_Chans[:2])\n",
    "\n",
    "# print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### • Test prédiction\n",
    "\n",
    "# UNITS     : int   = 100\n",
    "BATCHSIZE : int   = 30\n",
    "EPOCH     : int   = 1000\n",
    "ZERO      : int   = 64\n",
    "DROPOUT   : float = .2\n",
    "# kl_divergence mean_squared_logarithmic_error mean_absolute_error\n",
    "LOSS      : None  = 'sparse_categorical_crossentropy'\n",
    "ACTIV     : None  = LeakyReLU   # PReLU, \n",
    "OPTIMIZER : None  = 'AdamW'     # adamax, , adafactor, adam, nadam\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# - 1 -\n",
    "model.add(Conv1D(filters = ZERO, kernel_size = (5), dilation_rate = 2,\n",
    "                 input_shape = train_dataset.shape[1: ]))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(ACTIV())\n",
    "\n",
    "# - 2 -\n",
    "model.add(Conv1D(filters = ZERO << 1, kernel_size = (5), dilation_rate = 2))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(ACTIV())\n",
    "\n",
    "# - 3 -\n",
    "model.add(Conv1D(filters = ZERO << 2, kernel_size = (5), dilation_rate = 2))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(ACTIV())\n",
    "model.add(GlobalAveragePooling1D()) # \n",
    "\n",
    "# Classification\n",
    "model.add(Dense(ZERO << 2))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(ACTIV())\n",
    "model.add(Dense(len(hands_event), activation = 'softmax')) # sigmoid  \n",
    "\n",
    "'''\n",
    "# Ajout de la premiere couche lstm\n",
    "model.add(LSTM(ZERO, input_shape = train_dataset.shape[1:], activation = ACTIV(), return_sequences = True)) #\n",
    "model.add(LSTM(ZERO, dropout = DROPOUT, return_sequences = False))\n",
    "\n",
    "# Ajout de la couche de sortie\n",
    "model.add(Dense(len(hands_event), activation = 'softmax'))\n",
    "'''\n",
    "\n",
    "model.compile(optimizer = OPTIMIZER, loss = LOSS, metrics = ['accuracy'])\n",
    "# model.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop    = EarlyStopping(monitor = 'val_accuracy', mode = 'max', verbose = 1, patience = 50)\n",
    "history = model.fit(train_dataset, y_train, validation_data = (test_dataset, y_test), verbose = 1,\n",
    "                    batch_size = BATCHSIZE, epochs = EPOCH, callbacks = [stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_dataset)\n",
    "\n",
    "sum([np.where(x > .5)[0][0] for x in pred] == y_test) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values  = history_dict['loss']\n",
    "acc_values   = history_dict['accuracy']\n",
    "absc         = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.figure(figsize = (12, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(absc, loss_values, label = 'Loss')\n",
    "plt.plot(absc, acc_values, label = 'Accuracy')\n",
    "plt.title('Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(absc, history_dict['val_loss'], label = 'Loss')\n",
    "plt.plot(absc, history_dict['val_accuracy'], label = 'Accuracy')\n",
    "plt.title('Testing')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation densité spectrale du Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Densité Spectral du Signal\n",
    "\n",
    "plot_psd(train_csv, train_eras, rate = SAMPLE_RATE, Channels = eeg_Chans, titled = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Densité spectrale / échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Densité spectral / échantillon\n",
    "\n",
    "inc   = 40\n",
    "scp = samples(train_samples, inc)\n",
    "boolInt = -2\n",
    "\n",
    "plt.figure(figsize = (15, inc * 1.5))\n",
    "\n",
    "for i in scp :\n",
    "    boolInt += 2\n",
    "\n",
    "    for c in eeg_Chans :\n",
    "        x = train_eras[0][c][i]\n",
    "        yest, Pxx_den = signal.welch(x, SAMPLE_RATE)   # , scaling = 'spectrum'\n",
    "        \n",
    "        plt.subplot(inc, 4, boolInt + 1)\n",
    "        plt.semilogy(yest, Pxx_den, label = c)\n",
    "        plt.title(f\"welch - {i + 1}\", fontsize = 11)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(inc, 4, boolInt + 2)\n",
    "        r, _ = plt.psd(x, Fs = SAMPLE_RATE, label = c) # , NFFT = NFFT\n",
    "        plt.title(f\"psd - {i + 1}\", fontsize = 11)\n",
    "        plt.xlabel('')\n",
    "        plt.ylabel('')\n",
    "        # plt.legend()\n",
    "\n",
    "plt.xlabel('frequency [Hz]')\n",
    "# plt.ylabel('PSD [V**2/Hz]')\n",
    "plt.tight_layout()\n",
    "plt.show();\n",
    "\n",
    "# f, Pxx_den = signal.welch(train_eras[0]['C3'][752], SAMPLE_RATE)\n",
    "\n",
    "# print(len(Pxx_den))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation Epoques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Epoques\n",
    "\n",
    "for i in range(len(fics))[:: 3] :\n",
    "    plot_signal(train_csv[i], train_parts[i], train_spots[0][i], train_spots[1][i], channels = eeg_Chans, # \n",
    "                period = SCOPE, lag = LAG, title = headers[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation décomposition des signaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Décomposition des signaux\n",
    "\n",
    "# Test de décomposition des signaux en bandes de fréquences spécifiques compatibles avec les répartitions usuelles\n",
    "# dans le domaine des EEG ['Delta', 'Theta', 'Alpha', 'Beta', 'Gamma']\n",
    "\n",
    "for df, token in zip(train_eras, ['Gauche', 'Droite']) :\n",
    "    print(f\"Exemples - Évènement Discriminé Main {token}\")\n",
    "    plot_wavelets(df, bands_coeff, eeg_Chans, scope = 30, headers = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation des spectrogrammes (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in zip(num_events, ['Gauche', 'Droite']) :\n",
    "    print(f\"Exemples - Évènement Discriminé Main {t}\")\n",
    "    spectrogram(train_eras[i], SAMPLE_RATE, eeg_Chans, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test PCA - (Non cloncluant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Test PCA - (Non cloncluant)\n",
    "\n",
    "nca = SCOPE >> 2\n",
    "pca = PCA(nca)\n",
    "\n",
    "_, ax = plt.subplots(nrows = 2, ncols = 3, figsize = (15, 5))\n",
    "\n",
    "for i, d in enumerate(train_eras) :\n",
    "    for j, c in enumerate(eeg_Chans) :\n",
    "        Z = sc.fit_transform(list(d[c].to_list())) # \n",
    "        principal_components = pca.fit_transform(Z)\n",
    "        \n",
    "        ax[i, j].plot(range(nca), np.cumsum(pca.explained_variance_ratio_))\n",
    "        ax[i, j].set_title(f'{c} . {i}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • MNE époque (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • MNE époque (test)\n",
    "\n",
    "raw_csv = train_csv[0][eeg_Chans]\n",
    "info    = mne.create_info(ch_names = eeg_Chans, sfreq = SAMPLE_RATE, ch_types = 'eeg')\n",
    "raw_mne = mne.io.RawArray(raw_csv.T * 1e-6, info)\n",
    "loc     = np.where(train_csv[0]['EventStart'] == 1)[0]\n",
    "\n",
    "# display(compare(np.sort(np.concatenate((train_spots[0][0], train_spots[1][0]))), loc))\n",
    "\n",
    "tmin, tmax = -0., 1\n",
    "\n",
    "# loc = mne.find_events(raw_mne, stim_channel = 'C3')\n",
    "# event_id = dict(C3 = 1, aud_r = 2, vis_l = 3, vis_r = 4)\n",
    "# raw = mne.io.Raw(raw_mne, preload = True)\n",
    "# raw.filter(2, None, method = 'iir')           # replace baselining with high-pass\n",
    "# events = mne.read_events(event_fname)\n",
    "\n",
    "# raw.info['bads'] = ['MEG 2443']  # set bad channels\n",
    "# picks = mne.pick_types(info, meg = 'grad', eeg = True, eog = False, exclude = 'bads')\n",
    "# Read epochs\n",
    "absc = mne.Epochs(raw_mne, np.array([loc, loc, loc]).T, None, tmin, tmax, proj = False,\n",
    "                    picks = None, baseline = None, preload = True, verbose = False) # event_id picks\n",
    "\n",
    "# labels = epochs.events[::5, -1]\n",
    "\n",
    "# events\n",
    "\n",
    "# raw_mne.plot();\n",
    "\n",
    "# raw_mne['C3'][0][0], len(df_train_csv[2]['Cz'])\n",
    "\n",
    "display(absc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test de classification - Proposition inputs 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### • Test de classification - Proposition inputs 01\n",
    "\n",
    "# X1 = np.where(trains['EventType'] == 0, trains['C3'], trains['Cz']) \n",
    "# X2 = np.where(trains['EventType'] == 0, trains['Cz'], trains['C4']) \n",
    "# X = [signal.welch(list(v), SAMPLE_RATE)[1] for v in X1 + X2]\n",
    "\n",
    "# X = [np.append([], list(harmonic(v, bands_coeff).values())) for v in X]\n",
    "# X = np.where(trains['EventType'] == 0, trains['C3'], 2 * trains['Cz'])\n",
    "# X = [signal.welch(list(v), SAMPLE_RATE)[1] for v in X], trains['C3'] + trains['Cz'], trains['C4'] + trains['Cz']\n",
    "\n",
    "# diff = np.array([np.cos(x) for x in (trains['C3'] ** 2 + trains['C4'] ** 2)])  + 2 * trains['Cz']\n",
    "\n",
    "# dist =np.array([list(np.sqrt(x)) for x in (trains['C3'] ** 2 + trains['C4'] ** 2)])\n",
    "# display(np.shape(np.array()))\n",
    "\n",
    "# diff = np.array([d - v for v, d in zip(, dist)]) - trains['Cz'] * [x.mean() for x in trains['Cz']]\n",
    "\n",
    "# cz_min = [v.min() for v in trains['Cz']]\n",
    "# cz_max = [v.max() for v in trains['Cz']]\n",
    "\n",
    "# display(compare(cz_min, cz_max), )\n",
    "\n",
    "# trio = zip(trains['C4'], trains['C3'], trains['Cz'])\n",
    "\n",
    "# prd = [(c3 - c4) * cz for c3, c4, cz in trio] \n",
    "\n",
    "# 'Delta', 'Theta', 'Alpha', 'Beta', 'Gamma'\n",
    "\n",
    "# b, a = bands_coeff['Delta']\n",
    "\n",
    "# c3 = np.array([bandpass_filter(bw, b, a) for bw in trains['C3']])\n",
    "# c4 = np.array([bandpass_filter(bw, b, a) for bw in trains['C4']])\n",
    "\n",
    "# diff = (c4 - c3) / trains['Cz'].max() #/ cz.max() # [for v in cz]\n",
    "# X = [np.sign(s) for s in trains['C4'] - trains['C3']]\n",
    "# X = np.array([normalized(bandpass_filter(bw, b, a)) for bw in X])\n",
    "\n",
    "# X = [sk_p.minmax_scale(pywt.dwt(c4, wavelet = 'db4')[0] - pywt.dwt(c3, wavelet = 'db4')[0]) for c3, c4 in zip(trains['C3'], trains['C4'])]\n",
    "# X = [v) for v in X]\n",
    "\n",
    "# X = [pywt.dwt(v, wavelet = 'db4')[0] for v in X]\n",
    "# X = [normalized(pywt.dwt(v, wavelet = 'db4')[0]) for v in X]\n",
    "# X = [normalized(pywt.dwt(v, wavelet = 'db4')[0]) for v in X]\n",
    "# X = [normalized(v) for v in X]\n",
    "\n",
    "# X = [np.append([], v.tolist()) for v in X]\n",
    "\n",
    "# ret = pywt.dwt(data, wavelet = 'db1') #, level = 4, mode = 'antisymmetric'\n",
    "\n",
    "# for o in ret :\n",
    "#     print(np.shape(o))\n",
    "\n",
    "# # np.shape(ret[3])[0] / np.shape(train_csv[0]['Cz'])[0]\n",
    "# # (ret)\n",
    "\n",
    "# plt.figure(figsize = (20, 4))\n",
    "\n",
    "# # plt.subplot(1, 3, 1)\n",
    "# plt.plot(data)\n",
    "\n",
    "# for o in ret :\n",
    "#     plt.plot(o)\n",
    "\n",
    "\n",
    "# # plt.title(\"Original Signal\")\n",
    "# # plt.subplot(1, 3, 2)\n",
    "# # plt.title(\"Approximation Coefficients\")\n",
    "# # plt.subplot(1, 3, 3)\n",
    "# # plt.plot(cD)\n",
    "# # plt.title(\"Detail Coefficients\")\n",
    "# # plt.tight_layout()\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test de classification - Proposition inputs 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Test de classification\n",
    "\n",
    "# pca = PCA()\n",
    "# sc = StandardScaler()\n",
    "\n",
    "# X = pca.fit_transform(X)\n",
    "\n",
    "# display(np.shape(X))\n",
    "\n",
    "# X = (trains['C4'] - trains['C3']) # / [v.max() for v in trains['Cz']]\n",
    "# X = [sk_p.minmax_scale(pywt.dwt(v, wavelet = 'db4')[0]) for v in X]\n",
    "# y = trains['EventType']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 42)\n",
    "\n",
    "X_train_scaled = X_train\n",
    "X_test_scaled  = X_test\n",
    "\n",
    "# X_train_scaled = sc.fit_transform(X_train)\n",
    "# X_test_scaled  = sc.transform(X_test)\n",
    "\n",
    "# csp = CSP(nfilter = 2)\n",
    "# K-plus proches voisins\n",
    "knc = neighbors.KNeighborsClassifier()\n",
    "# SVM (support vector machine)[, 'auto', kernel = 'rbf']\n",
    "clf = svm.SVC(gamma = 'scale')\n",
    "# RandomForest \n",
    "rfc = ensemble.RandomForestClassifier(n_jobs = -1)\n",
    "#\n",
    "lrg = LogisticRegression()\n",
    "#ExtraTreesClassifier \n",
    "# Voting_clf = VotingClassifier(estimators = [('knn', clf1), ('svm', clf2), ('rf', clf3)], voting = 'hard')\n",
    "# cv3        = model_selection.KFold(n_splits = 3, random_state = 42, shuffle = True), clf4\n",
    "\n",
    "# Create a pipeline\n",
    "# pip = Pipeline([('RFC', rfc), ('SVM', clf)])    # ('CSP', csp), \n",
    "\n",
    "# pip.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = pip.predict(X_test)\n",
    "\n",
    "# y_train = np.array(y_train)\n",
    "\n",
    "# print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n",
    "# print(\"Accuracy score:\", accuracy_score(y_test, y_pred)), cv = 5, lrg\n",
    "\n",
    "for reg in [rfc, knc, clf] :\n",
    "    scores : dict = cross_validate(reg, X_train, y_train, scoring = ['accuracy'])\n",
    "    \n",
    "    reg.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    r = scores['test_accuracy']\n",
    "    \n",
    "    print(f\"● {reg} : Accuracy -> {r.mean():.1%} (±{r.std():.2}, max : {r.max():.1%})\")\n",
    "    print(f\"\\t-> score / test : {reg.score(X_test_scaled, y_test):.1%}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, y = bands_coeff['Theta']\n",
    "\n",
    "i = np.random.randint(np.shape(X)[0])\n",
    "# X = [np.sign(s) for s in (trains['C4'] - trains['C3'])]\n",
    "# y = {band: bandpass_filter(X[i], b, a) for band, (b, a) in bands_coeff.items()}\n",
    "y = trains['C4'][i] - trains['C3'][i]\n",
    "\n",
    "plt.figure()\n",
    "# plt.style.use('')\n",
    "\n",
    "plt.plot(normalized(bandpass_filter(y, b, y)), label = 'C4 - C3')\n",
    "plt.plot(normalized(bandpass_filter(np.sign(y), b, y)), label = '[C4 - C3]', c = np.random.rand(1, 3)[0])\n",
    "# plt.plot(pywt.dwt(y, wavelet = 'db4')[0], label = 'C4 - C3')\n",
    "# plt.plot(pywt.dwt(np.sign(y), wavelet = 'db4')[0], label = '[C4 - C3]')\n",
    "# plt.plot(np.zeros(512), ls = '--', c = np.random.rand(1, 3)[0])\n",
    "\n",
    "# print(np.shape(pywt.dwt(y, wavelet = 'db8')))\n",
    "\n",
    "# plt.plot(bandpass_filter(np.sign(trains['C3'][i]), b, a), label = 'C3')\n",
    "# plt.plot(bandpass_filter(np.sign(trains['C4'][i]), b, a), label = 'C4')\n",
    "# plt.plot(bandpass_filter(np.sign(trains['Cz'][i]), b, a), label = 'Cz')\n",
    "\n",
    "# for (band, signal) in reversed(y.items()) :\n",
    "#     plt.plot(pd.Series(signal), label = f'{band}', c = np.random.rand(1, 3)[0])\n",
    "\n",
    "plt.title(f\"{i}\")\n",
    "plt.legend(loc = 'upper right')\n",
    "\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    ## K-plus proches voisins\n",
    "    'knn__n_neighbors' : range(2),\n",
    "    ## SVM\n",
    "    'svm__C'      : [0.1, 1, 5],\n",
    "    'svm__kernel' : ['linear', 'softmax', 'sigmoid', 'rbf'],\n",
    "    ## RandomForest\n",
    "    # 'rf__max_features'      : ['sqrt', 'log2', None],\n",
    "    # 'rf__min_samples_split' : range(2, 32, 2),\n",
    "    # , ('rf', clf3), ('rf', clf3)\n",
    "    'estimators': [[('knn', knc), ('svm', svm)], [('knn', knc), ('svm', svm)]] \n",
    "    }\n",
    "\n",
    "grid = model_selection.GridSearchCV(estimator = Voting_clf, param_grid = params, cv = 5) \\\n",
    "    .fit(X_train_scaled, y_train)\n",
    "\n",
    "# parametres = {'max_features' : ['log2', 'sqrt', None], 'min_samples_split' : range(2, 32, 2)}\n",
    "\n",
    "# vclf = model_selection.GridSearchCV(estimator = clf3, param_grid = parametres, cv = 3) \\\n",
    "#     .fit(X_train_scaled, y_train)\n",
    "\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n",
    "print('score train :', grid.score(X_train_scaled, y_train))\n",
    "print('score test :', grid.score(X_test_scaled, y_test))\n",
    "\n",
    "# print(vclf.best_estimator_, vclf.best_params_, vclf.best_score_)\n",
    "# print('score train :', grid.score(X_train_scaled, y_train), vclf.score(X_train_scaled, y_train))\n",
    "# print('score test  :', grid.score(X_test_scaled, y_test), vclf.score(X_test_scaled, y_test))\n",
    "\n",
    "df_train_cpy, event_start = fancy_df(train_csv, label_csv['EventType'], hands_event, SCOPE)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize = (24, 5), sharey = True)\n",
    "sig = .05\n",
    "\n",
    "axes.plot(train_csv['C3'])\n",
    "\n",
    "for p in event_start :\n",
    "    axes.axvspan(p[0] - (SCOPE >> 1), p[0] + 1.5 * SCOPE, facecolor = 'orangered', alpha = .5)\n",
    "\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolInt    = 16\n",
    "start  = event_start[boolInt][0]\n",
    "entrant  = start + SCOPE\n",
    "df   = df_train_cpy['C3_4'][start : entrant]\n",
    "smooth = df.copy()\n",
    "inc      = 5\n",
    "alpha  = 1 / 3\n",
    "dec    = int(inc / alpha)\n",
    "\n",
    "plt.figure(figsize = (24, 5))\n",
    "plot_window(train_csv, ['C3', 'C4', 'C3 + C4'], start, SCOPE)\n",
    "\n",
    "# Lissage des hautes fréquences\n",
    "for _ in range(inc) :\n",
    "    smooth = simple_exponential_smoothing(smooth, alpha, 0)\n",
    "\n",
    "smooth = pd.Series(index = range(start, entrant + inc - dec), data = smooth[dec :])\n",
    "\n",
    "# plt.plot(raw - smooth, label = hands[event_start[pos][1]])\n",
    "plt.plot(smooth, '--', label = hands_event[event_start[boolInt][1]])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Introduction :  \n",
    "L’électroencéphalogramme (EEG) est une technique d’imagerie cérébrale utilisée pour étudier les activités du cerveau.  \n",
    "En plaçant des capteurs sur le cuir chevelu, l’activité électrique du cerveau est enregistrée,  \n",
    "   ce qui permet de comprendre les fonctionnements cérébraux et d’identifier certains schémas que l’on peut ensuite attribuer à des comportements précis.  \n",
    "Un des schémas d'EEG qui a été beaucoup étudié est l’imagerie motrice (IM), ou le mouvement imaginaire de la main.  \n",
    "Les IM créent des schémas bien définis qui peuvent être détectés.  \n",
    "Le but de ce projet est de créer et d’entraîner un programme permettant de prédire si l’IM d’une personne correspond à un mouvement de la main droite ou de la main gauche.  \n",
    "# **2. Étapes du projet**\n",
    "- Prétraitement des Données :  \n",
    "Les données EEG sont sujettes à des artefacts ou des erreurs de collecte dues à des mouvements parasites ou des interférences.  \n",
    "Il est donc nécessaire d'appliquer un système de prétraitement des données pour réduire le bruit et extraire les bandes de fréquences pertinentes.\n",
    "\n",
    "- Segmentation des données et extraction des caractéristiques :  \n",
    "Les données EEG sont présentées comme un flux continu. Il est donc important, pour une meilleure analyse, de diviser les données en segments temporels correspondant à l’IM.  \n",
    "Ensuite, identifier et extraire les caractéristiques pertinentes des signaux EEG associées aux IM est essentiel.  \n",
    "Cela comprend la puissance et d'autres spécificités de l’activité électrique qui définissent les IM.\n",
    "\n",
    "- Analyse statistique exploratoire :  \n",
    "Utiliser les outils d’analyse exploratoire pour mieux comprendre les données et identifier les tendances ou les patterns significatifs.\n",
    "\n",
    "- Entraînement du modèle :  \n",
    "Entraîner un modèle permettant de distinguer les différences entre les IM des mains droite et gauche.  \n",
    "Optimiser le modèle et évaluer sa performance sur un ensemble de test.\n",
    "\n",
    "- Conclusion :  \n",
    "Ces étapes sont cruciales pour développer un programme efficace de prédiction des mouvements imaginaires de la main basé sur les données EEG.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "parts = []        #\n",
    "temp = [[], []]  # Les époques pour tous les cannaux et tous les évènements.\n",
    "spots = [[], []]  # Apparitions des évènements\n",
    "\n",
    "# Pour la standardisation du nombre d'échantillon max conservé\n",
    "ceil   = min([len(label_csv[i]['EventType']) for i in count])\n",
    "merge  = False\n",
    "\n",
    "# Extraction des données relavitives à l'apparition des évènements.\n",
    "for i in count :\n",
    "    df   = train_csv[i]\n",
    "    kind  = label_csv[i]['EventType'][: ceil]\n",
    "    loc = np.where(df['EventStart'] == 1)[: ceil]\n",
    "\n",
    "    parts.append(zero_removal(df['C3'], 75))\n",
    "\n",
    "    for i in num_events :\n",
    "        spots[i].append(np.array(*loc)[*np.where(kind == i)])\n",
    "\n",
    "        room = event_epochs(spots[i][-1], SCOPE, LAG)\n",
    "\n",
    "        temp[i].append([full_event(df[c], room, merge)for c in eeg_Channels])\n",
    "\n",
    "# Regroupement des données en fonction du type de l'évènement et du cannal d'observation\n",
    "if merge :\n",
    "    temp = [[[np.append([], T[j :: 3]) for T in temp[i]] for j in range(3)] for i in num_events]\n",
    "else :\n",
    "    store = [[[], [], []], [[], [], []]]\n",
    "    \n",
    "    [[[[store[i][j].append(G) for G in X] for j, X in enumerate(T)] for T in temp[i]] for i in num_events]\n",
    "\n",
    "    temp = store\n",
    "    # res = [[np.concatenate(np.stack(tries[i], axis = 1)[j], axis = 0) for j in range(3)] for i in n_type]\n",
    "\n",
    "eras    = [pd.DataFrame(dict(zip(eeg_Channels, [pd.Series(X) for X in temp[i]]))) for i in num_events]\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# display(df_left)\n",
    "# display(df_right)\n",
    "\n",
    "# left = np.array([[f(v) for v in tmp[c]] for c in eeg_Chans for f in foo])\n",
    "# display(pd.DataFrame(left.T, columns = [f\"L{c}_{x.__name__}\" for x in foo for c in eeg_Chans])), columns = [f\"L{c}_{x.__name__}\" for x in foo for c in eeg_Chans]\n",
    "\n",
    "# print(np.shape(left_df))\n",
    "\n",
    "# right = np.array([[f(v) for v in tmp[c]] for c in eeg_Chans for f in foo])\n",
    "# display(pd.DataFrame(right.T, columns = [f\"R{c}_{x.__name__}\" for x in foo for c in eeg_Chans]))\n",
    "\n",
    "# left[1] - left[0], left[4] - left[3], left[7] - left[6], left[10] - left[9], left[13] - left[12]\n",
    "# left[0], left[1]\n",
    "\n",
    "# left_min = [apply_func(tmp[c], np.min) for c in eeg_Chans]apply_func(tmp[c], f)\n",
    "# left_max = [apply_func(tmp[c], np.max) for c in eeg_Chans]\n",
    "# left_std = [apply_func(tmp[c], np.std) for c in eeg_Chans]\n",
    "# left_ptp = [apply_func(tmp[c], np.ptp) for c in eeg_Chans]\n",
    "# left_men = [apply_func(tmp[c], np.mean) for c in eeg_Chans]\n",
    "# left_med = [apply_func(tmp[c], np.median) for c in eeg_Chans]\n",
    "\n",
    "# right_min = [apply_func(tmp[c], np.min) for c in eeg_Chans]\n",
    "# right_max = [apply_func(tmp[c], np.max) for c in eeg_Chans]\n",
    "# right_std = [apply_func(tmp[c], np.std) for c in eeg_Chans]\n",
    "# right_ptp = [apply_func(tmp[c], np.ptp) for c in eeg_Chans]\n",
    "# right_men = [apply_func(tmp[c], np.mean) for c in eeg_Chans]\n",
    "# right_med = [apply_func(tmp[c], np.median) for c in eeg_Chans]\n",
    "\n",
    "# display(pd.DataFrame(np.array((*left_min, *left_max, *left_std, *left_ptp, *left_men, *left_med)).T))\n",
    "# display(pd.DataFrame(np.array((*right_min, *right_max, *right_std, *right_ptp, *right_men, *right_med)).T))\n",
    "\n",
    "# pd.DataFrame(np.array((lc3_i, rc3_i, lc3_a, rc3_a, lc3_p, rc3_p, lc3_m, rc3_m,\n",
    "#                        lc4_i, rc4_i, lc4_a, rc4_a, lc4_p, rc4_p, lc4_m, rc4_m,\n",
    "#                        lcz_i, rcz_i, lcz_a, rcz_a, lcz_p, rcz_p, lcz_m, rcz_m)).T)\n",
    "\n",
    "# lc3_i = np.array(apply_func(left['C3'], np.min))\n",
    "# lc4_i = np.array(apply_func(left['C4'], np.min))\n",
    "# lcz_i = np.array(apply_func(left['Cz'], np.min))\n",
    "\n",
    "# lc3_a = np.array(apply_func(left['C3'], np.max))\n",
    "# lc4_a = np.array(apply_func(left['C4'], np.max))\n",
    "# lcz_a = np.array(apply_func(left['Cz'], np.max))\n",
    "\n",
    "# lc3_m = np.array(apply_func(left['C3'], np.mean))\n",
    "# lc4_m = np.array(apply_func(left['C4'], np.mean))\n",
    "# lcz_m = np.array(apply_func(left['Cz'], np.mean))\n",
    "\n",
    "# rc3_i = np.array(apply_func(right['C3'], np.min))\n",
    "# rc4_i = np.array(apply_func(right['C4'], np.min))\n",
    "# rcz_i = np.array(apply_func(right['Cz'], np.min))\n",
    "\n",
    "# rc3_a = np.array(apply_func(right['C3'], np.max))\n",
    "# rc4_a = np.array(apply_func(right['C4'], np.max))\n",
    "# rcz_a = np.array(apply_func(right['Cz'], np.max))\n",
    "\n",
    "# rc3_m = np.array(apply_func(right['C3'], np.mean))\n",
    "# rc4_m = np.array(apply_func(right['C4'], np.mean))\n",
    "# rcz_m = np.array(apply_func(right['Cz'], np.mean))\n",
    "\n",
    "# plt.scatter(x = lc3_i, y = lc3_a)\n",
    "# plt.scatter(x = rc3_i, y = rc3_a)\n",
    "# plt.scatter(x = lc4_i, y = lc4_a)\n",
    "# plt.scatter(x = rc4_i, y = rc4_a)\n",
    "\"\"\"\n",
    "\n",
    "# def apply_func(data : Board, foo) -> Vector : return [foo(v) for v in data]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
