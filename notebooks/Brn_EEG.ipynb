{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"text-align:center\" ><b>EEG</b> - Prédiction des Mouvements Imaginaires de la Main</h2>\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Le projet**\n",
    "- Intoduction  \n",
    "https://github.com/DataScientest-Studio/mar24_cds_eeg/blob/eric/references/Description_projet_EEG.pdf  \n",
    "https://www.bbci.de/competition/iv/desc_2b.pdf\n",
    "- Ressources / Données   \n",
    "https://www.kaggle.com/competitions/ucsd-neural-data-challenge/overview  \n",
    "- Bibliographie  \n",
    "https://www.bbci.de/competition/iv/desc_2b.pdf\n",
    "#### **2. Liens utils**\n",
    "- SciPy - *open-source software for mathematics, science, and engineering*  \n",
    "https://docs.scipy.org/doc/scipy/index.html  \n",
    "https://docs.scipy.org/doc/scipy/reference/signal.html  \n",
    "- MNE - *MEG + EEG Analysis & Visualisation*\n",
    "   - Accueil  \n",
    "   https://mne.tools/stable/index.html\n",
    "\n",
    "   - MNE - Data structures from arbitrary data  \n",
    "   https://mne.tools/stable/auto_tutorials/io/10_reading_meg_data.html#creating-mne-data-structures-from-arbitrary-data-from-memory\n",
    "   \n",
    "   - MNE - EEG Preprocessing  \n",
    "   https://mne.tools/dev/auto_tutorials/preprocessing/index.html  \n",
    "\n",
    "- pyRiemann - *Biosignals classification with Riemannian geometry*  \n",
    "https://pyriemann.readthedocs.io/en/latest/  \n",
    "- neurodsp - *Neuro Digital Signal Processing Toolbox*  \n",
    "https://neurodsp-tools.github.io/neurodsp/index.html#\n",
    "- Rythme Mu  \n",
    "https://fr.wikipedia.org/wiki/Rythme_Mu\n",
    "- Spectrogram from EEG  \n",
    "https://www.kaggle.com/code/cdeotte/how-to-make-spectrogram-from-eeg\n",
    "- Divers  \n",
    "https://signalprocessingsociety.org/  \n",
    "https://fr.wikipedia.org/wiki/Filtre_de_Butterworth  \n",
    "https://fr.wikipedia.org/wiki/Moyenne_mobile  \n",
    "https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html  \n",
    "https://perso.etis-lab.fr/ghaffari/2014_CCMB_Floride_USA.pdf  \n",
    "https://www.youtube.com/watch?v=wB417SAbdak&list=PLXc9qfVbMMN2TAoLHVW5NvNmJtwiHurzw  \n",
    "https://fastercapital.com/fr/sujet/identification-des-artefacts-de-traitement-du-signal-dans-des-sc%C3%A9narios-r%C3%A9els.html#:~:text=L'inspection%20visuelle%20est%20la,des%20pertes%20et%20du%20bruit.  \n",
    "   - Z-Score Normalisation  \n",
    "   https://fr.wikipedia.org/wiki/Cote_Z_(statistiques)  \n",
    "   https://typeset.io/questions/why-is-z-score-normalisation-necessary-in-pre-processing-eeg-1xv5jepyq5  \n",
    "\n",
    "   - Traitement numérique du signal  \n",
    "   https://fr.wikipedia.org/wiki/Traitement_num%C3%A9rique_du_signal  \n",
    "   - Ondelette  \n",
    "      - Wiki  \n",
    "      https://fr.wikipedia.org/wiki/Ondelette  \n",
    "\n",
    "      - L’analyse par ondelettes dans la vie de tous les jours  \n",
    "      https://interstices.info/lanalyse-par-ondelettes-dans-la-vie-de-tous-les-jours/  \n",
    "\n",
    "      - A guide for using the Wavelet Transform in Machine Learning  \n",
    "      https://ataspinar.com/2018/12/21/a-guide-for-using-the-wavelet-transform-in-machine-learning/\n",
    "      \n",
    "      - pyWavelets - *open source wavelet transform*  \n",
    "      https://pywavelets.readthedocs.io/en/latest/\n",
    "\n",
    "      - Ondelettes et applications  \n",
    "      https://www.i2m.univ-amu.fr/~caroline.chaux/GEOMDATA/TI-te5215.pdf\n",
    "\n",
    "   - Maximum de vraisemblance  \n",
    "   https://pmarchand1.github.io/ECL8202/notes_cours/03-Maximum_vraisemblance.html  \n",
    "   https://fr.wikipedia.org/wiki/Maximum_de_vraisemblance#:~:text=En%20statistique%2C%20l'estimateur%20du,maximisant%20la%20fonction%20de%20vraisemblance  \n",
    "\n",
    "   - Transformation de Fourier discrète  \n",
    "   https://fr.wikipedia.org/wiki/Transformation_de_Fourier_discr%C3%A8te  \n",
    "      - La Transformation de Fourier n’est pas adaptée à l’analyse des signaux non stationnaires.\n",
    "   - Neural Data Science in Python  \n",
    "   https://neuraldatascience.io/intro.html\n",
    "\n",
    "   - Preprocessing of EEG  \n",
    "   https://www.frontiersin.org/articles/10.3389/fninf.2015.00016/full#:~:text=The%20depositable%20preprocessing%20pipeline%20consists,with%20a%20low%20recording%20SNR  \n",
    "   https://typeset.io/papers/preprocessing-of-eeg-4go8vhcbty  \n",
    "   https://learn.neurotechedu.com/preprocessing  \n",
    "   https://g0rella.github.io/gorella_mwn/preprocessing_eeg.html  \n",
    "   \n",
    "   - Biblio :  \n",
    "   https://perso.telecom-paristech.fr/bloch/P6Image/ondelettestrsp.pdf  \n",
    "   https://www.math.u-bordeaux.fr/~jbigot/Site/Enseignement_files/ondelettesIMAT.pdf  \n",
    "   http://w3.cran.univ-lorraine.fr/perso/radu.ranta/pdf/cours_deb_ond%28fr%29.pdf\n",
    "   \n",
    "   - Digital Filtering  \n",
    "   http://notebooks.pluxbiosignals.com/notebooks/Categories/Pre-Process/digital_filtering_eeg_rev.html\n",
    "\n",
    "   - Processus stationnaire  \n",
    "   https://fr.wikipedia.org/wiki/Processus_stationnaire\n",
    "\n",
    "   - Analyse en composantes principales  \n",
    "   https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales#:~:text=L'ACP%2C%20d%C3%A9sign%C3%A9e%20en%20g%C3%A9n%C3%A9ral,une%20grandeur%20physique%2C%20comme%20les"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Chargement des différentes librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Chargement des différentes librairies\n",
    "\n",
    "import sys, os, gc\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "from src.thot.sesh import *\n",
    "\n",
    "import librosa # type: ignore\n",
    "\n",
    "from sklearn import preprocessing, model_selection, ensemble, svm, neighbors    # type: ignore\n",
    "from sklearn.model_selection import train_test_split                            # type: ignore\n",
    "from sklearn.preprocessing import StandardScaler                                # type: ignore\n",
    "from sklearn.decomposition import PCA                                           # type: ignore\n",
    "from sklearn.ensemble import VotingClassifier                                   # type: ignore\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.optimizers import AdamW, Adam                 # type: ignore\n",
    "\n",
    "from keras.models import Sequential                                 # type: ignore\n",
    "from keras.layers import LSTM, Dense, LeakyReLU, Dropout, Conv1D, GlobalAveragePooling1D   # type: ignore  #, Bidirectional, TimeDistributed, RepeatVector, Flatten\n",
    "from keras.callbacks import EarlyStopping                           # type: ignore\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# from scipy.fft import fft\n",
    "# from sktime import pycatch22\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Fonctions dépréciées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Deprecated\n",
    "\n",
    "### Ce DataFrame sera utilisé pour l'étude de des signaux.\n",
    "# @deprecated(\"Plus utilisé dans le cadre de ce projet\")\n",
    "def fancy_df(df : pd.DataFrame, events : list, hands : dict, size : int,\n",
    "             expend : int = 0) -> tuple[pd.DataFrame, list] : # deprecated\n",
    "    df_cpy = pd.DataFrame({'C3_4'  : df[['C3', 'C4']].sum(axis = 1), # Somme des signaux 'C3' et 'C4'.\n",
    "                            # 'EventStart' : df['EventStart'],       # Survenue d'un évènement lié à une des mains.\n",
    "                           'Hand'  : 0,                              # Activité liée à l'une des mains.\n",
    "                           'Left'  : 0,                              # Activité liée à la main gauche.\n",
    "                           'Right' : 0})                             # Activité liée à la main droite.\n",
    "    evts  = list(zip(np.where(df['EventStart'] == 1)[0], events))\n",
    "    size += 2 * expend\n",
    "    # df_cpy['zCore'] = stats.zscore(df['C3_4'])\n",
    "\n",
    "    for i, j in evts :\n",
    "        i                        -= expend\n",
    "        fin                       = range(i, i + size)\n",
    "        df_cpy.loc[fin, 'Hand']   = np.ones(size)\n",
    "        df_cpy.loc[fin, hands[j]] = np.ones(size)\n",
    "\n",
    "    return df_cpy, evts # [i[0] for i in evts]\n",
    "\n",
    "### \n",
    "# @deprecated(\"Plus utilisé dans le cadre de ce projet\")\n",
    "def left_right_old(df : pd.DataFrame, events : list, size : int, canals, hand : int = 0,\n",
    "                      expend : int = 0) -> pd.DataFrame :\n",
    "    lp    = range(len(canals))\n",
    "    res   = [[] for _ in lp]\n",
    "    size += expend * 2\n",
    "\n",
    "    for i in events :\n",
    "        i   -= expend\n",
    "        fin = range(i, i + size)\n",
    "\n",
    "        for k, c in enumerate(canals) :\n",
    "            res[k] = np.append(res[k], df.loc[fin, c].values)\n",
    "\n",
    "    return pd.DataFrame({'signal_epoched' : res, 'canal' : canals, 'hand' : hand, 'data_split' : [events for _ in lp]})\n",
    "\n",
    "d = 511\n",
    "\n",
    "print((d >> 5) << 5)\n",
    "print((d // 32) * 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Déclaration de constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Déclaration de constantes\n",
    "\n",
    "# Fréquence d'échantillonnage - Hz (Nombre de valeur / sec)\n",
    "SAMPLE_RATE  = 250\n",
    "# Temps additionel pour étendre le domaines d'étude.\n",
    "LAG : int    = SAMPLE_RATE >> 4     # Décalage de 1/16 de sec\n",
    "#\n",
    "NFFT : int   = 1 << 10\n",
    "# Epoque en sec donnée en nombre d'échantillon consectutif # 2\" de données (multiple de 2)\n",
    "SCOPE : int  = 2 << int(np.ceil(np.log2(SAMPLE_RATE)))\n",
    "# Deux enregistrements bipolaires + neutre\n",
    "eeg_Channels = ['C3', 'C4', 'Cz']\n",
    "# Liste des cannaux eeg associés aux évènement 0 et 1\n",
    "eeg_left     = [f'{c}_0' for c in eeg_Channels]\n",
    "eeg_right    = [f'{c}_1' for c in eeg_Channels]\n",
    "full_eeg     = eeg_left + eeg_right\n",
    "# Trois enregistrements musculaires\n",
    "ecg_Channels = ['EOG:ch01', 'EOG:ch02', 'EOG:ch03']\n",
    "# Liste de tous les cannaux des dataframes\n",
    "all_channels = eeg_Channels + ecg_Channels\n",
    "# Correspondance pour la classification\n",
    "hands_event  = {0 : 'Left', 1 : 'Right'}\n",
    "# Les bandes de fréquences d'intérêt\n",
    "eeg_bands    = {'Delta': (.1, 4), 'Theta': (4, 8), 'Alpha': (8, 13), 'Beta': (13, 30), 'Gamma': (30, (SAMPLE_RATE >> 1) - 1)}\n",
    "# Coefficients pour filtres Butterworth numérique d'ordre N pour le filtrage passe-bande\n",
    "bands_coeff  = {band : butter_bandpass(low, high, SAMPLE_RATE) for band, (low, high) in eeg_bands.items()}\n",
    "# Nombre dévènement à prédir\n",
    "num_events   = range(len(hands_event))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Acquisition des données d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Acquisition des données d'entrainement\n",
    "\n",
    "target  = \"../data/data.zip\"\n",
    "count   = len('train/')\n",
    "fics    = [x[count :] for x in files_in_zip(target, directory = 'train')]\n",
    "\n",
    "# Acquisition des fichiers du répertoir dans le fichier zip\n",
    "train_csv = csv_in_zip(target, directory = 'train', files = fics)\n",
    "label_csv = csv_in_zip(target, directory = 'y_train_only', files = fics)\n",
    "\n",
    "notes   = filename(fics)\n",
    "headers = [f\"{t} . {i + 1}\" for i, t in enumerate(notes)]\n",
    "count   = range(len(train_csv))\n",
    "\n",
    "# fics    = [f'B0{i}0{j}T.csv' for i in range(1, 9) for j in range(1, 4)]\n",
    "# df_train_pkl = pkl_in_zip(path, fichier_specifique = 'epoched_train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Acquisition des données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Acquisition des données de test\n",
    "\n",
    "test_csv = csv_in_zip(target, directory = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Pré-traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Pré-traitement des données\n",
    "\n",
    "def data_spliting(datas : list[Board], Channels : Clause, number_event : int | Index,\n",
    "                  labels : list[Board] | None = None, merge : bool = False, level : bool = True) :\n",
    "    count = range(len(datas))\n",
    "    parts = []        #\n",
    "    temp  = [[], []]  # Les époques pour tous les cannaux et tous les évènements.\n",
    "    spots = [[], []]  # Apparitions des évènements\n",
    "    # Pour la standardisation du nombre d'échantillon max conservé\n",
    "    ceil  = min([len(labels[i]['EventType']) for i in count])\n",
    "\n",
    "    if type(number_event) == int : number_event = range(number_event)\n",
    "\n",
    "    # Extraction des données relavitives à l'apparition des évènements.\n",
    "    if level :\n",
    "        for i in count :\n",
    "            df   = datas[i]\n",
    "            kind = labels[i]['EventType'][: ceil]\n",
    "            loc  = np.where(df['EventStart'] == 1)[: ceil]\n",
    "\n",
    "            parts.append(zero_removal(df['C3'], 75))\n",
    "\n",
    "            for j in number_event :\n",
    "                spots[j].append(np.array(*loc)[*np.where(kind == j)])\n",
    "\n",
    "                room = event_epochs(spots[j][-1], SCOPE, LAG) # [tf.convert_to_tensor(X) for X in event_epochs(spots[j][-1], SCOPE, LAG)]\n",
    "\n",
    "                temp[j].append([full_event(df[c], room, merge) for c in Channels])\n",
    "    else :\n",
    "        for i in count :\n",
    "            df   = datas[i]\n",
    "            kind = label_csv[i]['EventType']\n",
    "            loc  = np.where(df['EventStart'] == 1)\n",
    "\n",
    "            parts.append(zero_removal(df['C3'], 75))\n",
    "\n",
    "            for j in number_event :\n",
    "                spots[j].append(np.array(*loc)[*np.where(kind == j)])\n",
    "\n",
    "                room = event_epochs(spots[j][-1], SCOPE, LAG) # [tf.convert_to_tensor(X) for X in event_epochs(spots[j][-1], SCOPE, LAG)]\n",
    "\n",
    "                temp[j].append([full_event(df[c], room, merge) for c in Channels])\n",
    "\n",
    "    # Regroupement des données en fonction du type de l'évènement et du cannal d'observation\n",
    "    if merge :\n",
    "        temp = [[[np.append([], T[j :: 3]) for T in temp[i]] for j in range(3)] for i in number_event]\n",
    "    else :\n",
    "        store = [[[], [], []], [[], [], []]]\n",
    "        \n",
    "        [[[[store[i][j].append(G) for G in X] for j, X in enumerate(T)] for T in temp[i]] for i in number_event]\n",
    "\n",
    "        temp = store\n",
    "\n",
    "    eras = [pd.DataFrame({**dict(zip(Channels, [pd.Series(X) for X in temp[i]])), 'EventType': i}) for i in number_event]\n",
    "\n",
    "    return eras, spots, parts\n",
    "\n",
    "train_eras, train_spots, train_parts = data_spliting(train_csv, eeg_Channels, num_events, label_csv)\n",
    "\n",
    "train_samples = samples(len(train_eras[0]))\n",
    "trains        = pd.concat(train_eras, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logMelSpectrogram(data : Vector, rate : int) -> Vector :\n",
    "    # Spectrogramme\n",
    "    stfts      = np.abs(librosa.stft(data, n_fft = rate, hop_length = 1 << 4, center = False)).T\n",
    "    # Filtre de MEL\n",
    "    mel_matrix = librosa.filters.mel(sr = rate, n_fft = rate + 1, n_mels = stfts.shape[-1]).T\n",
    "    # Application du filtre au spectrogramme\n",
    "    mel_spect  = np.tensordot(stfts, mel_matrix, 1)\n",
    "\n",
    "    return np.log(mel_spect + 1e-6)\n",
    "\n",
    "\n",
    "def plot_logMelSpectrogram(data, rate) :\n",
    "    sns.heatmap(np.rot90(logMelSpectrogram(data, rate)), cmap = 'inferno')\n",
    "    \n",
    "    # loc, _ = plt.xticks()\n",
    "    # l      = np.round((loc - loc.min()) * len(data) / fe / loc.max(), 2), vmin = -6\n",
    "\n",
    "    # plt.xticks(loc, l)\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Frequency (Mel)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains['Cz'][0])\n",
    "# np.shape(logMelSpectrogram(trains['Cz'][0], SAMPLE_RATE))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(trains.drop(columns = ['EventType']), trains['EventType'], test_size = .2)\n",
    "\n",
    "train_dataset = np.array([logMelSpectrogram(X, SAMPLE_RATE) for X in X_train['Cz']])\n",
    "test_dataset  = np.array([logMelSpectrogram(X, SAMPLE_RATE) for X in X_test['Cz']])\n",
    "\n",
    "print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### • Test prédiction\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(trains.drop(columns = ['EventType']), trains['EventType'], test_size = .2)\n",
    "\n",
    "# train_dataset = np.array([logMelSpectrogram(X, SAMPLE_RATE) for X in X_train['Cz']])\n",
    "\n",
    "# display(train_dataset)\n",
    "\n",
    "# feat_count = train_dataset.shape[1]\n",
    "\n",
    "'''\n",
    "UNITS     : int = feat_count\n",
    "OPTIMIZER : str = 'AdamW'   # rmsprop, , adam, , sgd\n",
    "LOSS      : str = 'sparse_categorical_crossentropy'     # 'mae' categorical_crossentropy #\n",
    "\n",
    "\n",
    "# Ajout de la premiere couche lstm\n",
    "model.add(LSTM(UNITS, input_shape = (feat_count, 1), activation = LeakyReLU(), return_sequences = True)) #  'tanh'\n",
    "model.add(LSTM(UNITS, dropout = DROPOUT, return_sequences = False))\n",
    "\n",
    "# Ajout de la couche de sortie\n",
    "# model.add(Dense(2, activation = 'softmax'))\n",
    "model.add(Dense(feat_count)) # , activation = \"softmax\", 'loss'\n",
    "model.compile(optimizer = OPTIMIZER, loss = LOSS, metrics = ['accuracy'])\n",
    "\n",
    "stop    = EarlyStopping(monitor = 'accuracy', mode = 'max', verbose = 1, patience = 100)\n",
    "history = model.fit(train_dataset, y_train, epochs = EPOCH, callbacks = [stop]) # , batch_size = BATCHSIZE, verbose = 2\n",
    "'''\n",
    "\n",
    "BATCHSIZE : int = 32\n",
    "EPOCH     : int = 400\n",
    "DROPOUT   : float = .2\n",
    "\n",
    "count = train_dataset.shape[2]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(64, kernel_size = 5, dilation_rate = 2, input_shape = train_dataset.shape[1:]))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(LeakyReLU())\n",
    "\n",
    "# Partie 2\n",
    "model.add(Conv1D(128, kernel_size = 5, dilation_rate = 2))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(LeakyReLU())\n",
    "\n",
    "# Partie 3\n",
    "# model.add(Conv1D(256, kernel_size = 5, dilation_rate = 2))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(LeakyReLU())\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# Classification\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = AdamW(), metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, y_train, verbose = 1, batch_size = BATCHSIZE, epochs = EPOCH)  # , validation_data = (test_dataset, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation densité spectrale du Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Densité Spectral du Signal\n",
    "\n",
    "plot_psd(train_csv, train_eras, rate = SAMPLE_RATE, Channels = eeg_Channels, titled = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Densité spectrale / échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Densité spectral / échantillon\n",
    "\n",
    "n   = 40\n",
    "scp = samples(train_samples, n)\n",
    "pos = -2\n",
    "\n",
    "plt.figure(figsize = (15, n * 1.5))\n",
    "\n",
    "for i in scp :\n",
    "    pos += 2\n",
    "\n",
    "    for c in eeg_Channels :\n",
    "        x = train_eras[0][c][i]\n",
    "        f, Pxx_den = signal.welch(x, SAMPLE_RATE)   # , scaling = 'spectrum'\n",
    "        \n",
    "        plt.subplot(n, 4, pos + 1)\n",
    "        plt.semilogy(f, Pxx_den, label = c)\n",
    "        plt.title(f\"welch - {i + 1}\", fontsize = 11)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(n, 4, pos + 2)\n",
    "        r, _ = plt.psd(x, Fs = SAMPLE_RATE, label = c) # , NFFT = NFFT\n",
    "        plt.title(f\"psd - {i + 1}\", fontsize = 11)\n",
    "        plt.xlabel('')\n",
    "        plt.ylabel('')\n",
    "        # plt.legend()\n",
    "\n",
    "plt.xlabel('frequency [Hz]')\n",
    "# plt.ylabel('PSD [V**2/Hz]')\n",
    "plt.tight_layout()\n",
    "plt.show();\n",
    "\n",
    "# f, Pxx_den = signal.welch(train_eras[0]['C3'][752], SAMPLE_RATE)\n",
    "\n",
    "# print(len(Pxx_den))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation Epoques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Epoques\n",
    "\n",
    "for i in range(len(fics))[:: 3] :\n",
    "    plot_signal(train_csv[i], train_parts[i], train_spots[0][i], train_spots[1][i], channels = eeg_Channels,\n",
    "                period = SCOPE, lag = LAG, title = headers[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation décomposition des signaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation Décomposition des signaux\n",
    "\n",
    "# Test de décomposition des signaux en bandes de fréquences spécifiques compatibles avec les répartitions usuelles\n",
    "# dans le domaine des EEG ['Delta', 'Theta', 'Alpha', 'Beta', 'Gamma']\n",
    "\n",
    "for df, token in zip(train_eras, ['Gauche', 'Droite']) :\n",
    "    print(f\"Exemples - Évènement Discriminé Main {token}\")\n",
    "    plot_wavelets(df, bands_coeff, eeg_Channels, scope = 30, headers = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Visualisation des spectrogrammes (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Visualisation des spectrogrammes / Test\n",
    "\n",
    "def logMelSpectrogram(raw : Vector, rate : int, nfft : int = 1 << 10) :\n",
    "    # Spectrogramme\n",
    "    stfts = np.abs(librosa.stft(y = raw, n_fft = nfft, hop_length = rate, center = False)).T\n",
    "    # Filtre de MEL\n",
    "    liny  = librosa.filters.mel(sr = rate, n_fft = nfft, n_mels = stfts.shape[-1]).T\n",
    "    # Application du filtre au spectrogramme\n",
    "    mel_  = np.tensordot(stfts, liny, 1)\n",
    "\n",
    "    return np.log(mel_ + 1e-6)\n",
    "\n",
    "def img_spectrogram(raw : Vector, rate : int, nfft : int = 1 << 10) -> Vector :\n",
    "    return librosa.feature.melspectrogram(y = raw, sr = rate, hop_length = 1, \n",
    "                            n_fft = nfft, n_mels = 32, fmin = 0, fmax = 20, win_length = 32)\n",
    "\n",
    "def spectrogram(data : Board, rate : int, channels : Clause, n_row : int = 5, n_col : int = 12) :\n",
    "    sample = np.random.default_rng().integers(data.shape[0], size = n_row)\n",
    "\n",
    "    sample.sort()\n",
    "\n",
    "    plt.figure(figsize = (18, 2 * .48 * n_row))\n",
    "\n",
    "    pos = 0\n",
    "\n",
    "    for k in sample :\n",
    "        for c in channels :\n",
    "            x   = normalized(data[c][k])\n",
    "            raw = img_spectrogram(raw = x, rate = rate)\n",
    "            pos += 1\n",
    "            \n",
    "            plt.subplot(n_row, n_col, pos)\n",
    "            plt.title(f\"{((pos - 1) // 3) + 1} . {k} - {c}\", fontsize = 8)\n",
    "            librosa.display.specshow(data = 1 - raw, sr = rate, hop_length = 1)\n",
    "            \n",
    "            # pos = n_col * (i >> 1) + j\n",
    "            # f, t, Sxx = signal.spectrogram(x, rate)\n",
    "            # plt.subplot(n_row, n_col, pos + 4)\n",
    "            # plt.pcolormesh(t, f, 1 - Sxx, shading = 'gouraud')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show();\n",
    "\n",
    "for i, t in zip(num_events, ['Gauche', 'Droite']) :\n",
    "    print(f\"Exemples - Évènement Discriminé Main {t}\")\n",
    "    spectrogram(train_eras[i], SAMPLE_RATE, eeg_Channels, 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test PCA - (Non cloncluant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Test PCA - (Non cloncluant)\n",
    "\n",
    "nca = 250\n",
    "sc  = StandardScaler()\n",
    "pca = PCA(nca)\n",
    "\n",
    "_, ax = plt.subplots(nrows = 2, ncols = 3, figsize = (15, 5))\n",
    "\n",
    "for i, d in enumerate(train_eras) :\n",
    "    for j, c in enumerate(eeg_Channels) :\n",
    "        Z = sc.fit_transform(list(d[c].to_list())) # \n",
    "        principal_components = pca.fit_transform(Z)\n",
    "        \n",
    "        ax[i, j].plot(range(nca), np.cumsum(pca.explained_variance_ratio_))\n",
    "        ax[i, j].set_title(f'{c} . {i}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • MNE époque (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • MNE époque (test)\n",
    "\n",
    "raw_csv = train_csv[0][eeg_Channels]\n",
    "info    = mne.create_info(ch_names = eeg_Channels, sfreq = SAMPLE_RATE, ch_types = 'eeg')\n",
    "raw_mne = mne.io.RawArray(raw_csv.T * 1e-6, info)\n",
    "loc     = np.where(train_csv[0]['EventStart'] == 1)[0]\n",
    "\n",
    "# display(compare(np.sort(np.concatenate((train_spots[0][0], train_spots[1][0]))), loc))\n",
    "\n",
    "tmin, tmax = -0., 1\n",
    "\n",
    "# loc = mne.find_events(raw_mne, stim_channel = 'C3')\n",
    "# event_id = dict(C3 = 1, aud_r = 2, vis_l = 3, vis_r = 4)\n",
    "# raw = mne.io.Raw(raw_mne, preload = True)\n",
    "# raw.filter(2, None, method = 'iir')           # replace baselining with high-pass\n",
    "# events = mne.read_events(event_fname)\n",
    "\n",
    "# raw.info['bads'] = ['MEG 2443']  # set bad channels\n",
    "# picks = mne.pick_types(info, meg = 'grad', eeg = True, eog = False, exclude = 'bads')\n",
    "# Read epochs\n",
    "epochs = mne.Epochs(raw_mne, np.array([loc, loc, loc]).T, None, tmin, tmax, proj = False,\n",
    "                    picks = None, baseline = None, preload = True, verbose = False) # event_id picks\n",
    "\n",
    "# labels = epochs.events[::5, -1]\n",
    "\n",
    "# events\n",
    "\n",
    "# raw_mne.plot();\n",
    "\n",
    "# raw_mne['C3'][0][0], len(df_train_csv[2]['Cz'])\n",
    "\n",
    "display(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Sans titre 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • ---\n",
    "\n",
    "extra = pd.concat([share_out(train, h0, h1, SCOPE, eeg_Channels) for train, h0, h1 in zip(train_csv, case_A, case_B)])\n",
    "\n",
    "extra.reset_index(drop = True, inplace = True)\n",
    "extra.fillna(0, inplace = True)\n",
    "\n",
    "print(extra)\n",
    "\n",
    "df_X = extra.drop(columns = ['data_split', 'hand', 'C3_dum', 'C4_dum', 'Cz_dum'])\n",
    "y    = extra['hand']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, y, test_size = .2)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train, y_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Test de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### • Test de classification\n",
    "\n",
    "# K-plus proches voisins\n",
    "clf1 = neighbors.KNeighborsClassifier()\n",
    "# SVM (support vector machine)[, 'auto']\n",
    "clf2 = svm.SVC(gamma = 'scale')\n",
    "# RandomForest\n",
    "clf3 = ensemble.RandomForestClassifier(n_jobs = -1)\n",
    "\n",
    "Voting_clf = VotingClassifier(estimators = [('knn', clf1), ('svm', clf2), ('rf', clf3)], voting = 'hard')\n",
    "# cv3        = model_selection.KFold(n_splits = 3, random_state = 42, shuffle = True)\n",
    "\n",
    "for clf in [clf1, clf2, clf3] :\n",
    "    print(f\"● {clf} :\")\n",
    "\n",
    "    scores : dict = model_selection.cross_validate(clf, X_train, y_train, cv = 3, scoring = ['accuracy'])\n",
    "    \n",
    "    # r = abs(scores['test_neg_mean_squared_error'])\n",
    "    # print(f\"  - Mean square ;{r.mean(): .3} (±{r.std(): .2})\")\n",
    "    r = scores['test_accuracy']\n",
    "    print(f\"  - Accuracy    ;{r.mean(): .3} (±{r.std(): .2})\")\n",
    "    print()\n",
    "\n",
    "params = {\n",
    "    ## K-plus proches voisins\n",
    "    'knn__n_neighbors' : range(2, 5),\n",
    "    ## SVM\n",
    "    'svm__C'      : [0.1, 1, 5],\n",
    "    'svm__kernel' : ['linear', 'sigmoid', 'rbf'],\n",
    "    ## RandomForest\n",
    "    # 'rf__max_features'      : ['sqrt', 'log2', None],\n",
    "    # 'rf__min_samples_split' : range(2, 32, 2),\n",
    "    # , ('rf', clf3), ('rf', clf3)\n",
    "    'estimators': [[('knn', clf1), ('svm', clf2)], [('knn', clf1), ('svm', clf2)]] \n",
    "    }\n",
    "\n",
    "grid = model_selection.GridSearchCV(estimator = Voting_clf, param_grid = params, cv = 5) \\\n",
    "    .fit(X_train_scaled, y_train)\n",
    "\n",
    "# parametres = {'max_features' : ['log2', 'sqrt', None], 'min_samples_split' : range(2, 32, 2)}\n",
    "\n",
    "# vclf = model_selection.GridSearchCV(estimator = clf3, param_grid = parametres, cv = 3) \\\n",
    "#     .fit(X_train_scaled, y_train)\n",
    "\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n",
    "print('score train :', grid.score(X_train_scaled, y_train))\n",
    "print('score test :', grid.score(X_test_scaled, y_test))\n",
    "\n",
    "# print(vclf.best_estimator_, vclf.best_params_, vclf.best_score_)\n",
    "# print('score train :', grid.score(X_train_scaled, y_train), vclf.score(X_train_scaled, y_train))\n",
    "# print('score test  :', grid.score(X_test_scaled, y_test), vclf.score(X_test_scaled, y_test))\n",
    "\n",
    "df_train_cpy, event_start = fancy_df(train_csv, label_csv['EventType'], hands_event, SCOPE)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize = (24, 5), sharey = True)\n",
    "sig = .05\n",
    "\n",
    "axes.plot(train_csv['C3'])\n",
    "\n",
    "for p in event_start :\n",
    "    axes.axvspan(p[0] - (SCOPE >> 1), p[0] + 1.5 * SCOPE, facecolor = 'orangered', alpha = .5)\n",
    "\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos    = 16\n",
    "start  = event_start[pos][0]\n",
    "entrant  = start + SCOPE\n",
    "df   = df_train_cpy['C3_4'][start : entrant]\n",
    "smooth = df.copy()\n",
    "n      = 5\n",
    "alpha  = 1 / 3\n",
    "dec    = int(n / alpha)\n",
    "\n",
    "plt.figure(figsize = (24, 5))\n",
    "plot_window(train_csv, ['C3', 'C4', 'C3 + C4'], start, SCOPE)\n",
    "\n",
    "# Lissage des hautes fréquences\n",
    "for _ in range(n) :\n",
    "    smooth = simple_exponential_smoothing(smooth, alpha, 0)\n",
    "\n",
    "smooth = pd.Series(index = range(start, entrant + n - dec), data = smooth[dec :])\n",
    "\n",
    "# plt.plot(raw - smooth, label = hands[event_start[pos][1]])\n",
    "plt.plot(smooth, '--', label = hands_event[event_start[pos][1]])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- Introduction :  \n",
    "L’électroencéphalogramme (EEG) est une technique d’imagerie cérébrale utilisée pour étudier les activités du cerveau.  \n",
    "En plaçant des capteurs sur le cuir chevelu, l’activité électrique du cerveau est enregistrée,  \n",
    "   ce qui permet de comprendre les fonctionnements cérébraux et d’identifier certains schémas que l’on peut ensuite attribuer à des comportements précis.  \n",
    "Un des schémas d'EEG qui a été beaucoup étudié est l’imagerie motrice (IM), ou le mouvement imaginaire de la main.  \n",
    "Les IM créent des schémas bien définis qui peuvent être détectés.  \n",
    "Le but de ce projet est de créer et d’entraîner un programme permettant de prédire si l’IM d’une personne correspond à un mouvement de la main droite ou de la main gauche.  \n",
    "# **2. Étapes du projet**\n",
    "- Prétraitement des Données :  \n",
    "Les données EEG sont sujettes à des artefacts ou des erreurs de collecte dues à des mouvements parasites ou des interférences.  \n",
    "Il est donc nécessaire d'appliquer un système de prétraitement des données pour réduire le bruit et extraire les bandes de fréquences pertinentes.\n",
    "\n",
    "- Segmentation des données et extraction des caractéristiques :  \n",
    "Les données EEG sont présentées comme un flux continu. Il est donc important, pour une meilleure analyse, de diviser les données en segments temporels correspondant à l’IM.  \n",
    "Ensuite, identifier et extraire les caractéristiques pertinentes des signaux EEG associées aux IM est essentiel.  \n",
    "Cela comprend la puissance et d'autres spécificités de l’activité électrique qui définissent les IM.\n",
    "\n",
    "- Analyse statistique exploratoire :  \n",
    "Utiliser les outils d’analyse exploratoire pour mieux comprendre les données et identifier les tendances ou les patterns significatifs.\n",
    "\n",
    "- Entraînement du modèle :  \n",
    "Entraîner un modèle permettant de distinguer les différences entre les IM des mains droite et gauche.  \n",
    "Optimiser le modèle et évaluer sa performance sur un ensemble de test.\n",
    "\n",
    "- Conclusion :  \n",
    "Ces étapes sont cruciales pour développer un programme efficace de prédiction des mouvements imaginaires de la main basé sur les données EEG.\n",
    "'''\n",
    "\n",
    "'''\n",
    "parts = []        #\n",
    "temp = [[], []]  # Les époques pour tous les cannaux et tous les évènements.\n",
    "spots = [[], []]  # Apparitions des évènements\n",
    "\n",
    "# Pour la standardisation du nombre d'échantillon max conservé\n",
    "ceil   = min([len(label_csv[i]['EventType']) for i in count])\n",
    "merge  = False\n",
    "\n",
    "# Extraction des données relavitives à l'apparition des évènements.\n",
    "for i in count :\n",
    "    df   = train_csv[i]\n",
    "    kind  = label_csv[i]['EventType'][: ceil]\n",
    "    loc = np.where(df['EventStart'] == 1)[: ceil]\n",
    "\n",
    "    parts.append(zero_removal(df['C3'], 75))\n",
    "\n",
    "    for i in num_events :\n",
    "        spots[i].append(np.array(*loc)[*np.where(kind == i)])\n",
    "\n",
    "        room = event_epochs(spots[i][-1], SCOPE, LAG)\n",
    "\n",
    "        temp[i].append([full_event(df[c], room, merge)for c in eeg_Channels])\n",
    "\n",
    "# Regroupement des données en fonction du type de l'évènement et du cannal d'observation\n",
    "if merge :\n",
    "    temp = [[[np.append([], T[j :: 3]) for T in temp[i]] for j in range(3)] for i in num_events]\n",
    "else :\n",
    "    store = [[[], [], []], [[], [], []]]\n",
    "    \n",
    "    [[[[store[i][j].append(G) for G in X] for j, X in enumerate(T)] for T in temp[i]] for i in num_events]\n",
    "\n",
    "    temp = store\n",
    "    # res = [[np.concatenate(np.stack(tries[i], axis = 1)[j], axis = 0) for j in range(3)] for i in n_type]\n",
    "\n",
    "eras    = [pd.DataFrame(dict(zip(eeg_Channels, [pd.Series(X) for X in temp[i]]))) for i in num_events]\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
